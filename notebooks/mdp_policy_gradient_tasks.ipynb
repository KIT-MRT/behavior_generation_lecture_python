{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.distributions.categorical import Categorical\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes: List[int], activation=nn.ReLU, output_activation=nn.Identity):\n",
    "    \"\"\"Returns a multi-layer perceptron\"\"\"\n",
    "    mlp = nn.Sequential()\n",
    "    for i in range(len(sizes) - 1):\n",
    "        mlp.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        if i < len(sizes) - 2:\n",
    "            mlp.append(activation())\n",
    "        else:\n",
    "            mlp.append(output_activation())\n",
    "    return mlp\n",
    "\n",
    "\n",
    "class CategorialPolicy:\n",
    "    def __init__(self, sizes: List[int], actions: List):\n",
    "        assert sizes[-1] == len(actions)\n",
    "        torch.manual_seed(1337)\n",
    "        self.net = mlp(sizes=sizes)\n",
    "        self.actions = actions\n",
    "        self._actions_tensor = torch.as_tensor(actions, dtype=torch.float32).view(\n",
    "            len(actions), -1\n",
    "        )\n",
    "\n",
    "    def _get_distribution(self, state: torch.Tensor) -> Categorical:\n",
    "        \"\"\"Calls the model and returns a categorial distribution over the actions.\"\"\"\n",
    "        logits = self.net(state)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(\n",
    "        self, state: torch.Tensor, deterministic: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Returns an action sample for the given state\"\"\"\n",
    "        policy = self._get_distribution(state)\n",
    "        if deterministic:\n",
    "            return self.actions[policy.mode.item()]\n",
    "        return self.actions[policy.sample().item()]\n",
    "\n",
    "    def get_log_prob(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the log-probability for taking the action, when being the given state\"\"\"\n",
    "        return self._get_distribution(states).log_prob(\n",
    "            self._get_action_id_from_action(actions)\n",
    "        )\n",
    "\n",
    "    def _get_action_id_from_action(self, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the indices of the passed actions in self.actions\"\"\"\n",
    "        reshaped_actions = actions.unsqueeze(1).expand(\n",
    "            -1, self._actions_tensor.size(0), -1\n",
    "        )\n",
    "        reshaped_actions_tensor = self._actions_tensor.unsqueeze(0).expand(\n",
    "            actions.size(0), -1, -1\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.all(reshaped_actions == reshaped_actions_tensor, dim=-1)\n",
    "        )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_MDP_DICT = {\n",
    "    \"states\": [1, 2],\n",
    "    \"actions\": [\"A\", \"B\"],\n",
    "    \"initial_state\": 1,\n",
    "    \"terminal_states\": [2],\n",
    "    \"transition_probabilities\": {\n",
    "        (1, \"A\"): [(0.2, 1), (0.8, 2)],\n",
    "        (1, \"B\"): [(0.5, 1), (0.5, 2)],\n",
    "        (2, \"A\"): [(1.0, 1)],\n",
    "        (2, \"B\"): [(0.3, 1), (0.7, 2)],\n",
    "    },\n",
    "    \"reward\": {1: -0.1, 2: -0.5},\n",
    "}\n",
    "\n",
    "GRID_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [-0.04, -0.04, -0.04, +1],\n",
    "        [-0.04, None, -0.04, -1],\n",
    "        [-0.04, -0.04, -0.04, -0.04],\n",
    "    ],\n",
    "    \"initial_state\": (1, 0),\n",
    "    \"terminal_states\": {(3, 2), (3, 1)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        (0, 1): [(0.8, (0, 1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (0, -1): [(0.8, (0, -1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (1, 0): [(0.8, (1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "        (-1, 0): [(0.8, (-1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "    },\n",
    "}\n",
    "\n",
    "LC_LEFT_ACTION, STAY_IN_LANE_ACTION, LC_RIGHT_ACTION = (1, 1), (1, 0), (1, -1)\n",
    "\n",
    "HIGHWAY_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [0, -1, -1, -1, -1, -1, -1, -1, -1, -50],\n",
    "        [0, -2, -2, -2, -2, -2, -2, -2, -2, -50],\n",
    "        [0, -3, -3, -3, -3, -3, -3, -3, -3, -50],\n",
    "        [None, None, None, None, None, None, -2, -2, -2, 0],\n",
    "    ],\n",
    "    \"initial_state\": (0, 2),\n",
    "    \"terminal_states\": {(9, 3), (9, 1), (9, 2), (9, 0)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        STAY_IN_LANE_ACTION: [(1.0, STAY_IN_LANE_ACTION)],\n",
    "        LC_LEFT_ACTION: [(0.5, LC_LEFT_ACTION), (0.5, STAY_IN_LANE_ACTION)],\n",
    "        LC_RIGHT_ACTION: [(0.75, LC_RIGHT_ACTION), (0.25, STAY_IN_LANE_ACTION)],\n",
    "    },\n",
    "    \"restrict_actions_to_available_states\": False,\n",
    "}\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: Set[Any],\n",
    "        actions: Set[Any],\n",
    "        initial_state: Any,\n",
    "        terminal_states: Set[Any],\n",
    "        transition_probabilities: Dict[Tuple[Any, Any], List[Tuple[float, Any]]],\n",
    "        reward: Dict[Any, float],\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process.\n",
    "\n",
    "        Args:\n",
    "            states: Set of states.\n",
    "            actions: Set of actions.\n",
    "            initial_state: Initial state.\n",
    "            terminal_states: Set of terminal states.\n",
    "            transition_probabilities: Dictionary of transition\n",
    "                probabilities, mapping from tuple (state, action) to\n",
    "                list of tuples (probability, next state).\n",
    "            reward: Dictionary of rewards per state, mapping from state\n",
    "                to reward.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "\n",
    "        self.actions = actions\n",
    "\n",
    "        assert initial_state in self.states\n",
    "        self.initial_state = initial_state\n",
    "\n",
    "        for terminal_state in terminal_states:\n",
    "            assert (\n",
    "                terminal_state in self.states\n",
    "            ), f\"The terminal state {terminal_state} is not in states {states}\"\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                if (state, action) not in transition_probabilities:\n",
    "                    continue\n",
    "                total_prob = 0\n",
    "                for prob, next_state in transition_probabilities[(state, action)]:\n",
    "                    assert (\n",
    "                        next_state in self.states\n",
    "                    ), f\"next_state={next_state} is not in states={states}\"\n",
    "                    total_prob += prob\n",
    "                assert math.isclose(total_prob, 1), \"Probabilities must add to one\"\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "        assert set(reward.keys()) == set(\n",
    "            self.states\n",
    "        ), \"Rewards must be defined for every state in the set of states\"\n",
    "        for state in self.states:\n",
    "            assert reward[state] is not None\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_states(self) -> Set[Any]:\n",
    "        \"\"\"Get the set of states.\"\"\"\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state) -> Set[Any]:\n",
    "        \"\"\"Get the set of actions available in a certain state, returns [None] for terminal states.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return {None}\n",
    "        return set(\n",
    "            [a for a in self.actions if (state, a) in self.transition_probabilities]\n",
    "        )\n",
    "\n",
    "    def get_reward(self, state) -> float:\n",
    "        \"\"\"Get the reward for a specific state.\"\"\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def is_terminal(self, state) -> bool:\n",
    "        \"\"\"Return whether a state is a terminal state.\"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_transitions_with_probabilities(\n",
    "        self, state, action\n",
    "    ) -> List[Tuple[float, Any]]:\n",
    "        \"\"\"Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.\"\"\"\n",
    "        if action is None or self.is_terminal(state):\n",
    "            return [(0.0, state)]\n",
    "        return self.transition_probabilities[(state, action)]\n",
    "\n",
    "    def sample_next_state(self, state, action) -> Any:\n",
    "        \"\"\"Randomly sample the next state given the current state and taken action.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return ValueError(\"No next state for terminal states.\")\n",
    "        if action is None:\n",
    "            return ValueError(\"Action must not be None.\")\n",
    "        prob_per_transition = self.get_transitions_with_probabilities(state, action)\n",
    "        num_actions = len(prob_per_transition)\n",
    "        choice = np.random.choice(\n",
    "            num_actions, p=[ppa[0] for ppa in prob_per_transition]\n",
    "        )\n",
    "        return prob_per_transition[choice][1]\n",
    "\n",
    "    def execute_action(self, state, action) -> Tuple[Any, float, bool]:\n",
    "        \"\"\"Executes the action in the current state and returns the new state, obtained reward and terminal flag.\"\"\"\n",
    "        new_state = self.sample_next_state(state=state, action=action)\n",
    "        reward = self.get_reward(state=new_state)\n",
    "        terminal = self.is_terminal(state=new_state)\n",
    "        return new_state, reward, terminal\n",
    "\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid: List[List[Union[float, None]]],\n",
    "        initial_state: Tuple[int, int],\n",
    "        terminal_states: Set[Tuple[int, int]],\n",
    "        transition_probabilities_per_action: Dict[\n",
    "            Tuple[int, int], List[Tuple[float, Tuple[int, int]]]\n",
    "        ],\n",
    "        restrict_actions_to_available_states: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process on a grid.\n",
    "\n",
    "        Args:\n",
    "            grid: List of lists, containing the rewards of the grid\n",
    "                states or None.\n",
    "            initial_state: Initial state in the grid.\n",
    "            terminal_states: Set of terminal states in the grid.\n",
    "            transition_probabilities_per_action: Dictionary of\n",
    "                transition probabilities per action, mapping from action\n",
    "                to list of tuples (probability, next state).\n",
    "            restrict_actions_to_available_states: Whether to restrict\n",
    "                actions to those that result in valid next states.\n",
    "        \"\"\"\n",
    "        states = set()\n",
    "        reward = {}\n",
    "        grid = grid.copy()\n",
    "        grid.reverse()  # y-axis pointing upwards\n",
    "        rows = len(grid)\n",
    "        cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(cols):\n",
    "            for y in range(rows):\n",
    "                if grid[y][x] is not None:\n",
    "                    states.add((x, y))\n",
    "                    reward[(x, y)] = grid[y][x]\n",
    "\n",
    "        transition_probabilities = {}\n",
    "        for state in states:\n",
    "            for action in transition_probabilities_per_action.keys():\n",
    "                transition_probability_list = self._generate_transition_probability_list(\n",
    "                    state=state,\n",
    "                    action=action,\n",
    "                    restrict_actions_to_available_states=restrict_actions_to_available_states,\n",
    "                    states=states,\n",
    "                    transition_probabilities_per_action=transition_probabilities_per_action,\n",
    "                    next_state_fn=self._next_state_deterministic,\n",
    "                )\n",
    "                if transition_probability_list:\n",
    "                    transition_probabilities[\n",
    "                        (state, action)\n",
    "                    ] = transition_probability_list\n",
    "\n",
    "        super().__init__(\n",
    "            states=states,\n",
    "            actions=set(transition_probabilities_per_action.keys()),\n",
    "            initial_state=initial_state,\n",
    "            terminal_states=terminal_states,\n",
    "            transition_probabilities=transition_probabilities,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_transition_probability_list(\n",
    "        state,\n",
    "        action,\n",
    "        restrict_actions_to_available_states,\n",
    "        states,\n",
    "        transition_probabilities_per_action,\n",
    "        next_state_fn,\n",
    "    ):\n",
    "        \"\"\"Generate the transition probability list of the grid.\"\"\"\n",
    "        transition_probability_list = []\n",
    "        none_in_next_states = False\n",
    "        for (\n",
    "            probability,\n",
    "            deterministic_action,\n",
    "        ) in transition_probabilities_per_action[action]:\n",
    "            next_state = next_state_fn(\n",
    "                state,\n",
    "                deterministic_action,\n",
    "                states,\n",
    "                output_none_if_non_existing_state=restrict_actions_to_available_states,\n",
    "            )\n",
    "            if next_state is None:\n",
    "                none_in_next_states = True\n",
    "                break\n",
    "            transition_probability_list.append((probability, next_state))\n",
    "\n",
    "        if not none_in_next_states:\n",
    "            return transition_probability_list\n",
    "\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_state_deterministic(\n",
    "        state, action, states, output_none_if_non_existing_state=False\n",
    "    ):\n",
    "        \"\"\"Output the next state given the action in a deterministic setting.\n",
    "        Output None if next state not existing in case output_none_if_non_existing_state is True.\n",
    "        \"\"\"\n",
    "        next_state_candidate = tuple(np.array(state) + np.array(action))\n",
    "        if next_state_candidate in states:\n",
    "            return next_state_candidate\n",
    "        if output_none_if_non_existing_state:\n",
    "            return None\n",
    "        return state\n",
    "\n",
    "\n",
    "def policy_gradient(\n",
    "    *,\n",
    "    mdp: MDP,\n",
    "    pol: CategorialPolicy,\n",
    "    lr: float = 1e-2,\n",
    "    iterations: int = 50,\n",
    "    batch_size: int = 5000,\n",
    "    return_history: bool = False,\n",
    "    use_random_init_state: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Union[List[CategorialPolicy], CategorialPolicy]:\n",
    "    \"\"\"Train a paramterized policy using vanilla policy gradient.\n",
    "\n",
    "    Adapted from: https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py\n",
    "\n",
    "    The MIT License (MIT)\n",
    "\n",
    "    Copyright (c) 2018 OpenAI (http://openai.com)\n",
    "\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        pol: The stochastic policy to be trained.\n",
    "        lr: Learning rate.\n",
    "        iterations: Number of iterations.\n",
    "        batch_size: Number of samples generated for each policy update.\n",
    "        return_history: Whether to return the whole history of value estimates\n",
    "            instead of just the final estimate.\n",
    "        use_random_init_state: bool, if the agent should be initialized randomly.\n",
    "        verbose: bool, if traing progress should be printed.\n",
    "\n",
    "    Returns:\n",
    "        The final policy, if return_history is false. The\n",
    "        history of policies as list, if return_history is true.\n",
    "    \"\"\"\n",
    "    np.random.seed(1337)\n",
    "    torch.manual_seed(1337)\n",
    "\n",
    "    # add untrained model to model_checkpoints\n",
    "    model_checkpoints = [deepcopy(pol)]\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = torch.optim.Adam(pol.net.parameters(), lr=lr)\n",
    "\n",
    "    # get non-terminal states\n",
    "    non_terminal_states = [state for state in mdp.states if not mdp.is_terminal(state)]\n",
    "\n",
    "    # training loop\n",
    "    for i in range(1, iterations + 1):\n",
    "\n",
    "        # make some empty lists for logging.\n",
    "        buffer = {\n",
    "            \"states\": [],\n",
    "            \"actions\": [],\n",
    "            \"weights\": [],\n",
    "            \"ep_rets\": [],\n",
    "            \"ep_lens\": [],\n",
    "        }\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        if use_random_init_state:\n",
    "            state = non_terminal_states[np.random.choice(len(non_terminal_states))]\n",
    "        else:\n",
    "            state = mdp.initial_state\n",
    "        episode_rewards = []\n",
    "\n",
    "        # collect experience by acting in the mdp\n",
    "        while True:\n",
    "            # save visited state\n",
    "            buffer[\"states\"].append(deepcopy(state))\n",
    "\n",
    "            # call model to get next action\n",
    "            action = pol.get_action(state=torch.as_tensor(state, dtype=torch.float32))\n",
    "\n",
    "            # execute action in the environment\n",
    "            state, reward, done = mdp.execute_action(state=state, action=action)\n",
    "\n",
    "            # save action, reward\n",
    "            buffer[\"actions\"].append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                episode_return = sum(episode_rewards)\n",
    "                episode_length = len(episode_rewards)\n",
    "                buffer[\"ep_rets\"].append(episode_return)\n",
    "                buffer[\"ep_lens\"].append(episode_length)\n",
    "                # todo 1: calculate the gradient weights for the current episode and append them to buffer[\"weights\"]\n",
    "                # must be the same length as the number of samples collected in that episode\n",
    "                buffer[\"weights\"] += [episode_return] * episode_length\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                if use_random_init_state:\n",
    "                    state = non_terminal_states[\n",
    "                        np.random.choice(len(non_terminal_states))\n",
    "                    ]\n",
    "                else:\n",
    "                    state = mdp.initial_state\n",
    "                episode_rewards = []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(buffer[\"states\"]) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # compute the loss\n",
    "        # todo 2: calculate the objective to minimize (max J = min -J)\n",
    "        # you can convert lists to tensors using torch.as_tensor(list, dtype=torch.float32)\n",
    "        logp = pol.get_log_prob(\n",
    "            states=torch.as_tensor(buffer[\"states\"], dtype=torch.float32),\n",
    "            actions=torch.as_tensor(buffer[\"actions\"], dtype=torch.float32),\n",
    "        )\n",
    "        batch_loss = -(\n",
    "            logp * torch.as_tensor(buffer[\"weights\"], dtype=torch.float32)\n",
    "        ).mean()\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"iteration: %3d;  return: %.3f;  episode_length: %.3f\"\n",
    "                % (i, np.mean(buffer[\"ep_rets\"]), np.mean(buffer[\"ep_lens\"]))\n",
    "            )\n",
    "        if return_history:\n",
    "            model_checkpoints.append(deepcopy(pol))\n",
    "    if return_history:\n",
    "        return model_checkpoints\n",
    "    return pol\n",
    "\n",
    "\n",
    "def derive_deterministic_policy(mdp: MDP, pol: CategorialPolicy) -> Dict[Any, Any]:\n",
    "    \"\"\"Compute the best policy for an MDP given the stochastic policy.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        pol: The stochastic policy.\n",
    "\n",
    "    Returns:\n",
    "        Policy, i.e. mapping from state to action.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for state in mdp.get_states():\n",
    "        if mdp.is_terminal(state):\n",
    "            continue\n",
    "        pi[state] = pol.get_action(\n",
    "            state=torch.as_tensor(state, dtype=torch.float32), deterministic=True\n",
    "        )\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_policy_step_function(columns, rows, policy_over_time, show=True):\n",
    "    \"\"\"Create a function that allows plotting a policy over time.\"\"\"\n",
    "\n",
    "    def plot_grid_step(iteration):\n",
    "        data = policy_over_time[iteration]\n",
    "        for row in range(rows):\n",
    "            for col in range(columns):\n",
    "                if not (col, row) in data:\n",
    "                    continue\n",
    "                x = col + 0.5\n",
    "                y = row + 0.5\n",
    "                if data[(col, row)] is None:\n",
    "                    plt.scatter([x], [y], color=\"black\")\n",
    "                    continue\n",
    "                dx = data[(col, row)][0]\n",
    "                dy = data[(col, row)][1]\n",
    "                scaling = np.sqrt(dx**2.0 + dy**2.0) * 2.5\n",
    "                dx /= scaling\n",
    "                dy /= scaling\n",
    "                plt.arrow(\n",
    "                    x,\n",
    "                    y,\n",
    "                    dx,\n",
    "                    dy,\n",
    "                    shape=\"full\",\n",
    "                    lw=1.0,\n",
    "                    length_includes_head=True,\n",
    "                    head_width=0.15,\n",
    "                )\n",
    "        plt.axis(\"equal\")\n",
    "        plt.xlim([0, columns])\n",
    "        plt.ylim([0, rows])\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mdp = GridMDP(**GRID_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = CategorialPolicy(\n",
    "    sizes=[len(grid_mdp.initial_state), 32, len(grid_mdp.actions)],\n",
    "    actions=list(grid_mdp.actions),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = policy_gradient(\n",
    "    mdp=grid_mdp,\n",
    "    pol=pol,\n",
    "    iterations=100,\n",
    "    return_history=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_array = [\n",
    "    derive_deterministic_policy(mdp=grid_mdp, pol=model) for model in model_checkpoints\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_grid_map = make_plot_policy_step_function(\n",
    "    columns=4, rows=3, policy_over_time=policy_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdocs_flag = False\n",
    "if mkdocs_flag:\n",
    "    import ipywidgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(model_checkpoints) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_policy_step_grid_map, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_grid_map(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIGHWAY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # we will change this to true later on, to see the effect\n",
    "    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n",
    "        (0.4, LC_RIGHT_ACTION),\n",
    "        (0.6, STAY_IN_LANE_ACTION),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGHWAY_MDP_DICT[\"restrict_actions_to_available_states\"] = False\n",
    "highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = CategorialPolicy(\n",
    "    sizes=[len(highway_mdp.initial_state), 32, len(highway_mdp.actions)],\n",
    "    actions=list(highway_mdp.actions),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoints = policy_gradient(\n",
    "    mdp=highway_mdp,\n",
    "    pol=pol,\n",
    "    iterations=200,\n",
    "    return_history=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_array = [\n",
    "    derive_deterministic_policy(mdp=highway_mdp, pol=model)\n",
    "    for model in model_checkpoints\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_grid_map = make_plot_policy_step_function(\n",
    "    columns=10, rows=4, policy_over_time=policy_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    import ipywidgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(model_checkpoints) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_policy_step_grid_map, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_grid_map(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('kit_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bfcc22b8a9648f05d0968a7c788c3e5df76410378997fbd5b919d0d78d9143a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
