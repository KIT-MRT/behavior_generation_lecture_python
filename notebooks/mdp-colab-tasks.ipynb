{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src/behavior_generation_lecture_python/mdp/mdp.py\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SIMPLE_MDP_DICT = {\n",
    "    \"states\": [1, 2],\n",
    "    \"actions\": [\"A\", \"B\"],\n",
    "    \"initial_state\": 1,\n",
    "    \"terminal_states\": [2],\n",
    "    \"transition_probabilities\": {\n",
    "        (1, \"A\"): [(0.2, 1), (0.8, 2)],\n",
    "        (1, \"B\"): [(0.5, 1), (0.5, 2)],\n",
    "        (2, \"A\"): [(1.0, 1)],\n",
    "        (2, \"B\"): [(0.3, 1), (0.7, 2)],\n",
    "    },\n",
    "    \"reward\": {1: -0.1, 2: -0.5},\n",
    "}\n",
    "\n",
    "GRID_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [-0.04, -0.04, -0.04, +1],\n",
    "        [-0.04, None, -0.04, -1],\n",
    "        [-0.04, -0.04, -0.04, -0.04],\n",
    "    ],\n",
    "    \"initial_state\": (1, 0),\n",
    "    \"terminal_states\": {(3, 2), (3, 1)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        (0, 1): [(0.8, (0, 1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (0, -1): [(0.8, (0, -1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (1, 0): [(0.8, (1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "        (-1, 0): [(0.8, (-1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "    },\n",
    "}\n",
    "\n",
    "LC_LEFT_ACTION, STAY_IN_LANE_ACTION, LC_RIGHT_ACTION = (1, 1), (1, 0), (1, -1)\n",
    "\n",
    "HIGHWAY_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [0, -1, -1, -1, -1, -1, -1, -1, -1, -50],\n",
    "        [0, -2, -2, -2, -2, -2, -2, -2, -2, -50],\n",
    "        [0, -3, -3, -3, -3, -3, -3, -3, -3, -50],\n",
    "        [None, None, None, None, None, None, -2, -2, -2, 0],\n",
    "    ],\n",
    "    \"initial_state\": (0, 2),\n",
    "    \"terminal_states\": {(9, 3), (9, 1), (9, 2), (9, 0)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        STAY_IN_LANE_ACTION: [(1.0, STAY_IN_LANE_ACTION)],\n",
    "        LC_LEFT_ACTION: [(0.5, LC_LEFT_ACTION), (0.5, STAY_IN_LANE_ACTION)],\n",
    "        LC_RIGHT_ACTION: [(0.75, LC_RIGHT_ACTION), (0.25, STAY_IN_LANE_ACTION)],\n",
    "    },\n",
    "    \"restrict_actions_to_available_states\": True,\n",
    "}\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: Set[Any],\n",
    "        actions: Set[Any],\n",
    "        initial_state: Any,\n",
    "        terminal_states: Set[Any],\n",
    "        transition_probabilities: Dict[Tuple[Any, Any], List[Tuple[float, Any]]],\n",
    "        reward: Dict[Any, float],\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process.\n",
    "\n",
    "        Args:\n",
    "            states: Set of states.\n",
    "            actions: Set of actions.\n",
    "            initial_state: Initial state.\n",
    "            terminal_states: Set of terminal states.\n",
    "            transition_probabilities: Dictionary of transition\n",
    "                probabilities, mapping from tuple (state, action) to\n",
    "                list of tuples (probability, next state).\n",
    "            reward: Dictionary of rewards per state, mapping from state\n",
    "                to reward.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "\n",
    "        self.actions = actions\n",
    "\n",
    "        assert initial_state in self.states\n",
    "        self.initial_state = initial_state\n",
    "\n",
    "        for terminal_state in terminal_states:\n",
    "            assert (\n",
    "                terminal_state in self.states\n",
    "            ), f\"The terminal state {terminal_state} is not in states {states}\"\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                if (state, action) not in transition_probabilities:\n",
    "                    continue\n",
    "                total_prob = 0\n",
    "                for prob, next_state in transition_probabilities[(state, action)]:\n",
    "                    assert (\n",
    "                        next_state in self.states\n",
    "                    ), f\"next_state={next_state} is not in states={states}\"\n",
    "                    total_prob += prob\n",
    "                assert math.isclose(total_prob, 1), \"Probabilities must add to one\"\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "        assert set(reward.keys()) == set(\n",
    "            self.states\n",
    "        ), \"Rewards must be defined for every state in the set of states\"\n",
    "        for state in self.states:\n",
    "            assert reward[state] is not None\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_states(self) -> Set[Any]:\n",
    "        \"\"\"Get the set of states.\"\"\"\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state) -> Set[Any]:\n",
    "        \"\"\"Get the set of actions available in a certain state, returns [None] for terminal states.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return {None}\n",
    "        return set(\n",
    "            [a for a in self.actions if (state, a) in self.transition_probabilities]\n",
    "        )\n",
    "\n",
    "    def get_reward(self, state) -> float:\n",
    "        \"\"\"Get the reward for a specific state.\"\"\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def is_terminal(self, state) -> bool:\n",
    "        \"\"\"Return whether a state is a terminal state.\"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_transitions_with_probabilities(\n",
    "        self, state, action\n",
    "    ) -> List[Tuple[float, Any]]:\n",
    "        \"\"\"Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.\"\"\"\n",
    "        if action is None or self.is_terminal(state):\n",
    "            return [(0.0, state)]\n",
    "        return self.transition_probabilities[(state, action)]\n",
    "\n",
    "    def sample_next_state(self, state, action):\n",
    "        if self.is_terminal(state):\n",
    "            return ValueError(\"No next state for terminal states.\")\n",
    "        if action is None:\n",
    "            return ValueError(\"Action must not be None.\")\n",
    "        prob_per_transition = self.get_transitions_with_probabilities(state, action)\n",
    "        num_actions = len(prob_per_transition)\n",
    "        choice = np.random.choice(\n",
    "            num_actions, 1, p=[ppa[0] for ppa in prob_per_transition]\n",
    "        )\n",
    "        return prob_per_transition[int(choice)][1]\n",
    "\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid: List[List[Union[float, None]]],\n",
    "        initial_state: Tuple[int, int],\n",
    "        terminal_states: Set[Tuple[int, int]],\n",
    "        transition_probabilities_per_action: Dict[\n",
    "            Tuple[int, int], List[Tuple[float, Tuple[int, int]]]\n",
    "        ],\n",
    "        restrict_actions_to_available_states: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process on a grid.\n",
    "\n",
    "        Args:\n",
    "            grid: List of lists, containing the rewards of the grid\n",
    "                states or None.\n",
    "            initial_state: Initial state in the grid.\n",
    "            terminal_states: Set of terminal states in the grid.\n",
    "            transition_probabilities_per_action: Dictionary of\n",
    "                transition probabilities per action, mapping from action\n",
    "                to list of tuples (probability, next state).\n",
    "            restrict_actions_to_available_states: Whether to restrict\n",
    "                actions to those that result in valid next states.\n",
    "        \"\"\"\n",
    "        states = set()\n",
    "        reward = {}\n",
    "        grid = grid.copy()\n",
    "        grid.reverse()  # y-axis pointing upwards\n",
    "        rows = len(grid)\n",
    "        cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(cols):\n",
    "            for y in range(rows):\n",
    "                if grid[y][x] is not None:\n",
    "                    states.add((x, y))\n",
    "                    reward[(x, y)] = grid[y][x]\n",
    "\n",
    "        transition_probabilities = {}\n",
    "        for state in states:\n",
    "            for action in transition_probabilities_per_action.keys():\n",
    "                transition_probability_list = self._generate_transition_probability_list(\n",
    "                    state=state,\n",
    "                    action=action,\n",
    "                    restrict_actions_to_available_states=restrict_actions_to_available_states,\n",
    "                    states=states,\n",
    "                    transition_probabilities_per_action=transition_probabilities_per_action,\n",
    "                    next_state_fn=self._next_state_deterministic,\n",
    "                )\n",
    "                if transition_probability_list:\n",
    "                    transition_probabilities[\n",
    "                        (state, action)\n",
    "                    ] = transition_probability_list\n",
    "\n",
    "        super().__init__(\n",
    "            states=states,\n",
    "            actions=set(transition_probabilities_per_action.keys()),\n",
    "            initial_state=initial_state,\n",
    "            terminal_states=terminal_states,\n",
    "            transition_probabilities=transition_probabilities,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_transition_probability_list(\n",
    "        state,\n",
    "        action,\n",
    "        restrict_actions_to_available_states,\n",
    "        states,\n",
    "        transition_probabilities_per_action,\n",
    "        next_state_fn,\n",
    "    ):\n",
    "        \"\"\"Generate the transition probability list of the grid.\"\"\"\n",
    "        transition_probability_list = []\n",
    "        none_in_next_states = False\n",
    "        for (\n",
    "            probability,\n",
    "            deterministic_action,\n",
    "        ) in transition_probabilities_per_action[action]:\n",
    "            next_state = next_state_fn(\n",
    "                state,\n",
    "                deterministic_action,\n",
    "                states,\n",
    "                output_none_if_non_existing_state=restrict_actions_to_available_states,\n",
    "            )\n",
    "            if next_state is None:\n",
    "                none_in_next_states = True\n",
    "                break\n",
    "            transition_probability_list.append((probability, next_state))\n",
    "\n",
    "        if not none_in_next_states:\n",
    "            return transition_probability_list\n",
    "\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_state_deterministic(\n",
    "        state, action, states, output_none_if_non_existing_state=False\n",
    "    ):\n",
    "        \"\"\"Output the next state given the action in a deterministic setting.\n",
    "        Output None if next state not existing in case output_none_if_non_existing_state is True.\n",
    "        \"\"\"\n",
    "        next_state_candidate = tuple(np.array(state) + np.array(action))\n",
    "        if next_state_candidate in states:\n",
    "            return next_state_candidate\n",
    "        if output_none_if_non_existing_state:\n",
    "            return None\n",
    "        return state\n",
    "\n",
    "\n",
    "def expected_utility_of_action(\n",
    "    mdp: MDP, state: Any, action: Any, utility_of_states: Dict[Any, float]\n",
    ") -> float:\n",
    "    \"\"\"Compute the expected utility of taking an action in a state.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        state: The start state.\n",
    "        action: The action to be taken.\n",
    "        utility_of_states: The dictionary containing the utility\n",
    "            (estimate) of all states.\n",
    "\n",
    "    Returns:\n",
    "        Expected utility\n",
    "    \"\"\"\n",
    "    # todo: return ...\n",
    "\n",
    "\n",
    "def derive_policy(mdp: MDP, utility_of_states: Dict[Any, float]) -> Dict[Any, Any]:\n",
    "    \"\"\"Compute the best policy for an MDP given the utility of the states.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        utility_of_states: The dictionary containing the utility\n",
    "            (estimate) of all states.\n",
    "\n",
    "    Returns:\n",
    "        Policy, i.e. mapping from state to action.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for state in mdp.get_states():\n",
    "        pi[state] = max(\n",
    "            mdp.get_actions(state),\n",
    "            key=lambda action: expected_utility_of_action(\n",
    "                mdp=mdp, state=state, action=action, utility_of_states=utility_of_states\n",
    "            ),\n",
    "        )\n",
    "    return pi\n",
    "\n",
    "\n",
    "def value_iteration(\n",
    "    mdp: MDP,\n",
    "    epsilon: float,\n",
    "    max_iterations: int,\n",
    "    return_history: Optional[bool] = False,\n",
    ") -> Union[Dict[Any, float], List[Dict[Any, float]]]:\n",
    "    \"\"\"Derive a utility estimate by means of value iteration.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        epsilon: Termination criterion: if maximum difference in utility\n",
    "            update is below epsilon, the iteration is terminated.\n",
    "        max_iterations: Maximum number of iterations, if exceeded,\n",
    "            RuntimeError is raised.\n",
    "        return_history: Whether to return the whole history of utilities\n",
    "            instead of just the final estimate.\n",
    "\n",
    "    Returns:\n",
    "        The final utility estimate, if return_history is false. The\n",
    "        history of utility estimates as list, if return_history is true.\n",
    "    \"\"\"\n",
    "    utility = {state: 0 for state in mdp.get_states()}\n",
    "    utility_history = []\n",
    "    for _ in range(max_iterations):\n",
    "        utility_old = utility.copy()\n",
    "        max_delta = 0\n",
    "        for state in mdp.get_states():\n",
    "            # todo: utility[state] =\n",
    "            max_delta = max(max_delta, abs(utility[state] - utility_old[state]))\n",
    "        if return_history:\n",
    "            utility_history.append(utility.copy())\n",
    "        if max_delta < epsilon:\n",
    "            if return_history:\n",
    "                return utility_history\n",
    "            return utility\n",
    "    raise RuntimeError(f\"Did not converge in {max_iterations} iterations\")\n",
    "\n",
    "\n",
    "def best_action_from_q_table(state, available_actions, q_table):\n",
    "    available_actions = list(available_actions)\n",
    "    values = np.array([q_table[(state, action)] for action in available_actions])\n",
    "    action = available_actions[np.argmax(values)]\n",
    "    return action\n",
    "\n",
    "\n",
    "def random_action(available_actions):\n",
    "    available_actions = list(available_actions)\n",
    "    num_actions = len(available_actions)\n",
    "    choice = np.random.choice(num_actions, 1)\n",
    "    return available_actions[int(choice)]\n",
    "\n",
    "\n",
    "def greedy_estimate_for_state(q_table, state):\n",
    "    available_actions = [\n",
    "        state_action[1] for state_action in q_table.keys() if state_action[0] == state\n",
    "    ]\n",
    "    return max([q_table[(state, action)] for action in available_actions])\n",
    "\n",
    "\n",
    "def q_learning(\n",
    "    mdp: MDP,\n",
    "    alpha: float,\n",
    "    epsilon: float,\n",
    "    iterations: int,\n",
    "    return_history: Optional[bool] = False,\n",
    "):\n",
    "    q_table = {}\n",
    "    for state in mdp.get_states():\n",
    "        for action in mdp.get_actions(state):\n",
    "            q_table[(state, action)] = 0\n",
    "    q_table_history = []\n",
    "    state = mdp.initial_state\n",
    "\n",
    "    np.random.seed(1337)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        q_table_old = q_table.copy()\n",
    "\n",
    "        # available actions:\n",
    "        avail_actions = mdp.get_actions(state)\n",
    "\n",
    "        # todo: implement action choice: chosen_action = ...\n",
    "\n",
    "        # interact with the environment\n",
    "        next_state = mdp.sample_next_state(state, chosen_action)\n",
    "\n",
    "        # todo: update Q-table: q_table[(state, chosen_action)] = ...\n",
    "\n",
    "        if return_history:\n",
    "            q_table_history.append(q_table.copy())\n",
    "\n",
    "        if mdp.is_terminal(next_state):\n",
    "            state = mdp.initial_state  # restart\n",
    "        else:\n",
    "            state = next_state  # continue\n",
    "\n",
    "    if return_history:\n",
    "        utility_history = []\n",
    "        for q_tab in q_table_history:\n",
    "            utility_history.append(\n",
    "                {\n",
    "                    state: greedy_estimate_for_state(q_tab, state)\n",
    "                    for state in mdp.get_states()\n",
    "                }\n",
    "            )\n",
    "        return utility_history\n",
    "\n",
    "    return {\n",
    "        state: greedy_estimate_for_state(q_table, state) for state in mdp.get_states()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/aimacode/aima-python\n",
    "\"\"\"\n",
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2016 aima-python contributors\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def make_plot_grid_step_function(columns, rows, U_over_time, show=True):\n",
    "    \"\"\"ipywidgets interactive function supports single parameter as input.\n",
    "    This function creates and return such a function by taking as input\n",
    "    other parameters.\"\"\"\n",
    "\n",
    "    def plot_grid_step(iteration):\n",
    "        data = U_over_time[iteration]\n",
    "        data = defaultdict(lambda: 0, data)\n",
    "        grid = []\n",
    "        for row in range(rows):\n",
    "            current_row = []\n",
    "            for column in range(columns):\n",
    "                current_row.append(data[(column, row)])\n",
    "            grid.append(current_row)\n",
    "        grid.reverse()  # output like book\n",
    "        grid = [[-200 if y is None else y for y in x] for x in grid]\n",
    "        fig = plt.imshow(grid, cmap=plt.cm.bwr, interpolation=\"nearest\")\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "        for col in range(len(grid)):\n",
    "            for row in range(len(grid[0])):\n",
    "                magic = grid[col][row]\n",
    "                fig.axes.text(\n",
    "                    row, col, \"{0:.2f}\".format(magic), va=\"center\", ha=\"center\"\n",
    "                )\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_plot_policy_step_function(columns, rows, policy_over_time, show=True):\n",
    "    def plot_grid_step(iteration):\n",
    "        data = policy_over_time[iteration]\n",
    "        for row in range(rows):\n",
    "            for col in range(columns):\n",
    "                if not (col, row) in data:\n",
    "                    continue\n",
    "                x = col + 0.5\n",
    "                y = row + 0.5\n",
    "                if data[(col, row)] is None:\n",
    "                    plt.scatter([x], [y], color=\"black\")\n",
    "                    continue\n",
    "                dx = data[(col, row)][0]\n",
    "                dy = data[(col, row)][1]\n",
    "                scaling = np.sqrt(dx**2.0 + dy**2.0) * 2.5\n",
    "                dx /= scaling\n",
    "                dy /= scaling\n",
    "                plt.arrow(x, y, dx, dy)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.xlim([0, columns])\n",
    "        plt.ylim([0, rows])\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mdp = GridMDP(**GRID_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_utility_history = value_iteration(\n",
    "    mdp=grid_mdp, epsilon=0.001, max_iterations=30, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_grid_step = make_plot_grid_step_function(\n",
    "    columns=4, rows=3, U_over_time=computed_utility_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mkdocs_flag = True  # set to true if you are running the notebook locally\n",
    "if mkdocs_flag:\n",
    "    import ipywidgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(computed_utility_history) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_utility_history_q = q_learning(\n",
    "    mdp=grid_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_grid_step_q = make_plot_grid_step_function(\n",
    "    columns=4, rows=3, U_over_time=computed_utility_history_q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    import ipywidgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(computed_utility_history_q) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step_q, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIGHWAY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # we will change this to true later on, to see the effect\n",
    "    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n",
    "        (0.4, LC_RIGHT_ACTION),\n",
    "        (0.6, STAY_IN_LANE_ACTION),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_history_highway = value_iteration(\n",
    "    highway_mdp, epsilon=0.001, max_iterations=30, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step_highway = make_plot_grid_step_function(\n",
    "    columns=10, rows=4, U_over_time=utility_history_highway\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step_highway(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_array = [\n",
    "    derive_policy(highway_mdp, utility) for utility in utility_history_highway\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_highway = make_plot_policy_step_function(\n",
    "    columns=10, rows=4, policy_over_time=policy_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_highway(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_history_highway_q = q_learning(\n",
    "    highway_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step_highway_q = make_plot_grid_step_function(\n",
    "    columns=10, rows=4, U_over_time=utility_history_highway_q\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway_q) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step_highway_q, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
