{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_MDP_DICT = {\n",
    "    \"states\": [1, 2],\n",
    "    \"actions\": [\"A\", \"B\"],\n",
    "    \"initial_state\": 1,\n",
    "    \"terminal_states\": [2],\n",
    "    \"transition_probabilities\": {\n",
    "        (1, \"A\"): [(0.2, 1), (0.8, 2)],\n",
    "        (1, \"B\"): [(0.5, 1), (0.5, 2)],\n",
    "        (2, \"A\"): [(1.0, 1)],\n",
    "        (2, \"B\"): [(0.3, 1), (0.7, 2)],\n",
    "    },\n",
    "    \"reward\": {1: -0.1, 2: -0.5},\n",
    "}\n",
    "\n",
    "GRID_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [-0.04, -0.04, -0.04, +1],\n",
    "        [-0.04, None, -0.04, -1],\n",
    "        [-0.04, -0.04, -0.04, -0.04],\n",
    "    ],\n",
    "    \"initial_state\": (1, 0),\n",
    "    \"terminal_states\": {(3, 2), (3, 1)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        (0, 1): [(0.8, (0, 1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (0, -1): [(0.8, (0, -1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (1, 0): [(0.8, (1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "        (-1, 0): [(0.8, (-1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "    },\n",
    "}\n",
    "\n",
    "LC_LEFT_ACTION, STAY_IN_LANE_ACTION, LC_RIGHT_ACTION = (1, 1), (1, 0), (1, -1)\n",
    "\n",
    "HIGHWAY_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [0, -1, -1, -1, -1, -1, -1, -1, -1, -50],\n",
    "        [0, -2, -2, -2, -2, -2, -2, -2, -2, -50],\n",
    "        [0, -3, -3, -3, -3, -3, -3, -3, -3, -50],\n",
    "        [None, None, None, None, None, None, -2, -2, -2, 0],\n",
    "    ],\n",
    "    \"initial_state\": (0, 2),\n",
    "    \"terminal_states\": {(9, 3), (9, 1), (9, 2), (9, 0)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        STAY_IN_LANE_ACTION: [(1.0, STAY_IN_LANE_ACTION)],\n",
    "        LC_LEFT_ACTION: [(0.5, LC_LEFT_ACTION), (0.5, STAY_IN_LANE_ACTION)],\n",
    "        LC_RIGHT_ACTION: [(0.75, LC_RIGHT_ACTION), (0.25, STAY_IN_LANE_ACTION)],\n",
    "    },\n",
    "    \"restrict_actions_to_available_states\": True,\n",
    "}\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: Set[Any],\n",
    "        actions: Set[Any],\n",
    "        initial_state: Any,\n",
    "        terminal_states: Set[Any],\n",
    "        transition_probabilities: Dict[Tuple[Any, Any], List[Tuple[float, Any]]],\n",
    "        reward: Dict[Any, float],\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process.\n",
    "\n",
    "        Args:\n",
    "            states: Set of states.\n",
    "            actions: Set of actions.\n",
    "            initial_state: Initial state.\n",
    "            terminal_states: Set of terminal states.\n",
    "            transition_probabilities: Dictionary of transition\n",
    "                probabilities, mapping from tuple (state, action) to\n",
    "                list of tuples (probability, next state).\n",
    "            reward: Dictionary of rewards per state, mapping from state\n",
    "                to reward.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "\n",
    "        self.actions = actions\n",
    "\n",
    "        assert initial_state in self.states\n",
    "        self.initial_state = initial_state\n",
    "\n",
    "        for terminal_state in terminal_states:\n",
    "            assert (\n",
    "                terminal_state in self.states\n",
    "            ), f\"The terminal state {terminal_state} is not in states {states}\"\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                if (state, action) not in transition_probabilities:\n",
    "                    continue\n",
    "                total_prob = 0\n",
    "                for prob, next_state in transition_probabilities[(state, action)]:\n",
    "                    assert (\n",
    "                        next_state in self.states\n",
    "                    ), f\"next_state={next_state} is not in states={states}\"\n",
    "                    total_prob += prob\n",
    "                assert math.isclose(total_prob, 1), \"Probabilities must add to one\"\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "        assert set(reward.keys()) == set(\n",
    "            self.states\n",
    "        ), \"Rewards must be defined for every state in the set of states\"\n",
    "        for state in self.states:\n",
    "            assert reward[state] is not None\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_states(self) -> Set[Any]:\n",
    "        \"\"\"Get the set of states.\"\"\"\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state) -> Set[Any]:\n",
    "        \"\"\"Get the set of actions available in a certain state, returns [None] for terminal states.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return {None}\n",
    "        return set(\n",
    "            [a for a in self.actions if (state, a) in self.transition_probabilities]\n",
    "        )\n",
    "\n",
    "    def get_reward(self, state) -> float:\n",
    "        \"\"\"Get the reward for a specific state.\"\"\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def is_terminal(self, state) -> bool:\n",
    "        \"\"\"Return whether a state is a terminal state.\"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_transitions_with_probabilities(\n",
    "        self, state, action\n",
    "    ) -> List[Tuple[float, Any]]:\n",
    "        \"\"\"Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.\"\"\"\n",
    "        if action is None or self.is_terminal(state):\n",
    "            return [(0.0, state)]\n",
    "        return self.transition_probabilities[(state, action)]\n",
    "\n",
    "    def sample_next_state(self, state, action) -> Any:\n",
    "        \"\"\"Randomly sample the next state given the current state and taken action.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return ValueError(\"No next state for terminal states.\")\n",
    "        if action is None:\n",
    "            return ValueError(\"Action must not be None.\")\n",
    "        prob_per_transition = self.get_transitions_with_probabilities(state, action)\n",
    "        num_actions = len(prob_per_transition)\n",
    "        choice = np.random.choice(\n",
    "            num_actions, p=[ppa[0] for ppa in prob_per_transition]\n",
    "        )\n",
    "        return prob_per_transition[choice][1]\n",
    "\n",
    "    def execute_action(self, state, action) -> Tuple[Any, float, bool]:\n",
    "        \"\"\"Executes the action in the current state and returns the new state, obtained reward and terminal flag.\"\"\"\n",
    "        new_state = self.sample_next_state(state=state, action=action)\n",
    "        reward = self.get_reward(state=new_state)\n",
    "        terminal = self.is_terminal(state=new_state)\n",
    "        return new_state, reward, terminal\n",
    "\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid: List[List[Union[float, None]]],\n",
    "        initial_state: Tuple[int, int],\n",
    "        terminal_states: Set[Tuple[int, int]],\n",
    "        transition_probabilities_per_action: Dict[\n",
    "            Tuple[int, int], List[Tuple[float, Tuple[int, int]]]\n",
    "        ],\n",
    "        restrict_actions_to_available_states: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process on a grid.\n",
    "\n",
    "        Args:\n",
    "            grid: List of lists, containing the rewards of the grid\n",
    "                states or None.\n",
    "            initial_state: Initial state in the grid.\n",
    "            terminal_states: Set of terminal states in the grid.\n",
    "            transition_probabilities_per_action: Dictionary of\n",
    "                transition probabilities per action, mapping from action\n",
    "                to list of tuples (probability, next state).\n",
    "            restrict_actions_to_available_states: Whether to restrict\n",
    "                actions to those that result in valid next states.\n",
    "        \"\"\"\n",
    "        states = set()\n",
    "        reward = {}\n",
    "        grid = grid.copy()\n",
    "        grid.reverse()  # y-axis pointing upwards\n",
    "        rows = len(grid)\n",
    "        cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(cols):\n",
    "            for y in range(rows):\n",
    "                if grid[y][x] is not None:\n",
    "                    states.add((x, y))\n",
    "                    reward[(x, y)] = grid[y][x]\n",
    "\n",
    "        transition_probabilities = {}\n",
    "        for state in states:\n",
    "            for action in transition_probabilities_per_action.keys():\n",
    "                transition_probability_list = self._generate_transition_probability_list(\n",
    "                    state=state,\n",
    "                    action=action,\n",
    "                    restrict_actions_to_available_states=restrict_actions_to_available_states,\n",
    "                    states=states,\n",
    "                    transition_probabilities_per_action=transition_probabilities_per_action,\n",
    "                    next_state_fn=self._next_state_deterministic,\n",
    "                )\n",
    "                if transition_probability_list:\n",
    "                    transition_probabilities[\n",
    "                        (state, action)\n",
    "                    ] = transition_probability_list\n",
    "\n",
    "        super().__init__(\n",
    "            states=states,\n",
    "            actions=set(transition_probabilities_per_action.keys()),\n",
    "            initial_state=initial_state,\n",
    "            terminal_states=terminal_states,\n",
    "            transition_probabilities=transition_probabilities,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_transition_probability_list(\n",
    "        state,\n",
    "        action,\n",
    "        restrict_actions_to_available_states,\n",
    "        states,\n",
    "        transition_probabilities_per_action,\n",
    "        next_state_fn,\n",
    "    ):\n",
    "        \"\"\"Generate the transition probability list of the grid.\"\"\"\n",
    "        transition_probability_list = []\n",
    "        none_in_next_states = False\n",
    "        for (\n",
    "            probability,\n",
    "            deterministic_action,\n",
    "        ) in transition_probabilities_per_action[action]:\n",
    "            next_state = next_state_fn(\n",
    "                state,\n",
    "                deterministic_action,\n",
    "                states,\n",
    "                output_none_if_non_existing_state=restrict_actions_to_available_states,\n",
    "            )\n",
    "            if next_state is None:\n",
    "                none_in_next_states = True\n",
    "                break\n",
    "            transition_probability_list.append((probability, next_state))\n",
    "\n",
    "        if not none_in_next_states:\n",
    "            return transition_probability_list\n",
    "\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_state_deterministic(\n",
    "        state, action, states, output_none_if_non_existing_state=False\n",
    "    ):\n",
    "        \"\"\"Output the next state given the action in a deterministic setting.\n",
    "        Output None if next state not existing in case output_none_if_non_existing_state is True.\n",
    "        \"\"\"\n",
    "        next_state_candidate = tuple(np.array(state) + np.array(action))\n",
    "        if next_state_candidate in states:\n",
    "            return next_state_candidate\n",
    "        if output_none_if_non_existing_state:\n",
    "            return None\n",
    "        return state\n",
    "\n",
    "\n",
    "def expected_utility_of_action(\n",
    "    mdp: MDP, state: Any, action: Any, utility_of_states: Dict[Any, float]\n",
    ") -> float:\n",
    "    \"\"\"Compute the expected utility of taking an action in a state.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        state: The start state.\n",
    "        action: The action to be taken.\n",
    "        utility_of_states: The dictionary containing the utility\n",
    "            (estimate) of all states.\n",
    "\n",
    "    Returns:\n",
    "        Expected utility\n",
    "    \"\"\"\n",
    "    return sum(\n",
    "        p * (mdp.get_reward(next_state) + utility_of_states[next_state])\n",
    "        for (p, next_state) in mdp.get_transitions_with_probabilities(\n",
    "            state=state, action=action\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def derive_policy(mdp: MDP, utility_of_states: Dict[Any, float]) -> Dict[Any, Any]:\n",
    "    \"\"\"Compute the best policy for an MDP given the utility of the states.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        utility_of_states: The dictionary containing the utility\n",
    "            (estimate) of all states.\n",
    "\n",
    "    Returns:\n",
    "        Policy, i.e. mapping from state to action.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for state in mdp.get_states():\n",
    "        pi[state] = max(\n",
    "            mdp.get_actions(state),\n",
    "            key=lambda action: expected_utility_of_action(\n",
    "                mdp=mdp, state=state, action=action, utility_of_states=utility_of_states\n",
    "            ),\n",
    "        )\n",
    "    return pi\n",
    "\n",
    "\n",
    "def best_action_from_q_table(\n",
    "    *, state: Any, available_actions: Set[Any], q_table: Dict[Tuple[Any, Any], float]\n",
    ") -> Any:\n",
    "    \"\"\"Derive the best action from a Q table.\n",
    "\n",
    "    Args:\n",
    "        state: The state in which to take an action.\n",
    "        available_actions: Set of available actions.\n",
    "        q_table: The Q table, mapping from state-action pair to value estimate.\n",
    "\n",
    "    Returns:\n",
    "        The best action according to the Q table.\n",
    "    \"\"\"\n",
    "    available_actions = list(available_actions)\n",
    "    values = np.array([q_table[(state, action)] for action in available_actions])\n",
    "    action = available_actions[np.argmax(values)]\n",
    "    return action\n",
    "\n",
    "\n",
    "def random_action(available_actions: Set[Any]) -> Any:\n",
    "    \"\"\"Derive a random action from the set of available actions.\n",
    "\n",
    "    Args:\n",
    "        available_actions: Set of available actions.\n",
    "\n",
    "    Returns:\n",
    "        A random action.\n",
    "    \"\"\"\n",
    "    available_actions = list(available_actions)\n",
    "    num_actions = len(available_actions)\n",
    "    choice = np.random.choice(num_actions)\n",
    "    return available_actions[choice]\n",
    "\n",
    "\n",
    "def greedy_value_estimate_for_state(\n",
    "    *, q_table: Dict[Tuple[Any, Any], float], state: Any\n",
    ") -> float:\n",
    "    \"\"\"Compute the greedy (best possible) value estimate for a state from the Q table.\n",
    "\n",
    "    Args:\n",
    "        state: The state for which to estimate the value, when being greedy.\n",
    "        q_table: The Q table, mapping from state-action pair to value estimate.\n",
    "\n",
    "    Returns:\n",
    "        The value based on the greedy estimate.\n",
    "    \"\"\"\n",
    "    available_actions = [\n",
    "        state_action[1] for state_action in q_table.keys() if state_action[0] == state\n",
    "    ]\n",
    "    return max([q_table[(state, action)] for action in available_actions])\n",
    "\n",
    "\n",
    "def q_learning(\n",
    "    *,\n",
    "    mdp: MDP,\n",
    "    alpha: float,\n",
    "    epsilon: float,\n",
    "    iterations: int,\n",
    "    return_history: Optional[bool] = False,\n",
    ") -> Dict[Tuple[Any, Any], float]:\n",
    "    \"\"\"Derive a value estimate for state-action pairs by means of Q learning.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        alpha: Learning rate.\n",
    "        epsilon: Exploration-exploitation threshold. A random action is taken with\n",
    "            probability epsilon, the best action otherwise.\n",
    "        iterations: Number of iterations.\n",
    "        return_history: Whether to return the whole history of value estimates\n",
    "            instead of just the final estimate.\n",
    "\n",
    "    Returns:\n",
    "        The final value estimate, if return_history is false. The\n",
    "        history of value estimates as list, if return_history is true.\n",
    "    \"\"\"\n",
    "    q_table = {}\n",
    "    for state in mdp.get_states():\n",
    "        for action in mdp.get_actions(state):\n",
    "            q_table[(state, action)] = 0.0\n",
    "    q_table_history = [q_table.copy()]\n",
    "    state = mdp.initial_state\n",
    "\n",
    "    np.random.seed(1337)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "\n",
    "        # available actions:\n",
    "        avail_actions = mdp.get_actions(state)\n",
    "\n",
    "        # choose action (exploration-exploitation trade-off)\n",
    "        rand = np.random.random()\n",
    "        if rand < (1 - epsilon):\n",
    "            chosen_action = best_action_from_q_table(\n",
    "                state=state, available_actions=avail_actions, q_table=q_table\n",
    "            )\n",
    "        else:\n",
    "            chosen_action = random_action(avail_actions)\n",
    "\n",
    "        # interact with environment\n",
    "        next_state = mdp.sample_next_state(state, chosen_action)\n",
    "\n",
    "        # update Q table\n",
    "        greedy_value_estimate_next_state = greedy_value_estimate_for_state(\n",
    "            q_table=q_table, state=next_state\n",
    "        )\n",
    "        q_table[(state, chosen_action)] = (1 - alpha) * q_table[\n",
    "            (state, chosen_action)\n",
    "        ] + alpha * (mdp.get_reward(next_state) + greedy_value_estimate_next_state)\n",
    "\n",
    "        if return_history:\n",
    "            q_table_history.append(q_table.copy())\n",
    "\n",
    "        if mdp.is_terminal(next_state):\n",
    "            state = mdp.initial_state  # restart\n",
    "        else:\n",
    "            state = next_state  # continue\n",
    "\n",
    "    if return_history:\n",
    "        utility_history = []\n",
    "        for q_tab in q_table_history:\n",
    "            utility_history.append(\n",
    "                {\n",
    "                    state: greedy_value_estimate_for_state(q_table=q_tab, state=state)\n",
    "                    for state in mdp.get_states()\n",
    "                }\n",
    "            )\n",
    "        return utility_history\n",
    "\n",
    "    return {\n",
    "        state: greedy_value_estimate_for_state(q_table=q_table, state=state)\n",
    "        for state in mdp.get_states()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_grid_step_function(columns, rows, U_over_time, show=True):\n",
    "    \"\"\"ipywidgets interactive function supports single parameter as input.\n",
    "\n",
    "    This function creates and return such a function by taking as input\n",
    "    other parameters.\n",
    "\n",
    "    from https://github.com/aimacode/aima-python\n",
    "\n",
    "    The MIT License (MIT)\n",
    "\n",
    "    Copyright (c) 2016 aima-python contributors\n",
    "\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_grid_step(iteration):\n",
    "        data = U_over_time[iteration]\n",
    "        data = defaultdict(lambda: 0, data)\n",
    "        grid = []\n",
    "        for row in range(rows):\n",
    "            current_row = []\n",
    "            for column in range(columns):\n",
    "                current_row.append(data[(column, row)])\n",
    "            grid.append(current_row)\n",
    "        grid.reverse()  # output like book\n",
    "        grid = [[-200 if y is None else y for y in x] for x in grid]\n",
    "        fig = plt.imshow(grid, cmap=plt.cm.bwr, interpolation=\"nearest\")\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "        for col in range(len(grid)):\n",
    "            for row in range(len(grid[0])):\n",
    "                magic = grid[col][row]\n",
    "                fig.axes.text(\n",
    "                    row, col, \"{0:.2f}\".format(magic), va=\"center\", ha=\"center\"\n",
    "                )\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step\n",
    "\n",
    "\n",
    "def make_plot_policy_step_function(columns, rows, policy_over_time, show=True):\n",
    "    \"\"\"Create a function that allows plotting a policy over time.\"\"\"\n",
    "\n",
    "    def plot_grid_step(iteration):\n",
    "        data = policy_over_time[iteration]\n",
    "        for row in range(rows):\n",
    "            for col in range(columns):\n",
    "                if not (col, row) in data:\n",
    "                    continue\n",
    "                x = col + 0.5\n",
    "                y = row + 0.5\n",
    "                if data[(col, row)] is None:\n",
    "                    plt.scatter([x], [y], color=\"black\")\n",
    "                    continue\n",
    "                dx = data[(col, row)][0]\n",
    "                dy = data[(col, row)][1]\n",
    "                scaling = np.sqrt(dx**2.0 + dy**2.0) * 2.5\n",
    "                dx /= scaling\n",
    "                dy /= scaling\n",
    "                plt.arrow(\n",
    "                    x,\n",
    "                    y,\n",
    "                    dx,\n",
    "                    dy,\n",
    "                    shape=\"full\",\n",
    "                    lw=1.0,\n",
    "                    length_includes_head=True,\n",
    "                    head_width=0.15,\n",
    "                )\n",
    "        plt.axis(\"equal\")\n",
    "        plt.xlim([0, columns])\n",
    "        plt.ylim([0, rows])\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mdp = GridMDP(**GRID_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_utility_history = q_learning(\n",
    "    mdp=grid_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_grid_step = make_plot_grid_step_function(\n",
    "    columns=4, rows=3, U_over_time=computed_utility_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mkdocs_flag = False  # set to true if you are running the notebook locally\n",
    "if mkdocs_flag:\n",
    "    import ipywidgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(computed_utility_history) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIGHWAY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # we will change this to true later on, to see the effect\n",
    "    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n",
    "        (0.4, LC_RIGHT_ACTION),\n",
    "        (0.6, STAY_IN_LANE_ACTION),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_history_highway = q_learning(\n",
    "    mdp=highway_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step_highway = make_plot_grid_step_function(\n",
    "    columns=10, rows=4, U_over_time=utility_history_highway\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step_highway(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_array = [\n",
    "    derive_policy(highway_mdp, utility) for utility in utility_history_highway\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_highway = make_plot_policy_step_function(\n",
    "    columns=10, rows=4, policy_over_time=policy_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mkdocs_flag:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)\n",
    "    display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_highway(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('kit_tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bfcc22b8a9648f05d0968a7c788c3e5df76410378997fbd5b919d0d78d9143a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
