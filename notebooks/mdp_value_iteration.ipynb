{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\"\"\"This module contains the Markov Decision Process, value iteration, Q learning and policy gradient.\"\"\"\n",
    "\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"This module contains the CategoricalPolicy implementation.\"\"\"\n",
    "\n",
    "from typing import List, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def multi_layer_perceptron(\n",
    "    sizes: List[int],\n",
    "    activation: Type[nn.Module] = nn.ReLU,\n",
    "    output_activation: Type[nn.Module] = nn.Identity,\n",
    "):\n",
    "    \"\"\"Returns a multi-layer perceptron\"\"\"\n",
    "    mlp = nn.Sequential()\n",
    "    for i in range(len(sizes) - 1):\n",
    "        mlp.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        if i < len(sizes) - 2:\n",
    "            mlp.append(activation())\n",
    "        else:\n",
    "            mlp.append(output_activation())\n",
    "    return mlp\n",
    "\n",
    "\n",
    "class CategorialPolicy:\n",
    "    def __init__(self, sizes: List[int], actions: List):\n",
    "        assert sizes[-1] == len(actions)\n",
    "        torch.manual_seed(1337)\n",
    "        self.net = multi_layer_perceptron(sizes=sizes)\n",
    "        self.actions = actions\n",
    "        self._actions_tensor = torch.tensor(actions, dtype=torch.long).view(\n",
    "            len(actions), -1\n",
    "        )\n",
    "\n",
    "    def _get_distribution(self, state: torch.Tensor):\n",
    "        \"\"\"Calls the model and returns a categorial distribution over the actions.\"\"\"\n",
    "        logits = self.net(state)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False):\n",
    "        \"\"\"Returns an action sample for the given state\"\"\"\n",
    "        policy = self._get_distribution(state)\n",
    "        if deterministic:\n",
    "            return self.actions[policy.mode.item()]\n",
    "        return self.actions[policy.sample().item()]\n",
    "\n",
    "    def get_log_prob(self, states: torch.Tensor, actions: torch.Tensor):\n",
    "        \"\"\"Returns the log-probability for taking the action, when being the given state\"\"\"\n",
    "        return self._get_distribution(states).log_prob(\n",
    "            self._get_action_id_from_action(actions)\n",
    "        )\n",
    "\n",
    "    def _get_action_id_from_action(self, actions: torch.Tensor):\n",
    "        \"\"\"Returns the indices of the passed actions in self.actions\"\"\"\n",
    "        reshaped_actions = actions.unsqueeze(1).expand(\n",
    "            -1, self._actions_tensor.size(0), -1\n",
    "        )\n",
    "        reshaped_actions_tensor = self._actions_tensor.unsqueeze(0).expand(\n",
    "            actions.size(0), -1, -1\n",
    "        )\n",
    "        return torch.where(\n",
    "            torch.all(reshaped_actions == reshaped_actions_tensor, dim=-1)\n",
    "        )[1]\n",
    "\n",
    "SIMPLE_MDP_DICT = {\n",
    "    \"states\": [1, 2],\n",
    "    \"actions\": [\"A\", \"B\"],\n",
    "    \"initial_state\": 1,\n",
    "    \"terminal_states\": [2],\n",
    "    \"transition_probabilities\": {\n",
    "        (1, \"A\"): [(0.2, 1), (0.8, 2)],\n",
    "        (1, \"B\"): [(0.5, 1), (0.5, 2)],\n",
    "        (2, \"A\"): [(1.0, 1)],\n",
    "        (2, \"B\"): [(0.3, 1), (0.7, 2)],\n",
    "    },\n",
    "    \"reward\": {1: -0.1, 2: -0.5},\n",
    "}\n",
    "\n",
    "GRID_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [-0.04, -0.04, -0.04, +1],\n",
    "        [-0.04, None, -0.04, -1],\n",
    "        [-0.04, -0.04, -0.04, -0.04],\n",
    "    ],\n",
    "    \"initial_state\": (1, 0),\n",
    "    \"terminal_states\": {(3, 2), (3, 1)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        (0, 1): [(0.8, (0, 1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (0, -1): [(0.8, (0, -1)), (0.1, (1, 0)), (0.1, (-1, 0))],\n",
    "        (1, 0): [(0.8, (1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "        (-1, 0): [(0.8, (-1, 0)), (0.1, (0, 1)), (0.1, (0, -1))],\n",
    "    },\n",
    "}\n",
    "\n",
    "LC_LEFT_ACTION, STAY_IN_LANE_ACTION, LC_RIGHT_ACTION = (1, 1), (1, 0), (1, -1)\n",
    "\n",
    "HIGHWAY_MDP_DICT = {\n",
    "    \"grid\": [\n",
    "        [0, -1, -1, -1, -1, -1, -1, -1, -1, -50],\n",
    "        [0, -2, -2, -2, -2, -2, -2, -2, -2, -50],\n",
    "        [0, -3, -3, -3, -3, -3, -3, -3, -3, -50],\n",
    "        [None, None, None, None, None, None, -2, -2, -2, 0],\n",
    "    ],\n",
    "    \"initial_state\": (0, 2),\n",
    "    \"terminal_states\": {(9, 3), (9, 1), (9, 2), (9, 0)},\n",
    "    \"transition_probabilities_per_action\": {\n",
    "        STAY_IN_LANE_ACTION: [(1.0, STAY_IN_LANE_ACTION)],\n",
    "        LC_LEFT_ACTION: [(0.5, LC_LEFT_ACTION), (0.5, STAY_IN_LANE_ACTION)],\n",
    "        LC_RIGHT_ACTION: [(0.75, LC_RIGHT_ACTION), (0.25, STAY_IN_LANE_ACTION)],\n",
    "    },\n",
    "    \"restrict_actions_to_available_states\": True,\n",
    "}\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov decision process.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: Set[Any],\n",
    "        actions: Set[Any],\n",
    "        initial_state: Any,\n",
    "        terminal_states: Set[Any],\n",
    "        transition_probabilities: Dict[Tuple[Any, Any], List[Tuple[float, Any]]],\n",
    "        reward: Dict[Any, float],\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process.\n",
    "\n",
    "        Args:\n",
    "            states: Set of states.\n",
    "            actions: Set of actions.\n",
    "            initial_state: Initial state.\n",
    "            terminal_states: Set of terminal states.\n",
    "            transition_probabilities: Dictionary of transition\n",
    "                probabilities, mapping from tuple (state, action) to\n",
    "                list of tuples (probability, next state).\n",
    "            reward: Dictionary of rewards per state, mapping from state\n",
    "                to reward.\n",
    "        \"\"\"\n",
    "        self.states = states\n",
    "\n",
    "        self.actions = actions\n",
    "\n",
    "        assert initial_state in self.states\n",
    "        self.initial_state = initial_state\n",
    "\n",
    "        for terminal_state in terminal_states:\n",
    "            assert (\n",
    "                terminal_state in self.states\n",
    "            ), f\"The terminal state {terminal_state} is not in states {states}\"\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "        for state in self.states:\n",
    "            for action in self.actions:\n",
    "                if (state, action) not in transition_probabilities:\n",
    "                    continue\n",
    "                total_prob = 0.0\n",
    "                for prob, next_state in transition_probabilities[(state, action)]:\n",
    "                    assert (\n",
    "                        next_state in self.states\n",
    "                    ), f\"next_state={next_state} is not in states={states}\"\n",
    "                    total_prob += prob\n",
    "                assert math.isclose(total_prob, 1), \"Probabilities must add to one\"\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "        assert set(reward.keys()) == set(\n",
    "            self.states\n",
    "        ), \"Rewards must be defined for every state in the set of states\"\n",
    "        for state in self.states:\n",
    "            assert reward[state] is not None\n",
    "        self.reward = reward\n",
    "\n",
    "    def get_states(self) -> Set[Any]:\n",
    "        \"\"\"Get the set of states.\"\"\"\n",
    "        return self.states\n",
    "\n",
    "    def get_actions(self, state) -> Set[Any]:\n",
    "        \"\"\"Get the set of actions available in a certain state, returns [None] for terminal states.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return {None}\n",
    "        return {a for a in self.actions if (state, a) in self.transition_probabilities}\n",
    "\n",
    "    def get_reward(self, state) -> float:\n",
    "        \"\"\"Get the reward for a specific state.\"\"\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def is_terminal(self, state) -> bool:\n",
    "        \"\"\"Return whether a state is a terminal state.\"\"\"\n",
    "        return state in self.terminal_states\n",
    "\n",
    "    def get_transitions_with_probabilities(\n",
    "        self, state, action\n",
    "    ) -> List[Tuple[float, Any]]:\n",
    "        \"\"\"Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.\"\"\"\n",
    "        if action is None or self.is_terminal(state):\n",
    "            return [(0.0, state)]\n",
    "        return self.transition_probabilities[(state, action)]\n",
    "\n",
    "    def sample_next_state(self, state, action) -> Any:\n",
    "        \"\"\"Randomly sample the next state given the current state and taken action.\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return ValueError(\"No next state for terminal states.\")\n",
    "        if action is None:\n",
    "            return ValueError(\"Action must not be None.\")\n",
    "        prob_per_transition = self.get_transitions_with_probabilities(state, action)\n",
    "        num_actions = len(prob_per_transition)\n",
    "        choice = np.random.choice(\n",
    "            num_actions, p=[ppa[0] for ppa in prob_per_transition]\n",
    "        )\n",
    "        return prob_per_transition[choice][1]\n",
    "\n",
    "    def execute_action(self, state, action) -> Tuple[Any, float, bool]:\n",
    "        \"\"\"Executes the action in the current state and returns the new state, obtained reward and terminal flag.\"\"\"\n",
    "        new_state = self.sample_next_state(state=state, action=action)\n",
    "        reward = self.get_reward(state=new_state)\n",
    "        terminal = self.is_terminal(state=new_state)\n",
    "        return new_state, reward, terminal\n",
    "\n",
    "\n",
    "GridState = Tuple[int, int]\n",
    "\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    \"\"\"A Markov decision process on a grid.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        grid: List[List[Union[float, None]]],\n",
    "        initial_state: GridState,\n",
    "        terminal_states: Set[GridState],\n",
    "        transition_probabilities_per_action: Dict[\n",
    "            GridState, List[Tuple[float, GridState]]\n",
    "        ],\n",
    "        restrict_actions_to_available_states: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        \"\"\"A Markov decision process on a grid.\n",
    "\n",
    "        Args:\n",
    "            grid: List of lists, containing the rewards of the grid\n",
    "                states or None.\n",
    "            initial_state: Initial state in the grid.\n",
    "            terminal_states: Set of terminal states in the grid.\n",
    "            transition_probabilities_per_action: Dictionary of\n",
    "                transition probabilities per action, mapping from action\n",
    "                to list of tuples (probability, next state).\n",
    "            restrict_actions_to_available_states: Whether to restrict\n",
    "                actions to those that result in valid next states.\n",
    "        \"\"\"\n",
    "        states = set()\n",
    "        reward = {}\n",
    "        grid = grid.copy()\n",
    "        grid.reverse()  # y-axis pointing upwards\n",
    "        rows = len(grid)\n",
    "        cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(cols):\n",
    "            for y in range(rows):\n",
    "                if grid[y][x] is not None:\n",
    "                    states.add((x, y))\n",
    "                    reward_xy = grid[y][x]\n",
    "                    assert reward_xy is not None\n",
    "                    reward[(x, y)] = reward_xy\n",
    "\n",
    "        transition_probabilities = {}\n",
    "        for state in states:\n",
    "            for action in transition_probabilities_per_action.keys():\n",
    "                transition_probability_list = self._generate_transition_probability_list(\n",
    "                    state=state,\n",
    "                    action=action,\n",
    "                    restrict_actions_to_available_states=restrict_actions_to_available_states,\n",
    "                    states=states,\n",
    "                    transition_probabilities_per_action=transition_probabilities_per_action,\n",
    "                    next_state_fn=self._next_state_deterministic,\n",
    "                )\n",
    "                if transition_probability_list:\n",
    "                    transition_probabilities[(state, action)] = (\n",
    "                        transition_probability_list\n",
    "                    )\n",
    "\n",
    "        super().__init__(\n",
    "            states=states,\n",
    "            actions=set(transition_probabilities_per_action.keys()),\n",
    "            initial_state=initial_state,\n",
    "            terminal_states=terminal_states,\n",
    "            transition_probabilities=transition_probabilities,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_transition_probability_list(\n",
    "        state,\n",
    "        action,\n",
    "        restrict_actions_to_available_states,\n",
    "        states,\n",
    "        transition_probabilities_per_action,\n",
    "        next_state_fn,\n",
    "    ):\n",
    "        \"\"\"Generate the transition probability list of the grid.\"\"\"\n",
    "        transition_probability_list = []\n",
    "        none_in_next_states = False\n",
    "        for (\n",
    "            probability,\n",
    "            deterministic_action,\n",
    "        ) in transition_probabilities_per_action[action]:\n",
    "            next_state = next_state_fn(\n",
    "                state,\n",
    "                deterministic_action,\n",
    "                states,\n",
    "                output_none_if_non_existing_state=restrict_actions_to_available_states,\n",
    "            )\n",
    "            if next_state is None:\n",
    "                none_in_next_states = True\n",
    "                break\n",
    "            transition_probability_list.append((probability, next_state))\n",
    "\n",
    "        if not none_in_next_states:\n",
    "            return transition_probability_list\n",
    "\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_state_deterministic(\n",
    "        state, action, states, output_none_if_non_existing_state=False\n",
    "    ):\n",
    "        \"\"\"Output the next state given the action in a deterministic setting.\n",
    "        Output None if next state not existing in case output_none_if_non_existing_state is True.\n",
    "        \"\"\"\n",
    "        next_state_candidate = tuple(np.array(state) + np.array(action))\n",
    "        if next_state_candidate in states:\n",
    "            return next_state_candidate\n",
    "        if output_none_if_non_existing_state:\n",
    "            return None\n",
    "        return state\n",
    "\n",
    "\n",
    "StateValueTable = Dict[Any, float]\n",
    "\n",
    "\n",
    "def expected_utility_of_action(\n",
    "    mdp: MDP, state: Any, action: Any, utility_of_states: StateValueTable\n",
    ") -> float:\n",
    "    \"\"\"Compute the expected utility of taking an action in a state.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        state: The start state.\n",
    "        action: The action to be taken.\n",
    "        utility_of_states: The dictionary containing the utility\n",
    "            (estimate) of all states.\n",
    "\n",
    "    Returns:\n",
    "        Expected utility\n",
    "    \"\"\"\n",
    "    return sum(\n",
    "        # todo 1: what to sum over?\n",
    "        for (p, next_state) in mdp.get_transitions_with_probabilities(\n",
    "            state=state, action=action\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def derive_policy(mdp: MDP, utility_of_states: StateValueTable) -> Dict[Any, Any]:\n",
    "    \"\"\"Compute the best policy for an MDP given the utility of the states.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        utility_of_states: The dictionary containing the utility\n",
    "            (estimate) of all states.\n",
    "\n",
    "    Returns:\n",
    "        Policy, i.e. mapping from state to action.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for state in mdp.get_states():\n",
    "        pi[state] = max(\n",
    "            mdp.get_actions(state),\n",
    "            key=lambda action: expected_utility_of_action(\n",
    "                mdp=mdp, state=state, action=action, utility_of_states=utility_of_states\n",
    "            ),\n",
    "        )\n",
    "    return pi\n",
    "\n",
    "\n",
    "def value_iteration(\n",
    "    mdp: MDP,\n",
    "    epsilon: float,\n",
    "    max_iterations: int,\n",
    "    return_history: Optional[bool] = False,\n",
    ") -> Union[StateValueTable, List[StateValueTable]]:\n",
    "    \"\"\"Derive a utility estimate by means of value iteration.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        epsilon: Termination criterion: if maximum difference in utility\n",
    "            update is below epsilon, the iteration is terminated.\n",
    "        max_iterations: Maximum number of iterations, if exceeded,\n",
    "            RuntimeError is raised.\n",
    "        return_history: Whether to return the whole history of utilities\n",
    "            instead of just the final estimate.\n",
    "\n",
    "    Returns:\n",
    "        The final utility estimate, if return_history is false. The\n",
    "        history of utility estimates as list, if return_history is true.\n",
    "    \"\"\"\n",
    "    utility = {state: 0.0 for state in mdp.get_states()}\n",
    "    utility_history = [utility.copy()]\n",
    "    for _ in range(max_iterations):\n",
    "        utility_old = utility.copy()\n",
    "        max_delta = 0.0\n",
    "        for state in mdp.get_states():\n",
    "            # todo 2: compute utility[state] using expected_utility_of_action\n",
    "            max_delta = max(max_delta, abs(utility[state] - utility_old[state]))\n",
    "        if return_history:\n",
    "            utility_history.append(utility.copy())\n",
    "        if max_delta < epsilon:\n",
    "            if return_history:\n",
    "                return utility_history\n",
    "            return utility\n",
    "    raise RuntimeError(f\"Did not converge in {max_iterations} iterations\")\n",
    "\n",
    "\n",
    "QTable = Dict[Tuple[Any, Any], float]\n",
    "\n",
    "\n",
    "def best_action_from_q_table(\n",
    "    *, state: Any, available_actions: Set[Any], q_table: QTable\n",
    ") -> Any:\n",
    "    \"\"\"Derive the best action from a Q table.\n",
    "\n",
    "    Args:\n",
    "        state: The state in which to take an action.\n",
    "        available_actions: Set of available actions.\n",
    "        q_table: The Q table, mapping from state-action pair to value estimate.\n",
    "\n",
    "    Returns:\n",
    "        The best action according to the Q table.\n",
    "    \"\"\"\n",
    "    available_actions_list = list(available_actions)\n",
    "    values = np.array([q_table[(state, action)] for action in available_actions_list])\n",
    "    action = available_actions_list[np.argmax(values)]\n",
    "    return action\n",
    "\n",
    "\n",
    "def random_action(available_actions: Set[Any]) -> Any:\n",
    "    \"\"\"Derive a random action from the set of available actions.\n",
    "\n",
    "    Args:\n",
    "        available_actions: Set of available actions.\n",
    "\n",
    "    Returns:\n",
    "        A random action.\n",
    "    \"\"\"\n",
    "    available_actions_list = list(available_actions)\n",
    "    num_actions = len(available_actions_list)\n",
    "    choice = np.random.choice(num_actions)\n",
    "    return available_actions_list[choice]\n",
    "\n",
    "\n",
    "def greedy_value_estimate_for_state(*, q_table: QTable, state: Any) -> float:\n",
    "    \"\"\"Compute the greedy (best possible) value estimate for a state from the Q table.\n",
    "\n",
    "    Args:\n",
    "        state: The state for which to estimate the value, when being greedy.\n",
    "        q_table: The Q table, mapping from state-action pair to value estimate.\n",
    "\n",
    "    Returns:\n",
    "        The value based on the greedy estimate.\n",
    "    \"\"\"\n",
    "    available_actions = [\n",
    "        state_action[1] for state_action in q_table.keys() if state_action[0] == state\n",
    "    ]\n",
    "    return max(q_table[state, action] for action in available_actions)\n",
    "\n",
    "\n",
    "def q_learning(\n",
    "    *,\n",
    "    mdp: MDP,\n",
    "    alpha: float,\n",
    "    epsilon: float,\n",
    "    iterations: int,\n",
    "    return_history: Optional[bool] = False,\n",
    ") -> Union[QTable, List[QTable]]:\n",
    "    \"\"\"Derive a value estimate for state-action pairs by means of Q learning.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        alpha: Learning rate.\n",
    "        epsilon: Exploration-exploitation threshold. A random action is taken with\n",
    "            probability epsilon, the best action otherwise.\n",
    "        iterations: Number of iterations.\n",
    "        return_history: Whether to return the whole history of value estimates\n",
    "            instead of just the final estimate.\n",
    "\n",
    "    Returns:\n",
    "        The final value estimate, if return_history is false. The\n",
    "        history of value estimates as list, if return_history is true.\n",
    "    \"\"\"\n",
    "    q_table = {}\n",
    "    for state in mdp.get_states():\n",
    "        for action in mdp.get_actions(state):\n",
    "            q_table[(state, action)] = 0.0\n",
    "    q_table_history = [q_table.copy()]\n",
    "    state = mdp.initial_state\n",
    "\n",
    "    np.random.seed(1337)\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # available actions:\n",
    "        avail_actions = mdp.get_actions(state)\n",
    "\n",
    "        # choose action (exploration-exploitation trade-off)\n",
    "        rand = np.random.random()\n",
    "        if rand < (1 - epsilon):\n",
    "            chosen_action = best_action_from_q_table(\n",
    "                state=state, available_actions=avail_actions, q_table=q_table\n",
    "            )\n",
    "        else:\n",
    "            chosen_action = random_action(avail_actions)\n",
    "\n",
    "        # interact with environment\n",
    "        next_state = mdp.sample_next_state(state, chosen_action)\n",
    "\n",
    "        # update Q table\n",
    "        greedy_value_estimate_next_state = greedy_value_estimate_for_state(\n",
    "            q_table=q_table, state=next_state\n",
    "        )\n",
    "        q_table[(state, chosen_action)] = (1 - alpha) * q_table[\n",
    "            (state, chosen_action)\n",
    "        ] + alpha * (mdp.get_reward(next_state) + greedy_value_estimate_next_state)\n",
    "\n",
    "        if return_history:\n",
    "            q_table_history.append(q_table.copy())\n",
    "\n",
    "        if mdp.is_terminal(next_state):\n",
    "            state = mdp.initial_state  # restart\n",
    "        else:\n",
    "            state = next_state  # continue\n",
    "\n",
    "    if return_history:\n",
    "        utility_history = []\n",
    "        for q_tab in q_table_history:\n",
    "            utility_history.append(\n",
    "                {\n",
    "                    state: greedy_value_estimate_for_state(q_table=q_tab, state=state)\n",
    "                    for state in mdp.get_states()\n",
    "                }\n",
    "            )\n",
    "        return utility_history\n",
    "\n",
    "    return {\n",
    "        state: greedy_value_estimate_for_state(q_table=q_table, state=state)\n",
    "        for state in mdp.get_states()\n",
    "    }\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PolicyGradientBuffer:\n",
    "    \"\"\"Buffer for the policy gradient method.\"\"\"\n",
    "\n",
    "    states: List[Any] = field(default_factory=list)\n",
    "    actions: List[Any] = field(default_factory=list)\n",
    "    weights: List[float] = field(default_factory=list)\n",
    "    episode_returns: List[float] = field(default_factory=list)\n",
    "    episode_lengths: List[int] = field(default_factory=list)\n",
    "\n",
    "    def mean_episode_return(self) -> float:\n",
    "        \"\"\"Mean episode return.\"\"\"\n",
    "        return float(np.mean(self.episode_returns))\n",
    "\n",
    "    def mean_episode_length(self) -> float:\n",
    "        \"\"\"Mean episode length.\"\"\"\n",
    "        return float(np.mean(self.episode_lengths))\n",
    "\n",
    "\n",
    "def policy_gradient(\n",
    "    *,\n",
    "    mdp: MDP,\n",
    "    policy: CategorialPolicy,\n",
    "    lr: float = 1e-2,\n",
    "    iterations: int = 50,\n",
    "    batch_size: int = 5000,\n",
    "    return_history: bool = False,\n",
    "    use_random_init_state: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Union[List[CategorialPolicy], CategorialPolicy]:\n",
    "    \"\"\"Train a paramterized policy using vanilla policy gradient.\n",
    "\n",
    "    Adapted from: https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py\n",
    "\n",
    "    The MIT License (MIT)\n",
    "\n",
    "    Copyright (c) 2018 OpenAI (http://openai.com)\n",
    "\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        policy: The stochastic policy to be trained.\n",
    "        lr: Learning rate.\n",
    "        iterations: Number of iterations.\n",
    "        batch_size: Number of samples generated for each policy update.\n",
    "        return_history: Whether to return the whole history of value estimates\n",
    "            instead of just the final estimate.\n",
    "        use_random_init_state: bool, if the agent should be initialized randomly.\n",
    "        verbose: bool, if traing progress should be printed.\n",
    "\n",
    "    Returns:\n",
    "        The final policy, if return_history is false. The\n",
    "        history of policies as list, if return_history is true.\n",
    "    \"\"\"\n",
    "    np.random.seed(1337)\n",
    "    torch.manual_seed(1337)\n",
    "\n",
    "    # add untrained model to model_checkpoints\n",
    "    model_checkpoints = [deepcopy(policy)]\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = torch.optim.Adam(policy.net.parameters(), lr=lr)\n",
    "\n",
    "    # get non-terminal states\n",
    "    non_terminal_states = [state for state in mdp.states if not mdp.is_terminal(state)]\n",
    "\n",
    "    # training loop\n",
    "    for i in range(1, iterations + 1):\n",
    "        # a buffer for storing intermediate values\n",
    "        buffer = PolicyGradientBuffer()\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        if use_random_init_state:\n",
    "            state = non_terminal_states[np.random.choice(len(non_terminal_states))]\n",
    "        else:\n",
    "            state = mdp.initial_state\n",
    "        episode_rewards = []\n",
    "\n",
    "        # collect experience by acting in the mdp\n",
    "        while True:\n",
    "            # save visited state\n",
    "            buffer.states.append(deepcopy(state))\n",
    "\n",
    "            # call model to get next action\n",
    "            action = policy.get_action(state=torch.tensor(state, dtype=torch.float32))\n",
    "\n",
    "            # execute action in the environment\n",
    "            state, reward, done = mdp.execute_action(state=state, action=action)\n",
    "\n",
    "            # save action, reward\n",
    "            buffer.actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                episode_return = sum(episode_rewards)\n",
    "                episode_length = len(episode_rewards)\n",
    "                buffer.episode_returns.append(episode_return)\n",
    "                buffer.episode_lengths.append(episode_length)\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                buffer.weights += [episode_return] * episode_length\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                if use_random_init_state:\n",
    "                    state = non_terminal_states[\n",
    "                        np.random.choice(len(non_terminal_states))\n",
    "                    ]\n",
    "                else:\n",
    "                    state = mdp.initial_state\n",
    "                episode_rewards = []\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(buffer.states) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # compute the loss\n",
    "        logp = policy.get_log_prob(\n",
    "            states=torch.tensor(buffer.states, dtype=torch.float),\n",
    "            actions=torch.tensor(buffer.actions, dtype=torch.long),\n",
    "        )\n",
    "        batch_loss = -(logp * torch.tensor(buffer.weights, dtype=torch.float)).mean()\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"iteration: {i:3d};  return: {buffer.mean_episode_return():.3f};  episode_length: {buffer.mean_episode_length():.3f}\"\n",
    "            )\n",
    "        if return_history:\n",
    "            model_checkpoints.append(deepcopy(policy))\n",
    "    if return_history:\n",
    "        return model_checkpoints\n",
    "    return policy\n",
    "\n",
    "\n",
    "def derive_deterministic_policy(mdp: MDP, policy: CategorialPolicy) -> Dict[Any, Any]:\n",
    "    \"\"\"Compute the best policy for an MDP given the stochastic policy.\n",
    "\n",
    "    Args:\n",
    "        mdp: The underlying MDP.\n",
    "        policy: The stochastic policy.\n",
    "\n",
    "    Returns:\n",
    "        Deterministic policy, i.e. mapping from state to action.\n",
    "    \"\"\"\n",
    "    pi = {}\n",
    "    for state in mdp.get_states():\n",
    "        if mdp.is_terminal(state):\n",
    "            continue\n",
    "        pi[state] = policy.get_action(\n",
    "            state=torch.as_tensor(state, dtype=torch.float32), deterministic=True\n",
    "        )\n",
    "    return pi\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_plot_grid_step_function(columns, rows, U_over_time, show=True):\n",
    "    \"\"\"ipywidgets interactive function supports single parameter as input.\n",
    "\n",
    "    This function creates and return such a function by taking as input\n",
    "    other parameters.\n",
    "\n",
    "    from https://github.com/aimacode/aima-python\n",
    "\n",
    "    The MIT License (MIT)\n",
    "\n",
    "    Copyright (c) 2016 aima-python contributors\n",
    "\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "    \"\"\"\n",
    "\n",
    "    def plot_grid_step(iteration):\n",
    "        data = U_over_time[iteration]\n",
    "        data = defaultdict(lambda: 0, data)\n",
    "        grid = []\n",
    "        for row in range(rows):\n",
    "            current_row = []\n",
    "            for column in range(columns):\n",
    "                current_row.append(data[(column, row)])\n",
    "            grid.append(current_row)\n",
    "        grid.reverse()  # output like book\n",
    "        grid = [[-200 if y is None else y for y in x] for x in grid]\n",
    "        fig = plt.imshow(grid, cmap=plt.cm.bwr, interpolation=\"nearest\")\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "        for col in range(len(grid)):\n",
    "            for row in range(len(grid[0])):\n",
    "                magic = grid[col][row]\n",
    "                fig.axes.text(\n",
    "                    row, col, \"{0:.2f}\".format(magic), va=\"center\", ha=\"center\"\n",
    "                )\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step\n",
    "\n",
    "\n",
    "def make_plot_policy_step_function(columns, rows, policy_over_time, show=True):\n",
    "    \"\"\"Create a function that allows plotting a policy over time.\"\"\"\n",
    "\n",
    "    def plot_grid_step(iteration):\n",
    "        data = policy_over_time[iteration]\n",
    "        for row in range(rows):\n",
    "            for col in range(columns):\n",
    "                if (col, row) not in data:\n",
    "                    continue\n",
    "                x = col + 0.5\n",
    "                y = row + 0.5\n",
    "                if data[(col, row)] is None:\n",
    "                    plt.scatter([x], [y], color=\"black\")\n",
    "                    continue\n",
    "                dx = data[(col, row)][0]\n",
    "                dy = data[(col, row)][1]\n",
    "                scaling = np.sqrt(dx**2.0 + dy**2.0) * 2.5\n",
    "                dx /= scaling\n",
    "                dy /= scaling\n",
    "                plt.arrow(\n",
    "                    x,\n",
    "                    y,\n",
    "                    dx,\n",
    "                    dy,\n",
    "                    shape=\"full\",\n",
    "                    lw=1.0,\n",
    "                    length_includes_head=True,\n",
    "                    head_width=0.15,\n",
    "                )\n",
    "        plt.axis(\"equal\")\n",
    "        plt.xlim([0, columns])\n",
    "        plt.ylim([0, rows])\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    return plot_grid_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_mdp = GridMDP(**GRID_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_utility_history = value_iteration(\n",
    "    mdp=grid_mdp, epsilon=0.001, max_iterations=30, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_grid_step = make_plot_grid_step_function(\n",
    "    columns=4, rows=3, U_over_time=computed_utility_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861edd212721459f8b20a4b17dee913f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=19), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI\n",
    "if interactive_widgets:\n",
    "    import ipywidgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(computed_utility_history) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)\n",
    "    display(w)\n",
    "else:\n",
    "    plot_grid_step(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIGHWAY EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # we will change this to true later on, to see the effect\n",
    "    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n",
    "        (0.4, LC_RIGHT_ACTION),\n",
    "        (0.6, STAY_IN_LANE_ACTION),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_history_highway = value_iteration(\n",
    "    highway_mdp, epsilon=0.001, max_iterations=30, return_history=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_step_highway = make_plot_grid_step_function(\n",
    "    columns=10, rows=4, U_over_time=utility_history_highway\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c360d87138a94b7bbf7cd83d3c27437d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=10), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if interactive_widgets:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)\n",
    "    display(w)\n",
    "else:\n",
    "    plot_grid_step_highway(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_array = [\n",
    "    derive_policy(highway_mdp, utility) for utility in utility_history_highway\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_step_highway = make_plot_policy_step_function(\n",
    "    columns=10, rows=4, policy_over_time=policy_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ae3b1d8cd14a0c97495b28783c12bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='iteration', max=10), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if interactive_widgets:\n",
    "    iteration_slider = ipywidgets.IntSlider(\n",
    "        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n",
    "    )\n",
    "    w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)\n",
    "    display(w)\n",
    "else:\n",
    "    plot_policy_step_highway(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "behavior_generation_lecture",
   "language": "python",
   "name": "behavior_generation_lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
