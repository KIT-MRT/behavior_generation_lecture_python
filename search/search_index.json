{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python Code for the Lecture Decision-Making and Motion Planning for Automated Driving at KIT","text":"<p>This repository contains the python code for the lecture Decision-Making and Motion Planning for Automated Driving at KIT. It is targeted towards both, exemplifying the content of the lecture, and giving a brief introduction to software development. (Please bare with us, the code is largely ported from matlab.)</p> <p>An API documentation for new parts of the code and exemplary jupyter notebooks can be found in the documentation.</p>"},{"location":"#preparing-the-environment","title":"Preparing the environment","text":"<p>We use <code>uv</code> as package and project manager. Having <code>uv</code> installed, run</p> <pre><code># clone this repo\ngit clone https://github.com/KIT-MRT/behavior_generation_lecture_python.git\n\n# change into the repo folder\ncd behavior_generation_lecture_python\n\n# set up a virtual env and install the requirements\nuv sync\n</code></pre> Making uv kernels available to jupyter? <ul> <li>create a kernel <code>uv run ipython kernel install --user --name=behavior_generation_lecture</code></li> <li>run jupyter <code>uv run --with jupyter jupyter lab</code> and chose kernel <code>behavior_generation_lecture</code> in the browser"},{"location":"#structure","title":"Structure","text":"<p>The structure of this repo is inspired by the PyPA sample project.</p> <ul> <li><code>src</code> contains the modules, which is the core implementation, at best browsed in your favorite IDE</li> <li><code>tests</code> contains unittests, at best browsed in your favorite IDE</li> <li><code>scripts</code> contains scripts that depict exemplary usage of the implemented modules, they can be run from the command line</li> <li><code>notebooks</code> contains jupyter notebooks, that can be browsed online, and interactively be run using jupyter</li> </ul>"},{"location":"#contribution","title":"Contribution","text":"<p>Feel free to open an issue if you found a bug or have a request.  You can also contribute to the lecture code yourself: Just fork this repository and open a pull request.</p>"},{"location":"#license","title":"License","text":"<p>Unless otherwise stated, this repo is distributed under the 3-Clause BSD License, see LICENSE.</p>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[1]: Copied! <pre>\"\"\"Generate the code reference pages and navigation.\"\"\"\n</pre> \"\"\"Generate the code reference pages and navigation.\"\"\" Out[1]: <pre>'Generate the code reference pages and navigation.'</pre> In\u00a0[2]: Copied! <pre>\"\"\"\nModified from https://github.com/mkdocstrings/mkdocstrings/blob/master/docs/recipes.md\nISC License, Copyright (c) 2019, Timoth\u00e9e Mazzucotelli\n\"\"\"\n</pre> \"\"\" Modified from https://github.com/mkdocstrings/mkdocstrings/blob/master/docs/recipes.md ISC License, Copyright (c) 2019, Timoth\u00e9e Mazzucotelli \"\"\" Out[2]: <pre>'\\nModified from https://github.com/mkdocstrings/mkdocstrings/blob/master/docs/recipes.md\\nISC License, Copyright (c) 2019, Timoth\u00e9e Mazzucotelli\\n'</pre> In\u00a0[3]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[4]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[5]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[6]: Copied! <pre>for path in sorted(Path(\"src\").rglob(\"*.py\")):\n    module_path = path.relative_to(\"src\").with_suffix(\"\")\n    doc_path = path.relative_to(\"src\").with_suffix(\".md\")\n    full_doc_path = Path(\"reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n    if \"mdp\" not in parts and \"graph_search\" not in parts:\n        continue  # todo: add other modules here once docstrings added\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = str(doc_path)\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        fd.write(f\"::: {ident}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\"src\").rglob(\"*.py\")):     module_path = path.relative_to(\"src\").with_suffix(\"\")     doc_path = path.relative_to(\"src\").with_suffix(\".md\")     full_doc_path = Path(\"reference\", doc_path)      parts = tuple(module_path.parts)     if \"mdp\" not in parts and \"graph_search\" not in parts:         continue  # todo: add other modules here once docstrings added      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")     elif parts[-1] == \"__main__\":         continue      nav[parts] = str(doc_path)      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         fd.write(f\"::: {ident}\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[7]: Copied! <pre>with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"notebooks/a_star_notebook/","title":"Graph Search","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nfrom behavior_generation_lecture_python.graph_search.a_star import Node, Graph\n\n%matplotlib inline\n</pre> import numpy as np from behavior_generation_lecture_python.graph_search.a_star import Node, Graph  %matplotlib inline In\u00a0[2]: Copied! <pre>def main():\n    nodes_list = [\n        [\"HH\", 170, 620, [\"H\", \"B\"]],\n        [\"H\", 150, 520, [\"B\", \"L\", \"F\", \"HH\"]],\n        [\"B\", 330, 540, [\"HH\", \"H\", \"L\"]],\n        [\"L\", 290, 420, [\"B\", \"H\", \"S\", \"M\"]],\n        [\"F\", 60, 270, [\"H\", \"S\"]],\n        [\"S\", 80, 120, [\"F\", \"L\", \"M\"]],\n        [\"M\", 220, 20, [\"S\", \"L\"]],\n    ]\n    nodes_dict = {}\n    for entry in nodes_list:\n        nodes_dict[entry[0]] = Node(\n            name=entry[0],\n            position=np.array([entry[1], entry[2]]),\n            connected_to=entry[3],\n        )\n\n    graph = Graph(nodes_dict=nodes_dict)\n    graph.draw_graph()\n\n    graph.a_star(start=\"M\", end=\"HH\")\n\n    graph.draw_result()\n</pre> def main():     nodes_list = [         [\"HH\", 170, 620, [\"H\", \"B\"]],         [\"H\", 150, 520, [\"B\", \"L\", \"F\", \"HH\"]],         [\"B\", 330, 540, [\"HH\", \"H\", \"L\"]],         [\"L\", 290, 420, [\"B\", \"H\", \"S\", \"M\"]],         [\"F\", 60, 270, [\"H\", \"S\"]],         [\"S\", 80, 120, [\"F\", \"L\", \"M\"]],         [\"M\", 220, 20, [\"S\", \"L\"]],     ]     nodes_dict = {}     for entry in nodes_list:         nodes_dict[entry[0]] = Node(             name=entry[0],             position=np.array([entry[1], entry[2]]),             connected_to=entry[3],         )      graph = Graph(nodes_dict=nodes_dict)     graph.draw_graph()      graph.a_star(start=\"M\", end=\"HH\")      graph.draw_result() In\u00a0[3]: Copied! <pre>main()\n</pre> main()"},{"location":"notebooks/compare_models_notebook/","title":"Compare Dynamic One Track Models","text":"In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport behavior_generation_lecture_python.vehicle_models.model_comparison as cm\nfrom behavior_generation_lecture_python.utils.plot_vehicle import plot_vehicle as pv\nfrom behavior_generation_lecture_python.utils.vizard import vizard as vz\n\n\ninteractive_widgets = not os.getenv(\"CI\") == \"true\"\nif interactive_widgets:\n    # Use widget backend locally, to be able to interact with the plots\n    %matplotlib widget\nelse:\n    # Use inline backend in CI, to render the notebooks for the hosted docs\n    %matplotlib inline\n</pre> import os import numpy as np import matplotlib.pyplot as plt  import behavior_generation_lecture_python.vehicle_models.model_comparison as cm from behavior_generation_lecture_python.utils.plot_vehicle import plot_vehicle as pv from behavior_generation_lecture_python.utils.vizard import vizard as vz   interactive_widgets = not os.getenv(\"CI\") == \"true\" if interactive_widgets:     # Use widget backend locally, to be able to interact with the plots     %matplotlib widget else:     # Use inline backend in CI, to render the notebooks for the hosted docs     %matplotlib inline In\u00a0[2]: Copied! <pre>def main():\n    print(\"Running simulation...\")\n\n    def delta(t):\n        stwa = 0\n        tp = [0.0, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1]\n        dp = [0.0, 0.0, 1.0, 1.0, -1.0, -1.0, 0.0]\n        stwa = np.interp(t, tp, dp)\n\n        stwa_ampl = 20 * np.pi / 180\n        stwa_ratio = 18\n\n        stwa_max = 520 * np.pi / 180\n        stwa = max(min(stwa_max, stwa), -stwa_max)\n\n        delta = stwa_ampl * stwa / stwa_ratio\n\n        return delta\n\n    vars_0 = [0.0, 0.0, 0.0, 0.0, 0.0]\n    ti = np.arange(0, 5, 0.05)\n    model = cm.CompareModels(vars_0, delta)\n    sol = model.simulate(ti, v=30)\n    x = sol[0][:, 0]\n    y = sol[0][:, 1]\n    psi = sol[0][:, 2]\n    beta = sol[0][:, 3]\n    r = sol[0][:, 4]\n    beta_lin = sol[1][:, 0]\n    r_lin = sol[1][:, 1]\n\n    delta_vals = [delta(t) for t in ti]\n\n    fig, (ax1, ax2) = plt.subplots(2)\n\n    ax1.axis(\"equal\")\n\n    ax2.plot(ti, delta_vals, \"k-\")\n    ax2.plot(ti, beta, \"r-\")\n    ax2.plot(ti, r, \"g-\")\n    ax2.plot(ti, beta_lin, \"m-\")\n    ax2.plot(ti, r_lin, \"b-\")\n\n    ax2.legend([\"delta\", \"beta\", \"r\", \"beta_lin\", \"r_lin\"])\n\n    (point1,) = ax1.plot([], [], marker=\"o\", color=\"blue\", ms=10)\n    (point_delta,) = ax2.plot([], [], marker=\"o\", color=\"black\", ms=3)\n    (point_beta,) = ax2.plot([], [], marker=\"o\", color=\"red\", ms=3)\n    (point_r,) = ax2.plot([], [], marker=\"o\", color=\"green\", ms=3)\n    (point_beta_lin,) = ax2.plot([], [], marker=\"o\", color=\"magenta\", ms=3)\n    (point_r_lin,) = ax2.plot([], [], marker=\"o\", color=\"blue\", ms=3)\n\n    def update(i, *fargs):\n        slice_ = slice(i + 1, i + 2)\n        [l.remove() for l in reversed(ax1.lines)]\n        ax1.plot(x[: i + 1], y[: i + 1], \"b-\", linewidth=0.5)\n        point1.set_data(x[slice_], y[slice_])\n        pv.plot_vehicle(ax1, x[i], y[i], psi[i], delta_vals[i])\n\n        point_delta.set_data(ti[slice_], delta_vals[slice_])\n        point_beta.set_data(ti[slice_], beta[slice_])\n        point_r.set_data(ti[slice_], r[slice_])\n        point_beta_lin.set_data(ti[slice_], beta_lin[slice_])\n        point_r_lin.set_data(ti[slice_], r_lin[slice_])\n        for farg in fargs:\n            print(farg)\n\n    viz = vz.Vizard(fig, update, ti)\n    plt.show()\n</pre> def main():     print(\"Running simulation...\")      def delta(t):         stwa = 0         tp = [0.0, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1]         dp = [0.0, 0.0, 1.0, 1.0, -1.0, -1.0, 0.0]         stwa = np.interp(t, tp, dp)          stwa_ampl = 20 * np.pi / 180         stwa_ratio = 18          stwa_max = 520 * np.pi / 180         stwa = max(min(stwa_max, stwa), -stwa_max)          delta = stwa_ampl * stwa / stwa_ratio          return delta      vars_0 = [0.0, 0.0, 0.0, 0.0, 0.0]     ti = np.arange(0, 5, 0.05)     model = cm.CompareModels(vars_0, delta)     sol = model.simulate(ti, v=30)     x = sol[0][:, 0]     y = sol[0][:, 1]     psi = sol[0][:, 2]     beta = sol[0][:, 3]     r = sol[0][:, 4]     beta_lin = sol[1][:, 0]     r_lin = sol[1][:, 1]      delta_vals = [delta(t) for t in ti]      fig, (ax1, ax2) = plt.subplots(2)      ax1.axis(\"equal\")      ax2.plot(ti, delta_vals, \"k-\")     ax2.plot(ti, beta, \"r-\")     ax2.plot(ti, r, \"g-\")     ax2.plot(ti, beta_lin, \"m-\")     ax2.plot(ti, r_lin, \"b-\")      ax2.legend([\"delta\", \"beta\", \"r\", \"beta_lin\", \"r_lin\"])      (point1,) = ax1.plot([], [], marker=\"o\", color=\"blue\", ms=10)     (point_delta,) = ax2.plot([], [], marker=\"o\", color=\"black\", ms=3)     (point_beta,) = ax2.plot([], [], marker=\"o\", color=\"red\", ms=3)     (point_r,) = ax2.plot([], [], marker=\"o\", color=\"green\", ms=3)     (point_beta_lin,) = ax2.plot([], [], marker=\"o\", color=\"magenta\", ms=3)     (point_r_lin,) = ax2.plot([], [], marker=\"o\", color=\"blue\", ms=3)      def update(i, *fargs):         slice_ = slice(i + 1, i + 2)         [l.remove() for l in reversed(ax1.lines)]         ax1.plot(x[: i + 1], y[: i + 1], \"b-\", linewidth=0.5)         point1.set_data(x[slice_], y[slice_])         pv.plot_vehicle(ax1, x[i], y[i], psi[i], delta_vals[i])          point_delta.set_data(ti[slice_], delta_vals[slice_])         point_beta.set_data(ti[slice_], beta[slice_])         point_r.set_data(ti[slice_], r[slice_])         point_beta_lin.set_data(ti[slice_], beta_lin[slice_])         point_r_lin.set_data(ti[slice_], r_lin[slice_])         for farg in fargs:             print(farg)      viz = vz.Vizard(fig, update, ti)     plt.show() In\u00a0[3]: Copied! <pre>main()\n</pre> main() <pre>Running simulation...\n</pre>"},{"location":"notebooks/lateral_control_riccati_notebook/","title":"Lateral Control (Riccati)","text":"In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport behavior_generation_lecture_python.lateral_control_riccati.lateral_control_riccati as cl\nimport behavior_generation_lecture_python.utils.generate_reference_curve as ref\nfrom behavior_generation_lecture_python.utils.plot_vehicle import plot_vehicle as pv\nfrom behavior_generation_lecture_python.utils.vizard import vizard as vz\nfrom behavior_generation_lecture_python.vehicle_models.vehicle_parameters import (\n    DEFAULT_VEHICLE_PARAMS,\n)\n\ninteractive_widgets = not os.getenv(\"CI\") == \"true\"\nif interactive_widgets:\n    # Use widget backend locally, to be able to interact with the plots\n    %matplotlib widget\nelse:\n    # Use inline backend in CI, to render the notebooks for the hosted docs\n    %matplotlib inline\n</pre> import os import numpy as np import matplotlib.pyplot as plt  import behavior_generation_lecture_python.lateral_control_riccati.lateral_control_riccati as cl import behavior_generation_lecture_python.utils.generate_reference_curve as ref from behavior_generation_lecture_python.utils.plot_vehicle import plot_vehicle as pv from behavior_generation_lecture_python.utils.vizard import vizard as vz from behavior_generation_lecture_python.vehicle_models.vehicle_parameters import (     DEFAULT_VEHICLE_PARAMS, )  interactive_widgets = not os.getenv(\"CI\") == \"true\" if interactive_widgets:     # Use widget backend locally, to be able to interact with the plots     %matplotlib widget else:     # Use inline backend in CI, to render the notebooks for the hosted docs     %matplotlib inline In\u00a0[2]: Copied! <pre>def main():\n    print(\"Running simulation...\")\n    radius = 500\n    vars_0 = [0.0, -radius, 0.0, 0.0, 0.0]\n    v_0 = 33.0\n\n    curve = ref.generate_reference_curve(\n        [0, radius, 0, -radius, 0], [-radius, 0, radius, 0, radius], 10.0\n    )\n    ti = np.arange(0, 40, 0.1)\n\n    # r = 10  # hectic steering behavior\n    r = 10000  # fairly calm steering behavior\n\n    model = cl.LateralControlRiccati(\n        initial_condition=vars_0,\n        curve=curve,\n        vehicle_params=DEFAULT_VEHICLE_PARAMS,\n        initial_velocity=v_0,\n        r=r,\n    )\n\n    sol = model.simulate(ti, v=v_0, t_step=0.1)\n    x = sol[:, 0]\n    y = sol[:, 1]\n    psi = sol[:, 2]\n    delta = sol[:, 5]\n\n    fig, ax = plt.subplots()\n\n    plt.plot(curve[\"x\"], curve[\"y\"], \"r-\", linewidth=0.5)\n    plt.plot(x, y, \"b-\")\n    plt.axis(\"equal\")\n\n    (point1,) = ax.plot([], [], marker=\"o\", color=\"blue\", ms=5)\n\n    def update(i, *fargs):\n        [l.remove() for l in reversed(ax.lines[1:])]\n        ax.plot(x[: i + 1], y[: i + 1], \"b-\", linewidth=0.5)\n        point1.set_data(x[i : i + 1], y[i : i + 1])\n        pv.plot_vehicle(ax, x[i], y[i], psi[i], delta[i])\n        for farg in fargs:\n            print(farg)\n\n    viz = vz.Vizard(fig, update, ti)\n    plt.show()\n</pre> def main():     print(\"Running simulation...\")     radius = 500     vars_0 = [0.0, -radius, 0.0, 0.0, 0.0]     v_0 = 33.0      curve = ref.generate_reference_curve(         [0, radius, 0, -radius, 0], [-radius, 0, radius, 0, radius], 10.0     )     ti = np.arange(0, 40, 0.1)      # r = 10  # hectic steering behavior     r = 10000  # fairly calm steering behavior      model = cl.LateralControlRiccati(         initial_condition=vars_0,         curve=curve,         vehicle_params=DEFAULT_VEHICLE_PARAMS,         initial_velocity=v_0,         r=r,     )      sol = model.simulate(ti, v=v_0, t_step=0.1)     x = sol[:, 0]     y = sol[:, 1]     psi = sol[:, 2]     delta = sol[:, 5]      fig, ax = plt.subplots()      plt.plot(curve[\"x\"], curve[\"y\"], \"r-\", linewidth=0.5)     plt.plot(x, y, \"b-\")     plt.axis(\"equal\")      (point1,) = ax.plot([], [], marker=\"o\", color=\"blue\", ms=5)      def update(i, *fargs):         [l.remove() for l in reversed(ax.lines[1:])]         ax.plot(x[: i + 1], y[: i + 1], \"b-\", linewidth=0.5)         point1.set_data(x[i : i + 1], y[i : i + 1])         pv.plot_vehicle(ax, x[i], y[i], psi[i], delta[i])         for farg in fargs:             print(farg)      viz = vz.Vizard(fig, update, ti)     plt.show() In\u00a0[3]: Copied! <pre>main()\n</pre> main() <pre>Running simulation...\n</pre>"},{"location":"notebooks/lateral_control_state_based_notebook/","title":"Lateral Control (state-based)","text":"In\u00a0[1]: Copied! <pre>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport behavior_generation_lecture_python.lateral_control_state_based.lateral_control_state_based as cl\nimport behavior_generation_lecture_python.utils.generate_reference_curve as ref\nfrom behavior_generation_lecture_python.utils.plot_vehicle import plot_vehicle as pv\nfrom behavior_generation_lecture_python.utils.vizard import vizard as vz\n\ninteractive_widgets = not os.getenv(\"CI\") == \"true\"\nif interactive_widgets:\n    # Use widget backend locally, to be able to interact with the plots\n    %matplotlib widget\nelse:\n    # Use inline backend in CI, to render the notebooks for the hosted docs\n    %matplotlib inline\n</pre> import os import numpy as np import matplotlib.pyplot as plt  import behavior_generation_lecture_python.lateral_control_state_based.lateral_control_state_based as cl import behavior_generation_lecture_python.utils.generate_reference_curve as ref from behavior_generation_lecture_python.utils.plot_vehicle import plot_vehicle as pv from behavior_generation_lecture_python.utils.vizard import vizard as vz  interactive_widgets = not os.getenv(\"CI\") == \"true\" if interactive_widgets:     # Use widget backend locally, to be able to interact with the plots     %matplotlib widget else:     # Use inline backend in CI, to render the notebooks for the hosted docs     %matplotlib inline In\u00a0[2]: Copied! <pre>def main():\n    print(\"Running simulation...\")\n    radius = 20\n    vars_0 = [0.1, -radius, 0.0]\n    curve = ref.generate_reference_curve(\n        [0, radius, 0, -radius, 0], [-radius, 0, radius, 0, radius], 1.0\n    )\n    ti = np.arange(0, 100, 0.1)\n    model = cl.LateralControlStateBased(vars_0, curve)\n    sol = model.simulate(ti, v=1)\n    x = sol[:, 0]\n    y = sol[:, 1]\n    psi = sol[:, 2]\n    delta = sol[:, 4]\n\n    fig, ax = plt.subplots()\n\n    plt.plot(curve[\"x\"], curve[\"y\"], \"r-\", linewidth=0.5)\n    plt.plot(x, y, \"b-\", linewidth=0.5)\n    plt.axis(\"equal\")\n\n    (point1,) = ax.plot([], [], marker=\"o\", color=\"blue\", ms=5)\n\n    def update(i, *fargs):\n        [l.remove() for l in reversed(ax.lines[1:])]\n        ax.plot(x[:i], y[:i], \"b-\", linewidth=0.5)\n        point1.set_data(x[i : i + 1], y[i : i + 1])\n        pv.plot_vehicle(ax, x[i], y[i], psi[i], delta[i])\n        for farg in fargs:\n            print(farg)\n\n    viz = vz.Vizard(fig, update, ti)\n    plt.show()\n</pre> def main():     print(\"Running simulation...\")     radius = 20     vars_0 = [0.1, -radius, 0.0]     curve = ref.generate_reference_curve(         [0, radius, 0, -radius, 0], [-radius, 0, radius, 0, radius], 1.0     )     ti = np.arange(0, 100, 0.1)     model = cl.LateralControlStateBased(vars_0, curve)     sol = model.simulate(ti, v=1)     x = sol[:, 0]     y = sol[:, 1]     psi = sol[:, 2]     delta = sol[:, 4]      fig, ax = plt.subplots()      plt.plot(curve[\"x\"], curve[\"y\"], \"r-\", linewidth=0.5)     plt.plot(x, y, \"b-\", linewidth=0.5)     plt.axis(\"equal\")      (point1,) = ax.plot([], [], marker=\"o\", color=\"blue\", ms=5)      def update(i, *fargs):         [l.remove() for l in reversed(ax.lines[1:])]         ax.plot(x[:i], y[:i], \"b-\", linewidth=0.5)         point1.set_data(x[i : i + 1], y[i : i + 1])         pv.plot_vehicle(ax, x[i], y[i], psi[i], delta[i])         for farg in fargs:             print(farg)      viz = vz.Vizard(fig, update, ti)     plt.show() In\u00a0[3]: Copied! <pre>main()\n</pre> main() <pre>Running simulation...\n</pre>"},{"location":"notebooks/mdp_policy_gradient/","title":"Policy Gradient","text":"In\u00a0[1]: Copied! <pre>import os\n\nfrom behavior_generation_lecture_python.mdp.policy import CategoricalPolicy\nfrom behavior_generation_lecture_python.utils.grid_plotting import (\n    make_plot_policy_step_function,\n)\nfrom behavior_generation_lecture_python.mdp.mdp import (\n    GridMDP,\n    policy_gradient,\n    derive_deterministic_policy,\n    GRID_MDP_DICT,\n    HIGHWAY_MDP_DICT,\n    LC_RIGHT_ACTION,\n    STAY_IN_LANE_ACTION,\n)\n\nHIGHWAY_MDP_DICT[\"restrict_actions_to_available_states\"] = False\n</pre> import os  from behavior_generation_lecture_python.mdp.policy import CategoricalPolicy from behavior_generation_lecture_python.utils.grid_plotting import (     make_plot_policy_step_function, ) from behavior_generation_lecture_python.mdp.mdp import (     GridMDP,     policy_gradient,     derive_deterministic_policy,     GRID_MDP_DICT,     HIGHWAY_MDP_DICT,     LC_RIGHT_ACTION,     STAY_IN_LANE_ACTION, )  HIGHWAY_MDP_DICT[\"restrict_actions_to_available_states\"] = False In\u00a0[2]: Copied! <pre>grid_mdp = GridMDP(**GRID_MDP_DICT)\n</pre> grid_mdp = GridMDP(**GRID_MDP_DICT) In\u00a0[3]: Copied! <pre>policy = CategoricalPolicy(\n    sizes=[len(grid_mdp.initial_state), 32, len(grid_mdp.actions)],\n    actions=list(grid_mdp.actions),\n)\n</pre> policy = CategoricalPolicy(     sizes=[len(grid_mdp.initial_state), 32, len(grid_mdp.actions)],     actions=list(grid_mdp.actions), ) In\u00a0[4]: Copied! <pre>model_checkpoints = policy_gradient(\n    mdp=grid_mdp,\n    policy=policy,\n    iterations=100,\n    return_history=True,\n)\n</pre> model_checkpoints = policy_gradient(     mdp=grid_mdp,     policy=policy,     iterations=100,     return_history=True, ) <pre>iteration:   1;  return: -1.958;  episode_length: 34.020\n</pre> <pre>iteration:   2;  return: -1.798;  episode_length: 33.046\n</pre> <pre>iteration:   3;  return: -1.260;  episode_length: 22.665\n</pre> <pre>iteration:   4;  return: -1.027;  episode_length: 18.454\n</pre> <pre>iteration:   5;  return: -0.972;  episode_length: 14.432\n</pre> <pre>iteration:   6;  return: -0.815;  episode_length: 13.452\n</pre> <pre>iteration:   7;  return: -0.553;  episode_length: 12.414\n</pre> <pre>iteration:   8;  return: -0.525;  episode_length: 11.220\n</pre> <pre>iteration:   9;  return: -0.380;  episode_length: 9.903\n</pre> <pre>iteration:  10;  return: -0.213;  episode_length: 11.190\n</pre> <pre>iteration:  11;  return: -0.130;  episode_length: 11.589\n</pre> <pre>iteration:  12;  return: -0.090;  episode_length: 12.678\n</pre> <pre>iteration:  13;  return: -0.096;  episode_length: 13.276\n</pre> <pre>iteration:  14;  return: -0.060;  episode_length: 15.457\n</pre> <pre>iteration:  15;  return: -0.029;  episode_length: 16.516\n</pre> <pre>iteration:  16;  return: -0.052;  episode_length: 16.756\n</pre> <pre>iteration:  17;  return: -0.008;  episode_length: 18.207\n</pre> <pre>iteration:  18;  return: -0.007;  episode_length: 18.185\n</pre> <pre>iteration:  19;  return: -0.110;  episode_length: 18.445\n</pre> <pre>iteration:  20;  return: 0.029;  episode_length: 16.645\n</pre> <pre>iteration:  21;  return: 0.052;  episode_length: 18.589\n</pre> <pre>iteration:  22;  return: 0.054;  episode_length: 15.216\n</pre> <pre>iteration:  23;  return: 0.074;  episode_length: 15.021\n</pre> <pre>iteration:  24;  return: 0.087;  episode_length: 14.991\n</pre> <pre>iteration:  25;  return: 0.134;  episode_length: 12.733\n</pre> <pre>iteration:  26;  return: 0.124;  episode_length: 13.161\n</pre> <pre>iteration:  27;  return: 0.178;  episode_length: 12.828\n</pre> <pre>iteration:  28;  return: 0.216;  episode_length: 11.983\n</pre> <pre>iteration:  29;  return: 0.192;  episode_length: 11.712\n</pre> <pre>iteration:  30;  return: 0.208;  episode_length: 11.501\n</pre> <pre>iteration:  31;  return: 0.166;  episode_length: 11.497\n</pre> <pre>iteration:  32;  return: 0.139;  episode_length: 11.690\n</pre> <pre>iteration:  33;  return: 0.224;  episode_length: 11.662\n</pre> <pre>iteration:  34;  return: 0.173;  episode_length: 12.116\n</pre> <pre>iteration:  35;  return: 0.225;  episode_length: 11.921\n</pre> <pre>iteration:  36;  return: 0.275;  episode_length: 12.519\n</pre> <pre>iteration:  37;  return: 0.252;  episode_length: 12.636\n</pre> <pre>iteration:  38;  return: 0.168;  episode_length: 12.607\n</pre> <pre>iteration:  39;  return: 0.279;  episode_length: 12.198\n</pre> <pre>iteration:  40;  return: 0.266;  episode_length: 12.568\n</pre> <pre>iteration:  41;  return: 0.324;  episode_length: 12.612\n</pre> <pre>iteration:  42;  return: 0.258;  episode_length: 12.866\n</pre> <pre>iteration:  43;  return: 0.286;  episode_length: 12.184\n</pre> <pre>iteration:  44;  return: 0.288;  episode_length: 12.290\n</pre> <pre>iteration:  45;  return: 0.269;  episode_length: 12.368\n</pre> <pre>iteration:  46;  return: 0.280;  episode_length: 11.800\n</pre> <pre>iteration:  47;  return: 0.295;  episode_length: 12.175\n</pre> <pre>iteration:  48;  return: 0.298;  episode_length: 11.664\n</pre> <pre>iteration:  49;  return: 0.353;  episode_length: 11.525\n</pre> <pre>iteration:  50;  return: 0.287;  episode_length: 11.477\n</pre> <pre>iteration:  51;  return: 0.342;  episode_length: 11.190\n</pre> <pre>iteration:  52;  return: 0.326;  episode_length: 11.604\n</pre> <pre>iteration:  53;  return: 0.336;  episode_length: 11.990\n</pre> <pre>iteration:  54;  return: 0.384;  episode_length: 11.394\n</pre> <pre>iteration:  55;  return: 0.356;  episode_length: 11.270\n</pre> <pre>iteration:  56;  return: 0.377;  episode_length: 11.529\n</pre> <pre>iteration:  57;  return: 0.364;  episode_length: 11.049\n</pre> <pre>iteration:  58;  return: 0.359;  episode_length: 11.662\n</pre> <pre>iteration:  59;  return: 0.363;  episode_length: 11.002\n</pre> <pre>iteration:  60;  return: 0.294;  episode_length: 11.669\n</pre> <pre>iteration:  61;  return: 0.300;  episode_length: 11.784\n</pre> <pre>iteration:  62;  return: 0.401;  episode_length: 11.100\n</pre> <pre>iteration:  63;  return: 0.376;  episode_length: 11.535\n</pre> <pre>iteration:  64;  return: 0.357;  episode_length: 10.950\n</pre> <pre>iteration:  65;  return: 0.426;  episode_length: 11.375\n</pre> <pre>iteration:  66;  return: 0.460;  episode_length: 10.736\n</pre> <pre>iteration:  67;  return: 0.413;  episode_length: 10.978\n</pre> <pre>iteration:  68;  return: 0.421;  episode_length: 11.381\n</pre> <pre>iteration:  69;  return: 0.425;  episode_length: 11.138\n</pre> <pre>iteration:  70;  return: 0.472;  episode_length: 11.004\n</pre> <pre>iteration:  71;  return: 0.488;  episode_length: 10.872\n</pre> <pre>iteration:  72;  return: 0.487;  episode_length: 10.803\n</pre> <pre>iteration:  73;  return: 0.532;  episode_length: 10.241\n</pre> <pre>iteration:  74;  return: 0.496;  episode_length: 10.872\n</pre> <pre>iteration:  75;  return: 0.490;  episode_length: 10.493\n</pre> <pre>iteration:  76;  return: 0.488;  episode_length: 10.533\n</pre> <pre>iteration:  77;  return: 0.502;  episode_length: 10.427\n</pre> <pre>iteration:  78;  return: 0.523;  episode_length: 10.339\n</pre> <pre>iteration:  79;  return: 0.566;  episode_length: 10.590\n</pre> <pre>iteration:  80;  return: 0.537;  episode_length: 10.651\n</pre> <pre>iteration:  81;  return: 0.581;  episode_length: 10.247\n</pre> <pre>iteration:  82;  return: 0.558;  episode_length: 10.581\n</pre> <pre>iteration:  83;  return: 0.573;  episode_length: 10.074\n</pre> <pre>iteration:  84;  return: 0.599;  episode_length: 10.305\n</pre> <pre>iteration:  85;  return: 0.596;  episode_length: 10.000\n</pre> <pre>iteration:  86;  return: 0.597;  episode_length: 10.074\n</pre> <pre>iteration:  87;  return: 0.599;  episode_length: 10.315\n</pre> <pre>iteration:  88;  return: 0.589;  episode_length: 10.068\n</pre> <pre>iteration:  89;  return: 0.614;  episode_length: 10.052\n</pre> <pre>iteration:  90;  return: 0.607;  episode_length: 9.929\n</pre> <pre>iteration:  91;  return: 0.584;  episode_length: 10.455\n</pre> <pre>iteration:  92;  return: 0.612;  episode_length: 10.283\n</pre> <pre>iteration:  93;  return: 0.618;  episode_length: 9.758\n</pre> <pre>iteration:  94;  return: 0.633;  episode_length: 9.779\n</pre> <pre>iteration:  95;  return: 0.630;  episode_length: 9.752\n</pre> <pre>iteration:  96;  return: 0.613;  episode_length: 9.892\n</pre> <pre>iteration:  97;  return: 0.603;  episode_length: 9.596\n</pre> <pre>iteration:  98;  return: 0.620;  episode_length: 9.804\n</pre> <pre>iteration:  99;  return: 0.632;  episode_length: 9.351\n</pre> <pre>iteration: 100;  return: 0.637;  episode_length: 9.418\n</pre> In\u00a0[5]: Copied! <pre>policy_array = [\n    derive_deterministic_policy(mdp=grid_mdp, policy=model)\n    for model in model_checkpoints\n]\n</pre> policy_array = [     derive_deterministic_policy(mdp=grid_mdp, policy=model)     for model in model_checkpoints ] In\u00a0[6]: Copied! <pre>plot_policy_step_grid_map = make_plot_policy_step_function(\n    columns=4, rows=3, policy_over_time=policy_array\n)\n</pre> plot_policy_step_grid_map = make_plot_policy_step_function(     columns=4, rows=3, policy_over_time=policy_array ) In\u00a0[7]: Copied! <pre>interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI\nif interactive_widgets:\n    import ipywidgets\n    from IPython.display import display\n\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(model_checkpoints) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_policy_step_grid_map, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_policy_step_grid_map(100)\n</pre> interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI if interactive_widgets:     import ipywidgets     from IPython.display import display      iteration_slider = ipywidgets.IntSlider(         min=0, max=len(model_checkpoints) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_policy_step_grid_map, iteration=iteration_slider)     display(w) else:     plot_policy_step_grid_map(100) In\u00a0[8]: Copied! <pre>if False:\n    # we will change this to true later on, to see the effect\n    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n        (0.4, LC_RIGHT_ACTION),\n        (0.6, STAY_IN_LANE_ACTION),\n    ]\n</pre> if False:     # we will change this to true later on, to see the effect     HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [         (0.4, LC_RIGHT_ACTION),         (0.6, STAY_IN_LANE_ACTION),     ] In\u00a0[9]: Copied! <pre>highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)\n</pre> highway_mdp = GridMDP(**HIGHWAY_MDP_DICT) In\u00a0[10]: Copied! <pre>policy = CategoricalPolicy(\n    sizes=[len(highway_mdp.initial_state), 32, len(highway_mdp.actions)],\n    actions=list(highway_mdp.actions),\n)\n</pre> policy = CategoricalPolicy(     sizes=[len(highway_mdp.initial_state), 32, len(highway_mdp.actions)],     actions=list(highway_mdp.actions), ) In\u00a0[11]: Copied! <pre>model_checkpoints = policy_gradient(\n    mdp=highway_mdp,\n    policy=policy,\n    iterations=200,\n    return_history=True,\n)\n</pre> model_checkpoints = policy_gradient(     mdp=highway_mdp,     policy=policy,     iterations=200,     return_history=True, ) <pre>iteration:   1;  return: -62.609;  episode_length: 9.889\n</pre> <pre>iteration:   2;  return: -57.321;  episode_length: 9.872\n</pre> <pre>iteration:   3;  return: -53.203;  episode_length: 9.958\n</pre> <pre>iteration:   4;  return: -49.116;  episode_length: 10.144\n</pre> <pre>iteration:   5;  return: -44.359;  episode_length: 10.551\n</pre> <pre>iteration:   6;  return: -35.690;  episode_length: 11.501\n</pre> <pre>iteration:   7;  return: -35.388;  episode_length: 12.533\n</pre> <pre>iteration:   8;  return: -33.855;  episode_length: 13.166\n</pre> <pre>iteration:   9;  return: -34.396;  episode_length: 13.488\n</pre> <pre>iteration:  10;  return: -36.870;  episode_length: 14.752\n</pre> <pre>iteration:  11;  return: -35.730;  episode_length: 14.513\n</pre> <pre>iteration:  12;  return: -33.986;  episode_length: 14.110\n</pre> <pre>iteration:  13;  return: -33.828;  episode_length: 13.856\n</pre> <pre>iteration:  14;  return: -33.851;  episode_length: 13.848\n</pre> <pre>iteration:  15;  return: -31.328;  episode_length: 13.246\n</pre> <pre>iteration:  16;  return: -30.681;  episode_length: 12.010\n</pre> <pre>iteration:  17;  return: -30.977;  episode_length: 11.603\n</pre> <pre>iteration:  18;  return: -29.952;  episode_length: 11.007\n</pre> <pre>iteration:  19;  return: -30.412;  episode_length: 11.066\n</pre> <pre>iteration:  20;  return: -30.926;  episode_length: 10.947\n</pre> <pre>iteration:  21;  return: -30.712;  episode_length: 11.086\n</pre> <pre>iteration:  22;  return: -28.176;  episode_length: 11.429\n</pre> <pre>iteration:  23;  return: -28.809;  episode_length: 11.795\n</pre> <pre>iteration:  24;  return: -29.580;  episode_length: 11.739\n</pre> <pre>iteration:  25;  return: -28.328;  episode_length: 12.383\n</pre> <pre>iteration:  26;  return: -28.071;  episode_length: 12.170\n</pre> <pre>iteration:  27;  return: -30.262;  episode_length: 12.348\n</pre> <pre>iteration:  28;  return: -29.177;  episode_length: 12.150\n</pre> <pre>iteration:  29;  return: -27.311;  episode_length: 11.689\n</pre> <pre>iteration:  30;  return: -28.071;  episode_length: 11.116\n</pre> <pre>iteration:  31;  return: -26.955;  episode_length: 10.838\n</pre> <pre>iteration:  32;  return: -27.556;  episode_length: 10.277\n</pre> <pre>iteration:  33;  return: -29.328;  episode_length: 10.126\n</pre> <pre>iteration:  34;  return: -29.534;  episode_length: 10.016\n</pre> <pre>iteration:  35;  return: -28.053;  episode_length: 9.872\n</pre> <pre>iteration:  36;  return: -26.133;  episode_length: 10.285\n</pre> <pre>iteration:  37;  return: -26.432;  episode_length: 10.339\n</pre> <pre>iteration:  38;  return: -25.581;  episode_length: 10.866\n</pre> <pre>iteration:  39;  return: -26.093;  episode_length: 11.363\n</pre> <pre>iteration:  40;  return: -25.835;  episode_length: 11.781\n</pre> <pre>iteration:  41;  return: -26.587;  episode_length: 11.895\n</pre> <pre>iteration:  42;  return: -26.950;  episode_length: 11.993\n</pre> <pre>iteration:  43;  return: -26.136;  episode_length: 11.767\n</pre> <pre>iteration:  44;  return: -26.855;  episode_length: 11.891\n</pre> <pre>iteration:  45;  return: -24.887;  episode_length: 11.266\n</pre> <pre>iteration:  46;  return: -25.130;  episode_length: 10.703\n</pre> <pre>iteration:  47;  return: -25.745;  episode_length: 10.539\n</pre> <pre>iteration:  48;  return: -25.257;  episode_length: 9.996\n</pre> <pre>iteration:  49;  return: -25.020;  episode_length: 10.012\n</pre> <pre>iteration:  50;  return: -25.536;  episode_length: 9.835\n</pre> <pre>iteration:  51;  return: -24.784;  episode_length: 9.762\n</pre> <pre>iteration:  52;  return: -24.091;  episode_length: 9.724\n</pre> <pre>iteration:  53;  return: -24.696;  episode_length: 10.083\n</pre> <pre>iteration:  54;  return: -23.777;  episode_length: 10.064\n</pre> <pre>iteration:  55;  return: -23.392;  episode_length: 10.320\n</pre> <pre>iteration:  56;  return: -23.623;  episode_length: 10.305\n</pre> <pre>iteration:  57;  return: -23.344;  episode_length: 10.559\n</pre> <pre>iteration:  58;  return: -23.406;  episode_length: 10.362\n</pre> <pre>iteration:  59;  return: -23.121;  episode_length: 10.107\n</pre> <pre>iteration:  60;  return: -23.777;  episode_length: 10.132\n</pre> <pre>iteration:  61;  return: -22.056;  episode_length: 10.066\n</pre> <pre>iteration:  62;  return: -23.356;  episode_length: 9.913\n</pre> <pre>iteration:  63;  return: -23.739;  episode_length: 9.814\n</pre> <pre>iteration:  64;  return: -22.574;  episode_length: 9.831\n</pre> <pre>iteration:  65;  return: -22.470;  episode_length: 10.014\n</pre> <pre>iteration:  66;  return: -22.357;  episode_length: 10.093\n</pre> <pre>iteration:  67;  return: -22.324;  episode_length: 9.962\n</pre> <pre>iteration:  68;  return: -21.758;  episode_length: 9.673\n</pre> <pre>iteration:  69;  return: -22.106;  episode_length: 9.856\n</pre> <pre>iteration:  70;  return: -20.828;  episode_length: 9.768\n</pre> <pre>iteration:  71;  return: -21.949;  episode_length: 9.850\n</pre> <pre>iteration:  72;  return: -21.886;  episode_length: 9.860\n</pre> <pre>iteration:  73;  return: -22.053;  episode_length: 10.167\n</pre> <pre>iteration:  74;  return: -21.695;  episode_length: 9.848\n</pre> <pre>iteration:  75;  return: -21.552;  episode_length: 9.633\n</pre> <pre>iteration:  76;  return: -21.686;  episode_length: 9.703\n</pre> <pre>iteration:  77;  return: -21.596;  episode_length: 9.514\n</pre> <pre>iteration:  78;  return: -21.618;  episode_length: 9.433\n</pre> <pre>iteration:  79;  return: -21.917;  episode_length: 9.443\n</pre> <pre>iteration:  80;  return: -21.125;  episode_length: 9.449\n</pre> <pre>iteration:  81;  return: -20.944;  episode_length: 9.424\n</pre> <pre>iteration:  82;  return: -21.463;  episode_length: 9.607\n</pre> <pre>iteration:  83;  return: -21.507;  episode_length: 9.585\n</pre> <pre>iteration:  84;  return: -21.669;  episode_length: 9.564\n</pre> <pre>iteration:  85;  return: -21.209;  episode_length: 9.614\n</pre> <pre>iteration:  86;  return: -20.799;  episode_length: 9.600\n</pre> <pre>iteration:  87;  return: -20.615;  episode_length: 9.677\n</pre> <pre>iteration:  88;  return: -21.458;  episode_length: 9.732\n</pre> <pre>iteration:  89;  return: -21.202;  episode_length: 9.739\n</pre> <pre>iteration:  90;  return: -20.893;  episode_length: 9.717\n</pre> <pre>iteration:  91;  return: -20.502;  episode_length: 9.692\n</pre> <pre>iteration:  92;  return: -20.648;  episode_length: 9.568\n</pre> <pre>iteration:  93;  return: -20.533;  episode_length: 9.594\n</pre> <pre>iteration:  94;  return: -20.170;  episode_length: 9.481\n</pre> <pre>iteration:  95;  return: -20.697;  episode_length: 9.533\n</pre> <pre>iteration:  96;  return: -20.057;  episode_length: 9.477\n</pre> <pre>iteration:  97;  return: -19.870;  episode_length: 9.422\n</pre> <pre>iteration:  98;  return: -20.781;  episode_length: 9.309\n</pre> <pre>iteration:  99;  return: -20.681;  episode_length: 9.274\n</pre> <pre>iteration: 100;  return: -20.702;  episode_length: 9.214\n</pre> <pre>iteration: 101;  return: -20.006;  episode_length: 9.232\n</pre> <pre>iteration: 102;  return: -19.779;  episode_length: 9.240\n</pre> <pre>iteration: 103;  return: -19.212;  episode_length: 9.215\n</pre> <pre>iteration: 104;  return: -19.848;  episode_length: 9.259\n</pre> <pre>iteration: 105;  return: -19.203;  episode_length: 9.315\n</pre> <pre>iteration: 106;  return: -20.315;  episode_length: 9.272\n</pre> <pre>iteration: 107;  return: -19.933;  episode_length: 9.345\n</pre> <pre>iteration: 108;  return: -19.756;  episode_length: 9.320\n</pre> <pre>iteration: 109;  return: -19.787;  episode_length: 9.261\n</pre> <pre>iteration: 110;  return: -19.755;  episode_length: 9.371\n</pre> <pre>iteration: 111;  return: -19.816;  episode_length: 9.398\n</pre> <pre>iteration: 112;  return: -20.057;  episode_length: 9.456\n</pre> <pre>iteration: 113;  return: -18.943;  episode_length: 9.494\n</pre> <pre>iteration: 114;  return: -19.365;  episode_length: 9.456\n</pre> <pre>iteration: 115;  return: -20.128;  episode_length: 9.303\n</pre> <pre>iteration: 116;  return: -19.772;  episode_length: 9.348\n</pre> <pre>iteration: 117;  return: -20.024;  episode_length: 9.363\n</pre> <pre>iteration: 118;  return: -19.367;  episode_length: 9.371\n</pre> <pre>iteration: 119;  return: -19.236;  episode_length: 9.280\n</pre> <pre>iteration: 120;  return: -18.797;  episode_length: 9.313\n</pre> <pre>iteration: 121;  return: -19.350;  episode_length: 9.225\n</pre> <pre>iteration: 122;  return: -19.062;  episode_length: 9.334\n</pre> <pre>iteration: 123;  return: -19.399;  episode_length: 9.240\n</pre> <pre>iteration: 124;  return: -19.187;  episode_length: 9.282\n</pre> <pre>iteration: 125;  return: -19.221;  episode_length: 9.225\n</pre> <pre>iteration: 126;  return: -19.793;  episode_length: 9.167\n</pre> <pre>iteration: 127;  return: -18.872;  episode_length: 9.155\n</pre> <pre>iteration: 128;  return: -19.665;  episode_length: 9.212\n</pre> <pre>iteration: 129;  return: -19.005;  episode_length: 9.167\n</pre> <pre>iteration: 130;  return: -19.941;  episode_length: 9.238\n</pre> <pre>iteration: 131;  return: -18.555;  episode_length: 9.159\n</pre> <pre>iteration: 132;  return: -18.759;  episode_length: 9.152\n</pre> <pre>iteration: 133;  return: -19.142;  episode_length: 9.236\n</pre> <pre>iteration: 134;  return: -19.359;  episode_length: 9.361\n</pre> <pre>iteration: 135;  return: -18.791;  episode_length: 9.440\n</pre> <pre>iteration: 136;  return: -19.037;  episode_length: 9.267\n</pre> <pre>iteration: 137;  return: -19.808;  episode_length: 9.330\n</pre> <pre>iteration: 138;  return: -18.940;  episode_length: 9.341\n</pre> <pre>iteration: 139;  return: -18.950;  episode_length: 9.270\n</pre> <pre>iteration: 140;  return: -19.074;  episode_length: 9.270\n</pre> <pre>iteration: 141;  return: -19.277;  episode_length: 9.238\n</pre> <pre>iteration: 142;  return: -19.279;  episode_length: 9.189\n</pre> <pre>iteration: 143;  return: -18.489;  episode_length: 9.159\n</pre> <pre>iteration: 144;  return: -18.533;  episode_length: 9.139\n</pre> <pre>iteration: 145;  return: -19.000;  episode_length: 9.105\n</pre> <pre>iteration: 146;  return: -19.283;  episode_length: 9.074\n</pre> <pre>iteration: 147;  return: -18.832;  episode_length: 9.074\n</pre> <pre>iteration: 148;  return: -19.951;  episode_length: 9.078\n</pre> <pre>iteration: 149;  return: -19.459;  episode_length: 9.043\n</pre> <pre>iteration: 150;  return: -19.732;  episode_length: 9.058\n</pre> <pre>iteration: 151;  return: -19.576;  episode_length: 9.063\n</pre> <pre>iteration: 152;  return: -19.414;  episode_length: 9.056\n</pre> <pre>iteration: 153;  return: -19.826;  episode_length: 9.056\n</pre> <pre>iteration: 154;  return: -19.755;  episode_length: 9.105\n</pre> <pre>iteration: 155;  return: -18.399;  episode_length: 9.109\n</pre> <pre>iteration: 156;  return: -19.126;  episode_length: 9.126\n</pre> <pre>iteration: 157;  return: -19.439;  episode_length: 9.176\n</pre> <pre>iteration: 158;  return: -18.529;  episode_length: 9.163\n</pre> <pre>iteration: 159;  return: -18.616;  episode_length: 9.152\n</pre> <pre>iteration: 160;  return: -19.322;  episode_length: 9.161\n</pre> <pre>iteration: 161;  return: -19.033;  episode_length: 9.240\n</pre> <pre>iteration: 162;  return: -18.707;  episode_length: 9.232\n</pre> <pre>iteration: 163;  return: -19.665;  episode_length: 9.426\n</pre> <pre>iteration: 164;  return: -19.214;  episode_length: 9.309\n</pre> <pre>iteration: 165;  return: -19.310;  episode_length: 9.338\n</pre> <pre>iteration: 166;  return: -19.183;  episode_length: 9.332\n</pre> <pre>iteration: 167;  return: -19.215;  episode_length: 9.336\n</pre> <pre>iteration: 168;  return: -19.476;  episode_length: 9.412\n</pre> <pre>iteration: 169;  return: -18.867;  episode_length: 9.270\n</pre> <pre>iteration: 170;  return: -19.102;  episode_length: 9.251\n</pre> <pre>iteration: 171;  return: -18.891;  episode_length: 9.231\n</pre> <pre>iteration: 172;  return: -19.598;  episode_length: 9.265\n</pre> <pre>iteration: 173;  return: -18.582;  episode_length: 9.223\n</pre> <pre>iteration: 174;  return: -18.648;  episode_length: 9.189\n</pre> <pre>iteration: 175;  return: -19.404;  episode_length: 9.191\n</pre> <pre>iteration: 176;  return: -18.891;  episode_length: 9.229\n</pre> <pre>iteration: 177;  return: -18.908;  episode_length: 9.174\n</pre> <pre>iteration: 178;  return: -18.862;  episode_length: 9.219\n</pre> <pre>iteration: 179;  return: -19.046;  episode_length: 9.229\n</pre> <pre>iteration: 180;  return: -18.572;  episode_length: 9.178\n</pre> <pre>iteration: 181;  return: -19.257;  episode_length: 9.178\n</pre> <pre>iteration: 182;  return: -18.583;  episode_length: 9.148\n</pre> <pre>iteration: 183;  return: -18.599;  episode_length: 9.168\n</pre> <pre>iteration: 184;  return: -19.122;  episode_length: 9.150\n</pre> <pre>iteration: 185;  return: -18.872;  episode_length: 9.159\n</pre> <pre>iteration: 186;  return: -19.404;  episode_length: 9.152\n</pre> <pre>iteration: 187;  return: -18.740;  episode_length: 9.150\n</pre> <pre>iteration: 188;  return: -19.031;  episode_length: 9.128\n</pre> <pre>iteration: 189;  return: -18.308;  episode_length: 9.071\n</pre> <pre>iteration: 190;  return: -18.767;  episode_length: 9.165\n</pre> <pre>iteration: 191;  return: -18.483;  episode_length: 9.056\n</pre> <pre>iteration: 192;  return: -19.045;  episode_length: 9.098\n</pre> <pre>iteration: 193;  return: -18.494;  episode_length: 9.045\n</pre> <pre>iteration: 194;  return: -19.018;  episode_length: 9.043\n</pre> <pre>iteration: 195;  return: -19.034;  episode_length: 9.034\n</pre> <pre>iteration: 196;  return: -19.018;  episode_length: 9.071\n</pre> <pre>iteration: 197;  return: -19.080;  episode_length: 9.128\n</pre> <pre>iteration: 198;  return: -19.323;  episode_length: 9.018\n</pre> <pre>iteration: 199;  return: -18.634;  episode_length: 9.029\n</pre> <pre>iteration: 200;  return: -18.908;  episode_length: 9.045\n</pre> In\u00a0[12]: Copied! <pre>policy_array = [\n    derive_deterministic_policy(mdp=highway_mdp, policy=model)\n    for model in model_checkpoints\n]\n</pre> policy_array = [     derive_deterministic_policy(mdp=highway_mdp, policy=model)     for model in model_checkpoints ] In\u00a0[13]: Copied! <pre>plot_policy_step_grid_map = make_plot_policy_step_function(\n    columns=10, rows=4, policy_over_time=policy_array\n)\n</pre> plot_policy_step_grid_map = make_plot_policy_step_function(     columns=10, rows=4, policy_over_time=policy_array ) In\u00a0[14]: Copied! <pre>if interactive_widgets:\n    import ipywidgets\n    from IPython.display import display\n\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(model_checkpoints) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_policy_step_grid_map, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_policy_step_grid_map(200)\n</pre> if interactive_widgets:     import ipywidgets     from IPython.display import display      iteration_slider = ipywidgets.IntSlider(         min=0, max=len(model_checkpoints) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_policy_step_grid_map, iteration=iteration_slider)     display(w) else:     plot_policy_step_grid_map(200)"},{"location":"notebooks/mdp_policy_gradient/#toy-example","title":"TOY EXAMPLE\u00b6","text":""},{"location":"notebooks/mdp_policy_gradient/#highway-example","title":"HIGHWAY EXAMPLE\u00b6","text":""},{"location":"notebooks/mdp_q_learning/","title":"Q-Learning","text":"In\u00a0[1]: Copied! <pre>import os\n\nfrom behavior_generation_lecture_python.mdp.mdp import (\n    GridMDP,\n    derive_policy,\n    q_learning,\n    GRID_MDP_DICT,\n    HIGHWAY_MDP_DICT,\n    LC_RIGHT_ACTION,\n    STAY_IN_LANE_ACTION,\n)\nfrom behavior_generation_lecture_python.utils.grid_plotting import (\n    make_plot_grid_step_function,\n    make_plot_policy_step_function,\n)\n</pre> import os  from behavior_generation_lecture_python.mdp.mdp import (     GridMDP,     derive_policy,     q_learning,     GRID_MDP_DICT,     HIGHWAY_MDP_DICT,     LC_RIGHT_ACTION,     STAY_IN_LANE_ACTION, ) from behavior_generation_lecture_python.utils.grid_plotting import (     make_plot_grid_step_function,     make_plot_policy_step_function, ) In\u00a0[2]: Copied! <pre>grid_mdp = GridMDP(**GRID_MDP_DICT)\n</pre> grid_mdp = GridMDP(**GRID_MDP_DICT) In\u00a0[3]: Copied! <pre>computed_utility_history = q_learning(\n    mdp=grid_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True\n)\n</pre> computed_utility_history = q_learning(     mdp=grid_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True ) In\u00a0[4]: Copied! <pre>%matplotlib inline\nplot_grid_step = make_plot_grid_step_function(\n    columns=4, rows=3, U_over_time=computed_utility_history\n)\n</pre> %matplotlib inline plot_grid_step = make_plot_grid_step_function(     columns=4, rows=3, U_over_time=computed_utility_history ) In\u00a0[5]: Copied! <pre>interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI\nif interactive_widgets:\n    import ipywidgets\n    from IPython.display import display\n\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(computed_utility_history) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_grid_step(1000)\n</pre> interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI if interactive_widgets:     import ipywidgets     from IPython.display import display      iteration_slider = ipywidgets.IntSlider(         min=0, max=len(computed_utility_history) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)     display(w) else:     plot_grid_step(1000) In\u00a0[6]: Copied! <pre>if False:\n    # we will change this to true later on, to see the effect\n    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n        (0.4, LC_RIGHT_ACTION),\n        (0.6, STAY_IN_LANE_ACTION),\n    ]\n</pre> if False:     # we will change this to true later on, to see the effect     HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [         (0.4, LC_RIGHT_ACTION),         (0.6, STAY_IN_LANE_ACTION),     ] In\u00a0[7]: Copied! <pre>highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)\n</pre> highway_mdp = GridMDP(**HIGHWAY_MDP_DICT) In\u00a0[8]: Copied! <pre>utility_history_highway = q_learning(\n    mdp=highway_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True\n)\n</pre> utility_history_highway = q_learning(     mdp=highway_mdp, alpha=0.1, epsilon=0.1, iterations=10000, return_history=True ) In\u00a0[9]: Copied! <pre>plot_grid_step_highway = make_plot_grid_step_function(\n    columns=10, rows=4, U_over_time=utility_history_highway\n)\n</pre> plot_grid_step_highway = make_plot_grid_step_function(     columns=10, rows=4, U_over_time=utility_history_highway ) In\u00a0[10]: Copied! <pre>if interactive_widgets:\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_grid_step_highway(1000)\n</pre> if interactive_widgets:     iteration_slider = ipywidgets.IntSlider(         min=0, max=len(utility_history_highway) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)     display(w) else:     plot_grid_step_highway(1000) In\u00a0[11]: Copied! <pre>policy_array = [\n    derive_policy(highway_mdp, utility) for utility in utility_history_highway\n]\n</pre> policy_array = [     derive_policy(highway_mdp, utility) for utility in utility_history_highway ] In\u00a0[12]: Copied! <pre>plot_policy_step_highway = make_plot_policy_step_function(\n    columns=10, rows=4, policy_over_time=policy_array\n)\n</pre> plot_policy_step_highway = make_plot_policy_step_function(     columns=10, rows=4, policy_over_time=policy_array ) In\u00a0[13]: Copied! <pre>if interactive_widgets:\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_policy_step_highway(1000)\n</pre> if interactive_widgets:     iteration_slider = ipywidgets.IntSlider(         min=0, max=len(utility_history_highway) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)     display(w) else:     plot_policy_step_highway(1000)"},{"location":"notebooks/mdp_q_learning/#toy-example","title":"TOY EXAMPLE\u00b6","text":""},{"location":"notebooks/mdp_q_learning/#highway-example","title":"HIGHWAY EXAMPLE\u00b6","text":""},{"location":"notebooks/mdp_value_iteration/","title":"Value Iteration","text":"In\u00a0[1]: Copied! <pre>import os\n\nfrom behavior_generation_lecture_python.mdp.mdp import (\n    GridMDP,\n    derive_policy,\n    value_iteration,\n    GRID_MDP_DICT,\n    HIGHWAY_MDP_DICT,\n    LC_RIGHT_ACTION,\n    STAY_IN_LANE_ACTION,\n)\nfrom behavior_generation_lecture_python.utils.grid_plotting import (\n    make_plot_grid_step_function,\n    make_plot_policy_step_function,\n)\n</pre> import os  from behavior_generation_lecture_python.mdp.mdp import (     GridMDP,     derive_policy,     value_iteration,     GRID_MDP_DICT,     HIGHWAY_MDP_DICT,     LC_RIGHT_ACTION,     STAY_IN_LANE_ACTION, ) from behavior_generation_lecture_python.utils.grid_plotting import (     make_plot_grid_step_function,     make_plot_policy_step_function, ) In\u00a0[2]: Copied! <pre>grid_mdp = GridMDP(**GRID_MDP_DICT)\n</pre> grid_mdp = GridMDP(**GRID_MDP_DICT) In\u00a0[3]: Copied! <pre>computed_utility_history = value_iteration(\n    mdp=grid_mdp, epsilon=0.001, max_iterations=30, return_history=True\n)\n</pre> computed_utility_history = value_iteration(     mdp=grid_mdp, epsilon=0.001, max_iterations=30, return_history=True ) In\u00a0[4]: Copied! <pre>%matplotlib inline\nplot_grid_step = make_plot_grid_step_function(\n    columns=4, rows=3, U_over_time=computed_utility_history\n)\n</pre> %matplotlib inline plot_grid_step = make_plot_grid_step_function(     columns=4, rows=3, U_over_time=computed_utility_history ) In\u00a0[5]: Copied! <pre>interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI\nif interactive_widgets:\n    import ipywidgets\n    from IPython.display import display\n\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(computed_utility_history) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_grid_step(19)\n</pre> interactive_widgets = not os.getenv(\"CI\") == \"true\"  # non-interative in CI if interactive_widgets:     import ipywidgets     from IPython.display import display      iteration_slider = ipywidgets.IntSlider(         min=0, max=len(computed_utility_history) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_grid_step, iteration=iteration_slider)     display(w) else:     plot_grid_step(19) In\u00a0[6]: Copied! <pre>if False:\n    # we will change this to true later on, to see the effect\n    HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [\n        (0.4, LC_RIGHT_ACTION),\n        (0.6, STAY_IN_LANE_ACTION),\n    ]\n</pre> if False:     # we will change this to true later on, to see the effect     HIGHWAY_MDP_DICT[\"transition_probabilities_per_action\"][LC_RIGHT_ACTION] = [         (0.4, LC_RIGHT_ACTION),         (0.6, STAY_IN_LANE_ACTION),     ] In\u00a0[7]: Copied! <pre>highway_mdp = GridMDP(**HIGHWAY_MDP_DICT)\n</pre> highway_mdp = GridMDP(**HIGHWAY_MDP_DICT) In\u00a0[8]: Copied! <pre>utility_history_highway = value_iteration(\n    highway_mdp, epsilon=0.001, max_iterations=30, return_history=True\n)\n</pre> utility_history_highway = value_iteration(     highway_mdp, epsilon=0.001, max_iterations=30, return_history=True ) In\u00a0[9]: Copied! <pre>plot_grid_step_highway = make_plot_grid_step_function(\n    columns=10, rows=4, U_over_time=utility_history_highway\n)\n</pre> plot_grid_step_highway = make_plot_grid_step_function(     columns=10, rows=4, U_over_time=utility_history_highway ) In\u00a0[10]: Copied! <pre>if interactive_widgets:\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_grid_step_highway(10)\n</pre> if interactive_widgets:     iteration_slider = ipywidgets.IntSlider(         min=0, max=len(utility_history_highway) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_grid_step_highway, iteration=iteration_slider)     display(w) else:     plot_grid_step_highway(10) In\u00a0[11]: Copied! <pre>policy_array = [\n    derive_policy(highway_mdp, utility) for utility in utility_history_highway\n]\n</pre> policy_array = [     derive_policy(highway_mdp, utility) for utility in utility_history_highway ] In\u00a0[12]: Copied! <pre>plot_policy_step_highway = make_plot_policy_step_function(\n    columns=10, rows=4, policy_over_time=policy_array\n)\n</pre> plot_policy_step_highway = make_plot_policy_step_function(     columns=10, rows=4, policy_over_time=policy_array ) In\u00a0[13]: Copied! <pre>if interactive_widgets:\n    iteration_slider = ipywidgets.IntSlider(\n        min=0, max=len(utility_history_highway) - 1, step=1, value=0\n    )\n    w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)\n    display(w)\nelse:\n    plot_policy_step_highway(10)\n</pre> if interactive_widgets:     iteration_slider = ipywidgets.IntSlider(         min=0, max=len(utility_history_highway) - 1, step=1, value=0     )     w = ipywidgets.interactive(plot_policy_step_highway, iteration=iteration_slider)     display(w) else:     plot_policy_step_highway(10)"},{"location":"notebooks/mdp_value_iteration/#toy-example","title":"TOY EXAMPLE\u00b6","text":""},{"location":"notebooks/mdp_value_iteration/#highway-example","title":"HIGHWAY EXAMPLE\u00b6","text":""},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>behavior_generation_lecture_python<ul> <li>graph_search<ul> <li>a_star</li> </ul> </li> <li>mdp<ul> <li>mdp</li> <li>policy</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/behavior_generation_lecture_python/graph_search/","title":"graph_search","text":""},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/","title":"a_star","text":""},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Graph","title":"<code>Graph</code>","text":"Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>class Graph:\n    def __init__(self, nodes_dict: Dict[str, Node]) -&gt; None:\n        \"\"\"\n        A graph for A* computation.\n\n        :param nodes_dict: The dictionary containing the nodes of the graph.\n        \"\"\"\n        self.nodes_dict = nodes_dict\n\n        self._end_node = None\n\n        fig, ax = plt.subplots()\n        self.fig = fig\n        self.ax = ax\n\n    def draw_graph(self) -&gt; None:\n        \"\"\"\n        Draw all nodes and their connections in the graph.\n\n        :return:\n        \"\"\"\n        self.ax.set_xlim([0, 700])\n        self.ax.set_ylim([0, 700])\n\n        for node in self.nodes_dict.values():\n            self.ax.plot(\n                node.position[0], node.position[1], marker=\"o\", markersize=6, color=\"k\"\n            )\n            self.ax.annotate(node.name, (node.position[0] + 10, node.position[1] + 10))\n            for connected_to in node.connected_to:\n                connected_node = self.nodes_dict[connected_to]\n                self.ax.plot(\n                    [node.position[0], connected_node.position[0]],\n                    [node.position[1], connected_node.position[1]],\n                    linewidth=1,\n                    color=\"k\",\n                )\n\n    def draw_result(self) -&gt; None:\n        \"\"\"\n        Draw the solution to the shortest path problem.\n\n        :return:\n        \"\"\"\n        assert (\n            self._end_node\n        ), \"End node not defined, run a_star() before drawing the result.\"\n        current_node = self._end_node\n        while self.nodes_dict[current_node].predecessor:\n            curr_node = self.nodes_dict[current_node]\n            predecessor = curr_node.predecessor\n            pred_node = self.nodes_dict[predecessor]\n\n            self.ax.plot(\n                [curr_node.position[0], pred_node.position[0]],\n                [curr_node.position[1], pred_node.position[1]],\n                linewidth=2,\n                color=\"b\",\n            )\n\n            distance = np.linalg.norm(curr_node.position - pred_node.position)\n            x_mid = (curr_node.position[0] + pred_node.position[0]) / 2.0\n            y_mid = (curr_node.position[1] + pred_node.position[1]) / 2.0\n            self.ax.annotate(f\"{distance:.2f}\", (x_mid + 10, y_mid + 10))\n            current_node = predecessor\n\n        plt.show()\n\n    def a_star(self, start: str, end: str) -&gt; bool:\n        \"\"\"\n        Compute the shortest path through the graph with the A* algorithm.\n\n        :param start: Name of the start node.\n        :param end: Name of the end node.\n        :return: True if shortest path found, False otherwise.\n        \"\"\"\n        assert start in self.nodes_dict, f\"Start node '{start}' must be in graph\"\n        assert end in self.nodes_dict, f\"End node '{end}' must be in graph\"\n\n        self._end_node = end\n        open_set = set()\n        closed_set = set()\n        for node in self.nodes_dict.values():\n            node.compute_heuristic_cost_to_go(self.nodes_dict[end])\n\n        open_set.add(start)\n        self.nodes_dict[start].cost_to_come = 0\n\n        while open_set:\n            current_node = extract_min(node_set=open_set, node_dict=self.nodes_dict)\n\n            if current_node == end:\n                return True\n\n            closed_set.add(current_node)\n\n            for successor_node in self.nodes_dict[current_node].connected_to:\n                if successor_node in closed_set:\n                    continue\n\n                tentative_cost_to_come = self.nodes_dict[\n                    current_node\n                ].cost_to_come + np.linalg.norm(\n                    self.nodes_dict[current_node].position\n                    - self.nodes_dict[successor_node].position\n                )\n                if (\n                    successor_node in open_set\n                    and tentative_cost_to_come\n                    &gt;= self.nodes_dict[successor_node].cost_to_come\n                ):\n                    continue\n\n                self.nodes_dict[successor_node].predecessor = current_node\n                self.nodes_dict[successor_node].cost_to_come = tentative_cost_to_come\n                open_set.add(successor_node)\n\n        return False\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Graph.__init__","title":"<code>__init__(nodes_dict)</code>","text":"<p>A graph for A* computation.</p> <p>:param nodes_dict: The dictionary containing the nodes of the graph.</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def __init__(self, nodes_dict: Dict[str, Node]) -&gt; None:\n    \"\"\"\n    A graph for A* computation.\n\n    :param nodes_dict: The dictionary containing the nodes of the graph.\n    \"\"\"\n    self.nodes_dict = nodes_dict\n\n    self._end_node = None\n\n    fig, ax = plt.subplots()\n    self.fig = fig\n    self.ax = ax\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Graph.a_star","title":"<code>a_star(start, end)</code>","text":"<p>Compute the shortest path through the graph with the A* algorithm.</p> <p>:param start: Name of the start node. :param end: Name of the end node. :return: True if shortest path found, False otherwise.</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def a_star(self, start: str, end: str) -&gt; bool:\n    \"\"\"\n    Compute the shortest path through the graph with the A* algorithm.\n\n    :param start: Name of the start node.\n    :param end: Name of the end node.\n    :return: True if shortest path found, False otherwise.\n    \"\"\"\n    assert start in self.nodes_dict, f\"Start node '{start}' must be in graph\"\n    assert end in self.nodes_dict, f\"End node '{end}' must be in graph\"\n\n    self._end_node = end\n    open_set = set()\n    closed_set = set()\n    for node in self.nodes_dict.values():\n        node.compute_heuristic_cost_to_go(self.nodes_dict[end])\n\n    open_set.add(start)\n    self.nodes_dict[start].cost_to_come = 0\n\n    while open_set:\n        current_node = extract_min(node_set=open_set, node_dict=self.nodes_dict)\n\n        if current_node == end:\n            return True\n\n        closed_set.add(current_node)\n\n        for successor_node in self.nodes_dict[current_node].connected_to:\n            if successor_node in closed_set:\n                continue\n\n            tentative_cost_to_come = self.nodes_dict[\n                current_node\n            ].cost_to_come + np.linalg.norm(\n                self.nodes_dict[current_node].position\n                - self.nodes_dict[successor_node].position\n            )\n            if (\n                successor_node in open_set\n                and tentative_cost_to_come\n                &gt;= self.nodes_dict[successor_node].cost_to_come\n            ):\n                continue\n\n            self.nodes_dict[successor_node].predecessor = current_node\n            self.nodes_dict[successor_node].cost_to_come = tentative_cost_to_come\n            open_set.add(successor_node)\n\n    return False\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Graph.draw_graph","title":"<code>draw_graph()</code>","text":"<p>Draw all nodes and their connections in the graph.</p> <p>:return:</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def draw_graph(self) -&gt; None:\n    \"\"\"\n    Draw all nodes and their connections in the graph.\n\n    :return:\n    \"\"\"\n    self.ax.set_xlim([0, 700])\n    self.ax.set_ylim([0, 700])\n\n    for node in self.nodes_dict.values():\n        self.ax.plot(\n            node.position[0], node.position[1], marker=\"o\", markersize=6, color=\"k\"\n        )\n        self.ax.annotate(node.name, (node.position[0] + 10, node.position[1] + 10))\n        for connected_to in node.connected_to:\n            connected_node = self.nodes_dict[connected_to]\n            self.ax.plot(\n                [node.position[0], connected_node.position[0]],\n                [node.position[1], connected_node.position[1]],\n                linewidth=1,\n                color=\"k\",\n            )\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Graph.draw_result","title":"<code>draw_result()</code>","text":"<p>Draw the solution to the shortest path problem.</p> <p>:return:</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def draw_result(self) -&gt; None:\n    \"\"\"\n    Draw the solution to the shortest path problem.\n\n    :return:\n    \"\"\"\n    assert (\n        self._end_node\n    ), \"End node not defined, run a_star() before drawing the result.\"\n    current_node = self._end_node\n    while self.nodes_dict[current_node].predecessor:\n        curr_node = self.nodes_dict[current_node]\n        predecessor = curr_node.predecessor\n        pred_node = self.nodes_dict[predecessor]\n\n        self.ax.plot(\n            [curr_node.position[0], pred_node.position[0]],\n            [curr_node.position[1], pred_node.position[1]],\n            linewidth=2,\n            color=\"b\",\n        )\n\n        distance = np.linalg.norm(curr_node.position - pred_node.position)\n        x_mid = (curr_node.position[0] + pred_node.position[0]) / 2.0\n        y_mid = (curr_node.position[1] + pred_node.position[1]) / 2.0\n        self.ax.annotate(f\"{distance:.2f}\", (x_mid + 10, y_mid + 10))\n        current_node = predecessor\n\n    plt.show()\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Node","title":"<code>Node</code>","text":"Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>class Node:\n    def __init__(\n        self, name: str, position: np.ndarray, connected_to: List[str]\n    ) -&gt; None:\n        \"\"\"\n        Node in a graph for A* computation.\n\n        :param name: Name of the node.\n        :param position: Position of the node (x,y).\n        :param connected_to: List of the names of nodes, that this node is connected to.\n        \"\"\"\n        self.name = name\n        self.position = position\n        self.connected_to = connected_to\n        self.predecessor = None\n        self.cost_to_come = None\n        self.heuristic_cost_to_go = None\n\n    def compute_heuristic_cost_to_go(self, goal_node: Node) -&gt; None:\n        \"\"\"\n        Computes the heuristic cost to go to the goal node based on the distance and assigns it to the node object.\n\n        :param goal_node: The goal node.\n        :return:\n        \"\"\"\n        self.heuristic_cost_to_go = np.linalg.norm(goal_node.position - self.position)\n\n    def total_cost(self) -&gt; float:\n        \"\"\"\n        Computes the expected total cost to reach the goal node as sum of cost to come and heuristic cost to go.\n\n        :return: The expected total cost to reach the goal node.\n        \"\"\"\n        return self.cost_to_come + self.heuristic_cost_to_go\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Node.__init__","title":"<code>__init__(name, position, connected_to)</code>","text":"<p>Node in a graph for A* computation.</p> <p>:param name: Name of the node. :param position: Position of the node (x,y). :param connected_to: List of the names of nodes, that this node is connected to.</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def __init__(\n    self, name: str, position: np.ndarray, connected_to: List[str]\n) -&gt; None:\n    \"\"\"\n    Node in a graph for A* computation.\n\n    :param name: Name of the node.\n    :param position: Position of the node (x,y).\n    :param connected_to: List of the names of nodes, that this node is connected to.\n    \"\"\"\n    self.name = name\n    self.position = position\n    self.connected_to = connected_to\n    self.predecessor = None\n    self.cost_to_come = None\n    self.heuristic_cost_to_go = None\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Node.compute_heuristic_cost_to_go","title":"<code>compute_heuristic_cost_to_go(goal_node)</code>","text":"<p>Computes the heuristic cost to go to the goal node based on the distance and assigns it to the node object.</p> <p>:param goal_node: The goal node. :return:</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def compute_heuristic_cost_to_go(self, goal_node: Node) -&gt; None:\n    \"\"\"\n    Computes the heuristic cost to go to the goal node based on the distance and assigns it to the node object.\n\n    :param goal_node: The goal node.\n    :return:\n    \"\"\"\n    self.heuristic_cost_to_go = np.linalg.norm(goal_node.position - self.position)\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.Node.total_cost","title":"<code>total_cost()</code>","text":"<p>Computes the expected total cost to reach the goal node as sum of cost to come and heuristic cost to go.</p> <p>:return: The expected total cost to reach the goal node.</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def total_cost(self) -&gt; float:\n    \"\"\"\n    Computes the expected total cost to reach the goal node as sum of cost to come and heuristic cost to go.\n\n    :return: The expected total cost to reach the goal node.\n    \"\"\"\n    return self.cost_to_come + self.heuristic_cost_to_go\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/graph_search/a_star/#behavior_generation_lecture_python.graph_search.a_star.extract_min","title":"<code>extract_min(node_set, node_dict)</code>","text":"<p>Extract the node with minimal total cost from a set.</p> <p>:param node_set: The set of node names to be considered. :param node_dict: The node dict, containing the node information. :return: The name of the node with minimal total cost.</p> Source code in <code>src/behavior_generation_lecture_python/graph_search/a_star.py</code> <pre><code>def extract_min(node_set: Set[str], node_dict: Dict[str, Node]) -&gt; str:\n    \"\"\"\n    Extract the node with minimal total cost from a set.\n\n    :param node_set: The set of node names to be considered.\n    :param node_dict: The node dict, containing the node information.\n    :return: The name of the node with minimal total cost.\n    \"\"\"\n    min_node = min(node_set, key=lambda x: node_dict[x].total_cost())\n    node_set.remove(min_node)\n    return min_node\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/","title":"mdp","text":""},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/","title":"mdp","text":"<p>This module contains the Markov Decision Process, value iteration, Q learning and policy gradient.</p>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.GridMDP","title":"<code>GridMDP</code>","text":"<p>               Bases: <code>MDP</code></p> <p>A Markov decision process on a grid.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>class GridMDP(MDP):\n    \"\"\"A Markov decision process on a grid.\"\"\"\n\n    def __init__(\n        self,\n        grid: List[List[Union[float, None]]],\n        initial_state: GridState,\n        terminal_states: Set[GridState],\n        transition_probabilities_per_action: Dict[\n            GridState, List[Tuple[float, GridState]]\n        ],\n        restrict_actions_to_available_states: Optional[bool] = False,\n    ) -&gt; None:\n        \"\"\"A Markov decision process on a grid.\n\n        Args:\n            grid: List of lists, containing the rewards of the grid\n                states or None.\n            initial_state: Initial state in the grid.\n            terminal_states: Set of terminal states in the grid.\n            transition_probabilities_per_action: Dictionary of\n                transition probabilities per action, mapping from action\n                to list of tuples (probability, next state).\n            restrict_actions_to_available_states: Whether to restrict\n                actions to those that result in valid next states.\n        \"\"\"\n        states = set()\n        reward = {}\n        grid = grid.copy()\n        grid.reverse()  # y-axis pointing upwards\n        rows = len(grid)\n        cols = len(grid[0])\n        self.grid = grid\n        for x in range(cols):\n            for y in range(rows):\n                if grid[y][x] is not None:\n                    states.add((x, y))\n                    reward_xy = grid[y][x]\n                    assert reward_xy is not None\n                    reward[(x, y)] = reward_xy\n\n        transition_probabilities = {}\n        for state in states:\n            for action in transition_probabilities_per_action.keys():\n                transition_probability_list = self._generate_transition_probability_list(\n                    state=state,\n                    action=action,\n                    restrict_actions_to_available_states=restrict_actions_to_available_states,\n                    states=states,\n                    transition_probabilities_per_action=transition_probabilities_per_action,\n                    next_state_fn=self._next_state_deterministic,\n                )\n                if transition_probability_list:\n                    transition_probabilities[(state, action)] = (\n                        transition_probability_list\n                    )\n\n        super().__init__(\n            states=states,\n            actions=set(transition_probabilities_per_action.keys()),\n            initial_state=initial_state,\n            terminal_states=terminal_states,\n            transition_probabilities=transition_probabilities,\n            reward=reward,\n        )\n\n    @staticmethod\n    def _generate_transition_probability_list(\n        state,\n        action,\n        restrict_actions_to_available_states,\n        states,\n        transition_probabilities_per_action,\n        next_state_fn,\n    ):\n        \"\"\"Generate the transition probability list of the grid.\"\"\"\n        transition_probability_list = []\n        none_in_next_states = False\n        for (\n            probability,\n            deterministic_action,\n        ) in transition_probabilities_per_action[action]:\n            next_state = next_state_fn(\n                state,\n                deterministic_action,\n                states,\n                output_none_if_non_existing_state=restrict_actions_to_available_states,\n            )\n            if next_state is None:\n                none_in_next_states = True\n                break\n            transition_probability_list.append((probability, next_state))\n\n        if not none_in_next_states:\n            return transition_probability_list\n\n        return []\n\n    @staticmethod\n    def _next_state_deterministic(\n        state, action, states, output_none_if_non_existing_state=False\n    ):\n        \"\"\"Output the next state given the action in a deterministic setting.\n        Output None if next state not existing in case output_none_if_non_existing_state is True.\n        \"\"\"\n        next_state_candidate = tuple(np.array(state) + np.array(action))\n        if next_state_candidate in states:\n            return next_state_candidate\n        if output_none_if_non_existing_state:\n            return None\n        return state\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.GridMDP.__init__","title":"<code>__init__(grid, initial_state, terminal_states, transition_probabilities_per_action, restrict_actions_to_available_states=False)</code>","text":"<p>A Markov decision process on a grid.</p> <p>Parameters:</p> Name Type Description Default <code>grid</code> <code>List[List[Union[float, None]]]</code> <p>List of lists, containing the rewards of the grid states or None.</p> required <code>initial_state</code> <code>GridState</code> <p>Initial state in the grid.</p> required <code>terminal_states</code> <code>Set[GridState]</code> <p>Set of terminal states in the grid.</p> required <code>transition_probabilities_per_action</code> <code>Dict[GridState, List[Tuple[float, GridState]]]</code> <p>Dictionary of transition probabilities per action, mapping from action to list of tuples (probability, next state).</p> required <code>restrict_actions_to_available_states</code> <code>Optional[bool]</code> <p>Whether to restrict actions to those that result in valid next states.</p> <code>False</code> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def __init__(\n    self,\n    grid: List[List[Union[float, None]]],\n    initial_state: GridState,\n    terminal_states: Set[GridState],\n    transition_probabilities_per_action: Dict[\n        GridState, List[Tuple[float, GridState]]\n    ],\n    restrict_actions_to_available_states: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"A Markov decision process on a grid.\n\n    Args:\n        grid: List of lists, containing the rewards of the grid\n            states or None.\n        initial_state: Initial state in the grid.\n        terminal_states: Set of terminal states in the grid.\n        transition_probabilities_per_action: Dictionary of\n            transition probabilities per action, mapping from action\n            to list of tuples (probability, next state).\n        restrict_actions_to_available_states: Whether to restrict\n            actions to those that result in valid next states.\n    \"\"\"\n    states = set()\n    reward = {}\n    grid = grid.copy()\n    grid.reverse()  # y-axis pointing upwards\n    rows = len(grid)\n    cols = len(grid[0])\n    self.grid = grid\n    for x in range(cols):\n        for y in range(rows):\n            if grid[y][x] is not None:\n                states.add((x, y))\n                reward_xy = grid[y][x]\n                assert reward_xy is not None\n                reward[(x, y)] = reward_xy\n\n    transition_probabilities = {}\n    for state in states:\n        for action in transition_probabilities_per_action.keys():\n            transition_probability_list = self._generate_transition_probability_list(\n                state=state,\n                action=action,\n                restrict_actions_to_available_states=restrict_actions_to_available_states,\n                states=states,\n                transition_probabilities_per_action=transition_probabilities_per_action,\n                next_state_fn=self._next_state_deterministic,\n            )\n            if transition_probability_list:\n                transition_probabilities[(state, action)] = (\n                    transition_probability_list\n                )\n\n    super().__init__(\n        states=states,\n        actions=set(transition_probabilities_per_action.keys()),\n        initial_state=initial_state,\n        terminal_states=terminal_states,\n        transition_probabilities=transition_probabilities,\n        reward=reward,\n    )\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP","title":"<code>MDP</code>","text":"<p>A Markov decision process.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>class MDP:\n    \"\"\"A Markov decision process.\"\"\"\n\n    def __init__(\n        self,\n        states: Set[Any],\n        actions: Set[Any],\n        initial_state: Any,\n        terminal_states: Set[Any],\n        transition_probabilities: Dict[Tuple[Any, Any], List[Tuple[float, Any]]],\n        reward: Dict[Any, float],\n    ) -&gt; None:\n        \"\"\"A Markov decision process.\n\n        Args:\n            states: Set of states.\n            actions: Set of actions.\n            initial_state: Initial state.\n            terminal_states: Set of terminal states.\n            transition_probabilities: Dictionary of transition\n                probabilities, mapping from tuple (state, action) to\n                list of tuples (probability, next state).\n            reward: Dictionary of rewards per state, mapping from state\n                to reward.\n        \"\"\"\n        self.states = states\n\n        self.actions = actions\n\n        assert initial_state in self.states\n        self.initial_state = initial_state\n\n        for terminal_state in terminal_states:\n            assert (\n                terminal_state in self.states\n            ), f\"The terminal state {terminal_state} is not in states {states}\"\n        self.terminal_states = terminal_states\n\n        for state in self.states:\n            for action in self.actions:\n                if (state, action) not in transition_probabilities:\n                    continue\n                total_prob = 0.0\n                for prob, next_state in transition_probabilities[(state, action)]:\n                    assert (\n                        next_state in self.states\n                    ), f\"next_state={next_state} is not in states={states}\"\n                    total_prob += prob\n                assert math.isclose(total_prob, 1), \"Probabilities must add to one\"\n        self.transition_probabilities = transition_probabilities\n\n        assert set(reward.keys()) == set(\n            self.states\n        ), \"Rewards must be defined for every state in the set of states\"\n        for state in self.states:\n            assert reward[state] is not None\n        self.reward = reward\n\n    def get_states(self) -&gt; Set[Any]:\n        \"\"\"Get the set of states.\"\"\"\n        return self.states\n\n    def get_actions(self, state) -&gt; Set[Any]:\n        \"\"\"Get the set of actions available in a certain state, returns [None] for terminal states.\"\"\"\n        if self.is_terminal(state):\n            return {None}\n        return {a for a in self.actions if (state, a) in self.transition_probabilities}\n\n    def get_reward(self, state) -&gt; float:\n        \"\"\"Get the reward for a specific state.\"\"\"\n        return self.reward[state]\n\n    def is_terminal(self, state) -&gt; bool:\n        \"\"\"Return whether a state is a terminal state.\"\"\"\n        return state in self.terminal_states\n\n    def get_transitions_with_probabilities(\n        self, state, action\n    ) -&gt; List[Tuple[float, Any]]:\n        \"\"\"Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.\"\"\"\n        if action is None or self.is_terminal(state):\n            return [(0.0, state)]\n        return self.transition_probabilities[(state, action)]\n\n    def sample_next_state(self, state, action) -&gt; Any:\n        \"\"\"Randomly sample the next state given the current state and taken action.\"\"\"\n        if self.is_terminal(state):\n            raise ValueError(\"No next state for terminal states.\")\n        if action is None:\n            raise ValueError(\"Action must not be None.\")\n        prob_per_transition = self.get_transitions_with_probabilities(state, action)\n        num_actions = len(prob_per_transition)\n        choice = np.random.choice(\n            num_actions, p=[ppa[0] for ppa in prob_per_transition]\n        )\n        return prob_per_transition[choice][1]\n\n    def execute_action(self, state, action) -&gt; Tuple[Any, float, bool]:\n        \"\"\"Executes the action in the current state and returns the new state, obtained reward and terminal flag.\"\"\"\n        new_state = self.sample_next_state(state=state, action=action)\n        reward = self.get_reward(state=new_state)\n        terminal = self.is_terminal(state=new_state)\n        return new_state, reward, terminal\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.__init__","title":"<code>__init__(states, actions, initial_state, terminal_states, transition_probabilities, reward)</code>","text":"<p>A Markov decision process.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Set[Any]</code> <p>Set of states.</p> required <code>actions</code> <code>Set[Any]</code> <p>Set of actions.</p> required <code>initial_state</code> <code>Any</code> <p>Initial state.</p> required <code>terminal_states</code> <code>Set[Any]</code> <p>Set of terminal states.</p> required <code>transition_probabilities</code> <code>Dict[Tuple[Any, Any], List[Tuple[float, Any]]]</code> <p>Dictionary of transition probabilities, mapping from tuple (state, action) to list of tuples (probability, next state).</p> required <code>reward</code> <code>Dict[Any, float]</code> <p>Dictionary of rewards per state, mapping from state to reward.</p> required Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def __init__(\n    self,\n    states: Set[Any],\n    actions: Set[Any],\n    initial_state: Any,\n    terminal_states: Set[Any],\n    transition_probabilities: Dict[Tuple[Any, Any], List[Tuple[float, Any]]],\n    reward: Dict[Any, float],\n) -&gt; None:\n    \"\"\"A Markov decision process.\n\n    Args:\n        states: Set of states.\n        actions: Set of actions.\n        initial_state: Initial state.\n        terminal_states: Set of terminal states.\n        transition_probabilities: Dictionary of transition\n            probabilities, mapping from tuple (state, action) to\n            list of tuples (probability, next state).\n        reward: Dictionary of rewards per state, mapping from state\n            to reward.\n    \"\"\"\n    self.states = states\n\n    self.actions = actions\n\n    assert initial_state in self.states\n    self.initial_state = initial_state\n\n    for terminal_state in terminal_states:\n        assert (\n            terminal_state in self.states\n        ), f\"The terminal state {terminal_state} is not in states {states}\"\n    self.terminal_states = terminal_states\n\n    for state in self.states:\n        for action in self.actions:\n            if (state, action) not in transition_probabilities:\n                continue\n            total_prob = 0.0\n            for prob, next_state in transition_probabilities[(state, action)]:\n                assert (\n                    next_state in self.states\n                ), f\"next_state={next_state} is not in states={states}\"\n                total_prob += prob\n            assert math.isclose(total_prob, 1), \"Probabilities must add to one\"\n    self.transition_probabilities = transition_probabilities\n\n    assert set(reward.keys()) == set(\n        self.states\n    ), \"Rewards must be defined for every state in the set of states\"\n    for state in self.states:\n        assert reward[state] is not None\n    self.reward = reward\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.execute_action","title":"<code>execute_action(state, action)</code>","text":"<p>Executes the action in the current state and returns the new state, obtained reward and terminal flag.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def execute_action(self, state, action) -&gt; Tuple[Any, float, bool]:\n    \"\"\"Executes the action in the current state and returns the new state, obtained reward and terminal flag.\"\"\"\n    new_state = self.sample_next_state(state=state, action=action)\n    reward = self.get_reward(state=new_state)\n    terminal = self.is_terminal(state=new_state)\n    return new_state, reward, terminal\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.get_actions","title":"<code>get_actions(state)</code>","text":"<p>Get the set of actions available in a certain state, returns [None] for terminal states.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def get_actions(self, state) -&gt; Set[Any]:\n    \"\"\"Get the set of actions available in a certain state, returns [None] for terminal states.\"\"\"\n    if self.is_terminal(state):\n        return {None}\n    return {a for a in self.actions if (state, a) in self.transition_probabilities}\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.get_reward","title":"<code>get_reward(state)</code>","text":"<p>Get the reward for a specific state.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def get_reward(self, state) -&gt; float:\n    \"\"\"Get the reward for a specific state.\"\"\"\n    return self.reward[state]\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.get_states","title":"<code>get_states()</code>","text":"<p>Get the set of states.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def get_states(self) -&gt; Set[Any]:\n    \"\"\"Get the set of states.\"\"\"\n    return self.states\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.get_transitions_with_probabilities","title":"<code>get_transitions_with_probabilities(state, action)</code>","text":"<p>Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def get_transitions_with_probabilities(\n    self, state, action\n) -&gt; List[Tuple[float, Any]]:\n    \"\"\"Get the list of transitions with their probability, returns [(0.0, state)] for terminal states.\"\"\"\n    if action is None or self.is_terminal(state):\n        return [(0.0, state)]\n    return self.transition_probabilities[(state, action)]\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.is_terminal","title":"<code>is_terminal(state)</code>","text":"<p>Return whether a state is a terminal state.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def is_terminal(self, state) -&gt; bool:\n    \"\"\"Return whether a state is a terminal state.\"\"\"\n    return state in self.terminal_states\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.MDP.sample_next_state","title":"<code>sample_next_state(state, action)</code>","text":"<p>Randomly sample the next state given the current state and taken action.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def sample_next_state(self, state, action) -&gt; Any:\n    \"\"\"Randomly sample the next state given the current state and taken action.\"\"\"\n    if self.is_terminal(state):\n        raise ValueError(\"No next state for terminal states.\")\n    if action is None:\n        raise ValueError(\"Action must not be None.\")\n    prob_per_transition = self.get_transitions_with_probabilities(state, action)\n    num_actions = len(prob_per_transition)\n    choice = np.random.choice(\n        num_actions, p=[ppa[0] for ppa in prob_per_transition]\n    )\n    return prob_per_transition[choice][1]\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.PolicyGradientBuffer","title":"<code>PolicyGradientBuffer</code>  <code>dataclass</code>","text":"<p>Buffer for the policy gradient method.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>@dataclass\nclass PolicyGradientBuffer:\n    \"\"\"Buffer for the policy gradient method.\"\"\"\n\n    states: List[Any] = field(default_factory=list)\n    actions: List[Any] = field(default_factory=list)\n    weights: List[float] = field(default_factory=list)\n    episode_returns: List[float] = field(default_factory=list)\n    episode_lengths: List[int] = field(default_factory=list)\n\n    def mean_episode_return(self) -&gt; float:\n        \"\"\"Mean episode return.\"\"\"\n        return float(np.mean(self.episode_returns))\n\n    def mean_episode_length(self) -&gt; float:\n        \"\"\"Mean episode length.\"\"\"\n        return float(np.mean(self.episode_lengths))\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.PolicyGradientBuffer.mean_episode_length","title":"<code>mean_episode_length()</code>","text":"<p>Mean episode length.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def mean_episode_length(self) -&gt; float:\n    \"\"\"Mean episode length.\"\"\"\n    return float(np.mean(self.episode_lengths))\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.PolicyGradientBuffer.mean_episode_return","title":"<code>mean_episode_return()</code>","text":"<p>Mean episode return.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def mean_episode_return(self) -&gt; float:\n    \"\"\"Mean episode return.\"\"\"\n    return float(np.mean(self.episode_returns))\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.best_action_from_q_table","title":"<code>best_action_from_q_table(*, state, available_actions, q_table)</code>","text":"<p>Derive the best action from a Q table.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Any</code> <p>The state in which to take an action.</p> required <code>available_actions</code> <code>Set[Any]</code> <p>Set of available actions.</p> required <code>q_table</code> <code>QTable</code> <p>The Q table, mapping from state-action pair to value estimate.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The best action according to the Q table.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def best_action_from_q_table(\n    *, state: Any, available_actions: Set[Any], q_table: QTable\n) -&gt; Any:\n    \"\"\"Derive the best action from a Q table.\n\n    Args:\n        state: The state in which to take an action.\n        available_actions: Set of available actions.\n        q_table: The Q table, mapping from state-action pair to value estimate.\n\n    Returns:\n        The best action according to the Q table.\n    \"\"\"\n    available_actions_list = list(available_actions)\n    values = np.array([q_table[(state, action)] for action in available_actions_list])\n    action = available_actions_list[np.argmax(values)]\n    return action\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.derive_deterministic_policy","title":"<code>derive_deterministic_policy(mdp, policy)</code>","text":"<p>Compute the best policy for an MDP given the stochastic policy.</p> <p>Parameters:</p> Name Type Description Default <code>mdp</code> <code>MDP</code> <p>The underlying MDP.</p> required <code>policy</code> <code>CategoricalPolicy</code> <p>The stochastic policy.</p> required <p>Returns:</p> Type Description <code>Dict[Any, Any]</code> <p>Deterministic policy, i.e. mapping from state to action.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def derive_deterministic_policy(mdp: MDP, policy: CategoricalPolicy) -&gt; Dict[Any, Any]:\n    \"\"\"Compute the best policy for an MDP given the stochastic policy.\n\n    Args:\n        mdp: The underlying MDP.\n        policy: The stochastic policy.\n\n    Returns:\n        Deterministic policy, i.e. mapping from state to action.\n    \"\"\"\n    pi = {}\n    for state in mdp.get_states():\n        if mdp.is_terminal(state):\n            continue\n        pi[state] = policy.get_action(\n            state=torch.as_tensor(state, dtype=torch.float32), deterministic=True\n        )\n    return pi\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.derive_policy","title":"<code>derive_policy(mdp, utility_of_states)</code>","text":"<p>Compute the best policy for an MDP given the utility of the states.</p> <p>Parameters:</p> Name Type Description Default <code>mdp</code> <code>MDP</code> <p>The underlying MDP.</p> required <code>utility_of_states</code> <code>StateValueTable</code> <p>The dictionary containing the utility (estimate) of all states.</p> required <p>Returns:</p> Type Description <code>Dict[Any, Any]</code> <p>Policy, i.e. mapping from state to action.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def derive_policy(mdp: MDP, utility_of_states: StateValueTable) -&gt; Dict[Any, Any]:\n    \"\"\"Compute the best policy for an MDP given the utility of the states.\n\n    Args:\n        mdp: The underlying MDP.\n        utility_of_states: The dictionary containing the utility\n            (estimate) of all states.\n\n    Returns:\n        Policy, i.e. mapping from state to action.\n    \"\"\"\n    pi = {}\n    for state in mdp.get_states():\n        pi[state] = max(\n            mdp.get_actions(state),\n            key=lambda action: expected_utility_of_action(\n                mdp=mdp, state=state, action=action, utility_of_states=utility_of_states\n            ),\n        )\n    return pi\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.expected_utility_of_action","title":"<code>expected_utility_of_action(mdp, state, action, utility_of_states)</code>","text":"<p>Compute the expected utility of taking an action in a state.</p> <p>Parameters:</p> Name Type Description Default <code>mdp</code> <code>MDP</code> <p>The underlying MDP.</p> required <code>state</code> <code>Any</code> <p>The start state.</p> required <code>action</code> <code>Any</code> <p>The action to be taken.</p> required <code>utility_of_states</code> <code>StateValueTable</code> <p>The dictionary containing the utility (estimate) of all states.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Expected utility</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def expected_utility_of_action(\n    mdp: MDP, state: Any, action: Any, utility_of_states: StateValueTable\n) -&gt; float:\n    \"\"\"Compute the expected utility of taking an action in a state.\n\n    Args:\n        mdp: The underlying MDP.\n        state: The start state.\n        action: The action to be taken.\n        utility_of_states: The dictionary containing the utility\n            (estimate) of all states.\n\n    Returns:\n        Expected utility\n    \"\"\"\n    return sum(\n        p * (mdp.get_reward(next_state) + utility_of_states[next_state])\n        for (p, next_state) in mdp.get_transitions_with_probabilities(\n            state=state, action=action\n        )\n    )\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.greedy_value_estimate_for_state","title":"<code>greedy_value_estimate_for_state(*, q_table, state)</code>","text":"<p>Compute the greedy (best possible) value estimate for a state from the Q table.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Any</code> <p>The state for which to estimate the value, when being greedy.</p> required <code>q_table</code> <code>QTable</code> <p>The Q table, mapping from state-action pair to value estimate.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The value based on the greedy estimate.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def greedy_value_estimate_for_state(*, q_table: QTable, state: Any) -&gt; float:\n    \"\"\"Compute the greedy (best possible) value estimate for a state from the Q table.\n\n    Args:\n        state: The state for which to estimate the value, when being greedy.\n        q_table: The Q table, mapping from state-action pair to value estimate.\n\n    Returns:\n        The value based on the greedy estimate.\n    \"\"\"\n    available_actions = [\n        state_action[1] for state_action in q_table.keys() if state_action[0] == state\n    ]\n    return max(q_table[state, action] for action in available_actions)\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.policy_gradient","title":"<code>policy_gradient(*, mdp, policy, lr=0.01, iterations=50, batch_size=5000, seed=None, return_history=False, use_random_init_state=False, verbose=True)</code>","text":"<p>Train a paramterized policy using vanilla policy gradient.</p> <p>Adapted from: https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py</p> <p>The MIT License (MIT)</p> <p>Copyright (c) 2018 OpenAI (http://openai.com)</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Parameters:</p> Name Type Description Default <code>mdp</code> <code>MDP</code> <p>The underlying MDP.</p> required <code>policy</code> <code>CategoricalPolicy</code> <p>The stochastic policy to be trained.</p> required <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>0.01</code> <code>iterations</code> <code>int</code> <p>Number of iterations.</p> <code>50</code> <code>batch_size</code> <code>int</code> <p>Number of samples generated for each policy update.</p> <code>5000</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility (default: None).</p> <code>None</code> <code>return_history</code> <code>bool</code> <p>Whether to return the whole history of value estimates instead of just the final estimate.</p> <code>False</code> <code>use_random_init_state</code> <code>bool</code> <p>bool, if the agent should be initialized randomly.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>bool, if traing progress should be printed.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[List[CategoricalPolicy], CategoricalPolicy]</code> <p>The final policy, if return_history is false. The</p> <code>Union[List[CategoricalPolicy], CategoricalPolicy]</code> <p>history of policies as list, if return_history is true.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def policy_gradient(\n    *,\n    mdp: MDP,\n    policy: CategoricalPolicy,\n    lr: float = 1e-2,\n    iterations: int = 50,\n    batch_size: int = 5000,\n    seed: Optional[int] = None,\n    return_history: bool = False,\n    use_random_init_state: bool = False,\n    verbose: bool = True,\n) -&gt; Union[List[CategoricalPolicy], CategoricalPolicy]:\n    \"\"\"Train a paramterized policy using vanilla policy gradient.\n\n    Adapted from: https://github.com/openai/spinningup/blob/master/spinup/examples/pytorch/pg_math/1_simple_pg.py\n\n    The MIT License (MIT)\n\n    Copyright (c) 2018 OpenAI (http://openai.com)\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n    Args:\n        mdp: The underlying MDP.\n        policy: The stochastic policy to be trained.\n        lr: Learning rate.\n        iterations: Number of iterations.\n        batch_size: Number of samples generated for each policy update.\n        seed: Random seed for reproducibility (default: None).\n        return_history: Whether to return the whole history of value estimates\n            instead of just the final estimate.\n        use_random_init_state: bool, if the agent should be initialized randomly.\n        verbose: bool, if traing progress should be printed.\n\n    Returns:\n        The final policy, if return_history is false. The\n        history of policies as list, if return_history is true.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n\n    # add untrained model to model_checkpoints\n    model_checkpoints = [deepcopy(policy)]\n\n    # make optimizer\n    optimizer = torch.optim.Adam(policy.net.parameters(), lr=lr)\n\n    # get non-terminal states\n    non_terminal_states = [state for state in mdp.states if not mdp.is_terminal(state)]\n\n    # training loop\n    for i in range(1, iterations + 1):\n        # a buffer for storing intermediate values\n        buffer = PolicyGradientBuffer()\n\n        # reset episode-specific variables\n        if use_random_init_state:\n            state = non_terminal_states[np.random.choice(len(non_terminal_states))]\n        else:\n            state = mdp.initial_state\n        episode_rewards = []\n\n        # collect experience by acting in the mdp\n        while True:\n            # save visited state\n            buffer.states.append(deepcopy(state))\n\n            # call model to get next action\n            action = policy.get_action(state=torch.tensor(state, dtype=torch.float32))\n\n            # execute action in the environment\n            state, reward, done = mdp.execute_action(state=state, action=action)\n\n            # save action, reward\n            buffer.actions.append(action)\n            episode_rewards.append(reward)\n\n            if done:\n                # if episode is over, record info about episode\n                episode_return = sum(episode_rewards)\n                episode_length = len(episode_rewards)\n                buffer.episode_returns.append(episode_return)\n                buffer.episode_lengths.append(episode_length)\n                # the weight for each logprob(a|s) is R(tau)\n                buffer.weights += [episode_return] * episode_length\n\n                # reset episode-specific variables\n                if use_random_init_state:\n                    state = non_terminal_states[\n                        np.random.choice(len(non_terminal_states))\n                    ]\n                else:\n                    state = mdp.initial_state\n                episode_rewards = []\n\n                # end experience loop if we have enough of it\n                if len(buffer.states) &gt; batch_size:\n                    break\n\n        # compute the loss\n        logp = policy.get_log_prob(\n            states=torch.tensor(buffer.states, dtype=torch.float),\n            actions=torch.tensor(buffer.actions, dtype=torch.long),\n        )\n        batch_loss = -(logp * torch.tensor(buffer.weights, dtype=torch.float)).mean()\n\n        # take a single policy gradient update step\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n        # logging\n        if verbose:\n            print(\n                f\"iteration: {i:3d};  return: {buffer.mean_episode_return():.3f};  episode_length: {buffer.mean_episode_length():.3f}\"\n            )\n        if return_history:\n            model_checkpoints.append(deepcopy(policy))\n    if return_history:\n        return model_checkpoints\n    return policy\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.q_learning","title":"<code>q_learning(*, mdp, alpha, epsilon, iterations, seed=None, return_history=False)</code>","text":"<p>Derive a value estimate for state-action pairs by means of Q learning.</p> <p>Parameters:</p> Name Type Description Default <code>mdp</code> <code>MDP</code> <p>The underlying MDP.</p> required <code>alpha</code> <code>float</code> <p>Learning rate.</p> required <code>epsilon</code> <code>float</code> <p>Exploration-exploitation threshold. A random action is taken with probability epsilon, the best action otherwise.</p> required <code>iterations</code> <code>int</code> <p>Number of iterations.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility (default: None).</p> <code>None</code> <code>return_history</code> <code>Optional[bool]</code> <p>Whether to return the whole history of value estimates instead of just the final estimate.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[QTable, List[QTable]]</code> <p>The final value estimate, if return_history is false. The</p> <code>Union[QTable, List[QTable]]</code> <p>history of value estimates as list, if return_history is true.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def q_learning(\n    *,\n    mdp: MDP,\n    alpha: float,\n    epsilon: float,\n    iterations: int,\n    seed: Optional[int] = None,\n    return_history: Optional[bool] = False,\n) -&gt; Union[QTable, List[QTable]]:\n    \"\"\"Derive a value estimate for state-action pairs by means of Q learning.\n\n    Args:\n        mdp: The underlying MDP.\n        alpha: Learning rate.\n        epsilon: Exploration-exploitation threshold. A random action is taken with\n            probability epsilon, the best action otherwise.\n        iterations: Number of iterations.\n        seed: Random seed for reproducibility (default: None).\n        return_history: Whether to return the whole history of value estimates\n            instead of just the final estimate.\n\n    Returns:\n        The final value estimate, if return_history is false. The\n        history of value estimates as list, if return_history is true.\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n\n    q_table = {}\n    for state in mdp.get_states():\n        for action in mdp.get_actions(state):\n            q_table[(state, action)] = 0.0\n    q_table_history = [q_table.copy()]\n    state = mdp.initial_state\n\n    for _ in range(iterations):\n        # available actions:\n        avail_actions = mdp.get_actions(state)\n\n        # choose action (exploration-exploitation trade-off)\n        rand = np.random.random()\n        if rand &lt; (1 - epsilon):\n            chosen_action = best_action_from_q_table(\n                state=state, available_actions=avail_actions, q_table=q_table\n            )\n        else:\n            chosen_action = random_action(avail_actions)\n\n        # interact with environment\n        next_state = mdp.sample_next_state(state, chosen_action)\n\n        # update Q table\n        greedy_value_estimate_next_state = greedy_value_estimate_for_state(\n            q_table=q_table, state=next_state\n        )\n        q_table[(state, chosen_action)] = (1 - alpha) * q_table[\n            (state, chosen_action)\n        ] + alpha * (mdp.get_reward(next_state) + greedy_value_estimate_next_state)\n\n        if return_history:\n            q_table_history.append(q_table.copy())\n\n        if mdp.is_terminal(next_state):\n            state = mdp.initial_state  # restart\n        else:\n            state = next_state  # continue\n\n    if return_history:\n        utility_history = []\n        for q_tab in q_table_history:\n            utility_history.append(\n                {\n                    state: greedy_value_estimate_for_state(q_table=q_tab, state=state)\n                    for state in mdp.get_states()\n                }\n            )\n        return utility_history\n\n    return {\n        state: greedy_value_estimate_for_state(q_table=q_table, state=state)\n        for state in mdp.get_states()\n    }\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.random_action","title":"<code>random_action(available_actions)</code>","text":"<p>Derive a random action from the set of available actions.</p> <p>Parameters:</p> Name Type Description Default <code>available_actions</code> <code>Set[Any]</code> <p>Set of available actions.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A random action.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def random_action(available_actions: Set[Any]) -&gt; Any:\n    \"\"\"Derive a random action from the set of available actions.\n\n    Args:\n        available_actions: Set of available actions.\n\n    Returns:\n        A random action.\n    \"\"\"\n    available_actions_list = list(available_actions)\n    num_actions = len(available_actions_list)\n    choice = np.random.choice(num_actions)\n    return available_actions_list[choice]\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/mdp/#behavior_generation_lecture_python.mdp.mdp.value_iteration","title":"<code>value_iteration(mdp, epsilon, max_iterations, return_history=False)</code>","text":"<p>Derive a utility estimate by means of value iteration.</p> <p>Parameters:</p> Name Type Description Default <code>mdp</code> <code>MDP</code> <p>The underlying MDP.</p> required <code>epsilon</code> <code>float</code> <p>Termination criterion: if maximum difference in utility update is below epsilon, the iteration is terminated.</p> required <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations, if exceeded, RuntimeError is raised.</p> required <code>return_history</code> <code>Optional[bool]</code> <p>Whether to return the whole history of utilities instead of just the final estimate.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[StateValueTable, List[StateValueTable]]</code> <p>The final utility estimate, if return_history is false. The</p> <code>Union[StateValueTable, List[StateValueTable]]</code> <p>history of utility estimates as list, if return_history is true.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/mdp.py</code> <pre><code>def value_iteration(\n    mdp: MDP,\n    epsilon: float,\n    max_iterations: int,\n    return_history: Optional[bool] = False,\n) -&gt; Union[StateValueTable, List[StateValueTable]]:\n    \"\"\"Derive a utility estimate by means of value iteration.\n\n    Args:\n        mdp: The underlying MDP.\n        epsilon: Termination criterion: if maximum difference in utility\n            update is below epsilon, the iteration is terminated.\n        max_iterations: Maximum number of iterations, if exceeded,\n            RuntimeError is raised.\n        return_history: Whether to return the whole history of utilities\n            instead of just the final estimate.\n\n    Returns:\n        The final utility estimate, if return_history is false. The\n        history of utility estimates as list, if return_history is true.\n    \"\"\"\n    utility = {state: 0.0 for state in mdp.get_states()}\n    utility_history = [utility.copy()]\n    for _ in range(max_iterations):\n        utility_old = utility.copy()\n        max_delta = 0.0\n        for state in mdp.get_states():\n            utility[state] = max(\n                expected_utility_of_action(\n                    mdp, state=state, action=action, utility_of_states=utility_old\n                )\n                for action in mdp.get_actions(state)\n            )\n            max_delta = max(max_delta, abs(utility[state] - utility_old[state]))\n        if return_history:\n            utility_history.append(utility.copy())\n        if max_delta &lt; epsilon:\n            if return_history:\n                return utility_history\n            return utility\n    raise RuntimeError(f\"Did not converge in {max_iterations} iterations\")\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/policy/","title":"policy","text":"<p>This module contains the CategoricalPolicy implementation.</p>"},{"location":"reference/behavior_generation_lecture_python/mdp/policy/#behavior_generation_lecture_python.mdp.policy.CategoricalPolicy","title":"<code>CategoricalPolicy</code>","text":"<p>A categorical policy parameterized by a neural network.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/policy.py</code> <pre><code>class CategoricalPolicy:\n    \"\"\"A categorical policy parameterized by a neural network.\"\"\"\n\n    def __init__(\n        self, sizes: List[int], actions: List[Any], seed: Optional[int] = None\n    ) -&gt; None:\n        \"\"\"Initialize the categorical policy.\n\n        Args:\n            sizes: List of layer sizes for the MLP.\n            actions: List of available actions.\n            seed: Random seed for reproducibility (default: None).\n        \"\"\"\n        assert sizes[-1] == len(actions)\n        if seed is not None:\n            torch.manual_seed(seed)\n        self.net = multi_layer_perceptron(sizes=sizes)\n        self.actions = actions\n        self._actions_tensor = torch.tensor(actions, dtype=torch.long).view(\n            len(actions), -1\n        )\n\n    def _get_distribution(self, state: torch.Tensor) -&gt; Categorical:\n        \"\"\"Calls the model and returns a categorical distribution over the actions.\n\n        Args:\n            state: The current state tensor.\n\n        Returns:\n            A categorical distribution over actions.\n        \"\"\"\n        logits = self.net(state)\n        return Categorical(logits=logits)\n\n    def get_action(self, state: torch.Tensor, deterministic: bool = False) -&gt; Any:\n        \"\"\"Returns an action sample for the given state.\n\n        Args:\n            state: The current state tensor.\n            deterministic: If True, return the most likely action.\n\n        Returns:\n            The selected action.\n        \"\"\"\n        policy = self._get_distribution(state)\n        if deterministic:\n            return self.actions[policy.mode.item()]\n        return self.actions[policy.sample().item()]\n\n    def get_log_prob(self, states: torch.Tensor, actions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the log-probability for taking the action, when being in the given state.\n\n        Args:\n            states: Batch of state tensors.\n            actions: Batch of action tensors.\n\n        Returns:\n            Log-probabilities of the actions.\n        \"\"\"\n        return self._get_distribution(states).log_prob(\n            self._get_action_id_from_action(actions)\n        )\n\n    def _get_action_id_from_action(self, actions: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the indices of the passed actions in self.actions.\n\n        Args:\n            actions: Batch of action tensors.\n\n        Returns:\n            Tensor of action indices.\n        \"\"\"\n        reshaped_actions = actions.unsqueeze(1).expand(\n            -1, self._actions_tensor.size(0), -1\n        )\n        reshaped_actions_tensor = self._actions_tensor.unsqueeze(0).expand(\n            actions.size(0), -1, -1\n        )\n        return torch.where(\n            torch.all(reshaped_actions == reshaped_actions_tensor, dim=-1)\n        )[1]\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/policy/#behavior_generation_lecture_python.mdp.policy.CategoricalPolicy.__init__","title":"<code>__init__(sizes, actions, seed=None)</code>","text":"<p>Initialize the categorical policy.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[int]</code> <p>List of layer sizes for the MLP.</p> required <code>actions</code> <code>List[Any]</code> <p>List of available actions.</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducibility (default: None).</p> <code>None</code> Source code in <code>src/behavior_generation_lecture_python/mdp/policy.py</code> <pre><code>def __init__(\n    self, sizes: List[int], actions: List[Any], seed: Optional[int] = None\n) -&gt; None:\n    \"\"\"Initialize the categorical policy.\n\n    Args:\n        sizes: List of layer sizes for the MLP.\n        actions: List of available actions.\n        seed: Random seed for reproducibility (default: None).\n    \"\"\"\n    assert sizes[-1] == len(actions)\n    if seed is not None:\n        torch.manual_seed(seed)\n    self.net = multi_layer_perceptron(sizes=sizes)\n    self.actions = actions\n    self._actions_tensor = torch.tensor(actions, dtype=torch.long).view(\n        len(actions), -1\n    )\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/policy/#behavior_generation_lecture_python.mdp.policy.CategoricalPolicy.get_action","title":"<code>get_action(state, deterministic=False)</code>","text":"<p>Returns an action sample for the given state.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>Tensor</code> <p>The current state tensor.</p> required <code>deterministic</code> <code>bool</code> <p>If True, return the most likely action.</p> <code>False</code> <p>Returns:</p> Type Description <code>Any</code> <p>The selected action.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/policy.py</code> <pre><code>def get_action(self, state: torch.Tensor, deterministic: bool = False) -&gt; Any:\n    \"\"\"Returns an action sample for the given state.\n\n    Args:\n        state: The current state tensor.\n        deterministic: If True, return the most likely action.\n\n    Returns:\n        The selected action.\n    \"\"\"\n    policy = self._get_distribution(state)\n    if deterministic:\n        return self.actions[policy.mode.item()]\n    return self.actions[policy.sample().item()]\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/policy/#behavior_generation_lecture_python.mdp.policy.CategoricalPolicy.get_log_prob","title":"<code>get_log_prob(states, actions)</code>","text":"<p>Returns the log-probability for taking the action, when being in the given state.</p> <p>Parameters:</p> Name Type Description Default <code>states</code> <code>Tensor</code> <p>Batch of state tensors.</p> required <code>actions</code> <code>Tensor</code> <p>Batch of action tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log-probabilities of the actions.</p> Source code in <code>src/behavior_generation_lecture_python/mdp/policy.py</code> <pre><code>def get_log_prob(self, states: torch.Tensor, actions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the log-probability for taking the action, when being in the given state.\n\n    Args:\n        states: Batch of state tensors.\n        actions: Batch of action tensors.\n\n    Returns:\n        Log-probabilities of the actions.\n    \"\"\"\n    return self._get_distribution(states).log_prob(\n        self._get_action_id_from_action(actions)\n    )\n</code></pre>"},{"location":"reference/behavior_generation_lecture_python/mdp/policy/#behavior_generation_lecture_python.mdp.policy.multi_layer_perceptron","title":"<code>multi_layer_perceptron(sizes, activation=nn.ReLU, output_activation=nn.Identity)</code>","text":"<p>Returns a multi-layer perceptron</p> Source code in <code>src/behavior_generation_lecture_python/mdp/policy.py</code> <pre><code>def multi_layer_perceptron(\n    sizes: List[int],\n    activation: Type[nn.Module] = nn.ReLU,\n    output_activation: Type[nn.Module] = nn.Identity,\n):\n    \"\"\"Returns a multi-layer perceptron\"\"\"\n    mlp = nn.Sequential()\n    for i in range(len(sizes) - 1):\n        mlp.append(nn.Linear(sizes[i], sizes[i + 1]))\n        if i &lt; len(sizes) - 2:\n            mlp.append(activation())\n        else:\n            mlp.append(output_activation())\n    return mlp\n</code></pre>"}]}